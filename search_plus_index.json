{"./":{"url":"./","title":"Introduction","keywords":"","body":"About Some notes about k8s during working and studying. GitHub: https://github.com/zhiweiyin318/k8s-notes powered by GitbookUpdated: 2019-01-21 17:23:25 "},"content/concept.html":{"url":"content/concept.html","title":"IaaS PaaS SaaS","keywords":"","body":"定义 这些年的教育导致凡事学个啥都想着先找个定义，云计算的定义相信这些年随着时间和技术的变革，不停的在发生着变化，我试着找了找定义： 云计算 “云计算的本质是一种服务提供模型，通过这种模型可以随时、随地、按需地通过网络访问共享资源池的资源，这个资源池的内容包括计算资源、网络资源、存储资源等，这些资源能够被动态地分配和调整，在不同用户之间灵活的划分。凡是符合这些特征的IT服务都可以成为云计算服务。——Security Guidance for Critical Areas of Focus In Cloud Computing V3.0” NIST(U.S. National Institute of Standards and Technology)提出了一个定义云计算的标准“NIST Working Definition of Cloud Computing/NIST 800-145”，这个标准中提出云计算具备的五个基本要素：通过网络分发服务、自助服务、可衡量的服务、资源的灵活调度、资源池化。另外，这个标准还提到，云计算按照服务类型可以分为 IaaS、PaaS、SaaS 三类，按照部署模式分为 公有云、私有云、混合云和社区云 四种。 IaaS (Infrastructure as a service) Infrastructure as a service (IaaS) is a cloud computing offering in which a vendor provides users access to computing resources such as servers, storage and networking. Organizations use their own platforms and applications within a service provider’s infrastructure.Examples: DigitalOcean, Linode, Rackspace, Amazon Web Services (AWS), Microsoft Azure, Google Compute Engine (GCE), 阿里云，腾讯云 IaaS 要解决什么问题？ 用户不用购买硬件设备了，直接按需购买资源就行了。 用户可以按需扩容缩容自己所需要的资源。 (业务忙时多申请几个机子，业务不忙了少几个机子，反正时按资源收费的。) 不用安排专人维护硬件设备了，也不会有什么机房断电，设备单点故障了影响业务。 用起来让你感觉跟物理机没啥区别。 PaaS (Platform as a Service) Platform as a service (PaaS) is a cloud computing offering that provides users with a cloud environment in which they can develop, manage and deliver applications. In addition to storage and other computing resources, users are able to use a suite of prebuilt tools to develop, customize and test their own applications.Examples: AWS Elastic Beanstalk, Windows Azure, Heroku, Force.com, Google App Engine, Apache Stratos, OpenShift PaaS 要解决什么问题？云上有了资源（虚拟机）后，提供给用户一套开发，测试，打包，发布，管理和运维的平台，让你很方便的在IaaS上搞你的业务，不用care底层资源 。 SaaS (Software as a Service) Software as a service (SaaS) is a cloud computing offering that provides users with access to a vendor’s cloud-based software. Users do not install applications on their local devices. Instead, the applications reside on a remote cloud network accessed through the web or an API. Through the application, users can store and analyze data and collaborate on projects.Examples: Google Apps, Dropbox, Salesforce, Cisco WebEx, Concur, GoToMeeting SaaS 要解决什么问题？这个我也没接触过，理解就是所有软件都在云上了，你只要类似打开浏览器的东东就可以用各种软件，不用再安装了，你的数据啥的都在云上了。 参考: https://www.ibm.com/cloud/learn/iaas-paas-saashttps://www.computenext.com/blog/when-to-use-saas-paas-and-iaas/https://en.wikipedia.org/wiki/Platform_as_a_servicehttps://www.bmc.com/blogs/saas-vs-paas-vs-iaas-whats-the-difference-and-how-to-choose/ 历史 Docker 的出现 2013年那会云计算不再时虚无缥缈的概念了，已经商业化比较成熟了，AWS如日中天，Openstack火的一塌糊涂，以Cloud Foundry为代表的开源PaaS项目度过了最艰难的的概念普及和用户教育阶段，吸引力一堆知名技术厂商的投入，开始了以开源PaaS为核心构建平台层服务能力的变革，PaaS的时代来临了。 Cloud Foundry is an open source, multi cloud application platform as a service (PaaS) governed by the Cloud Foundry Foundation, a 501(c)(6) organization.The software was originally developed by VMware and then transferred to Pivotal Software, a joint venture by EMC, VMware and General Electric.-- https://en.wikipedia.org/wiki/Cloud_Foundry Docker公司那会还叫dotCloud也是PaaS热潮的中一个小公司，主打产品与主流的Cloud Foundry社区脱节，长期无人问津。 2013年3月，dotCloud公司推出自己的容器开源项目Docker。 2013年10月，dotCloud公司正式转换业务核心并将自身重新定名为Docker。到这时，Docker已经拥有超过200名贡献者，其中九成以上来自公司之外。Docker的下载量超过10万次，包括eBay在内的众多企业开始对其加以利用，相关社区也在全球范围内快速建立。短短几个月Cloud Foundry和其他PaaS社区还没来得及成为它的对手就已经出局了。 dotCloud公司改名为Docker公司。 遇到和解决了什么问题 Cloud Foundry的PaaS解决了什么问题 当时大家租赁AWS或者Openstack的虚机机，还是像以前管理物理机那样，通过脚本或者手动的方式在虚拟机上部署应用。但是云上的资源环境和物理机还是不一致的，当时的云计算服务比的是谁能更好的模拟本地服务器环境，能带来更好的“云上”体验。 PaaS的出现就是解决这个问题的一个最佳方案。举个例子，虚拟机建好后，运维人员只需在这些机子上部署一个Cloud Foundry项目，然后执行一条命令就可以把本地的应用部署到云上。 cf push \"我的应用\" Cloud Foundry这样的PaaS项目最核心的组件就是打包和分发机制。为每种语言都定义一种打包机制，push时相当于把可执行文件和启动脚本打包到一个压缩包里面，上传到云上的存储种，然后调度一个可用的虚机，这个虚机的agent下载应用启动运行。 由于同一个虚机会运行不同的应用，Cloud Foundry调用系统的Cgroups和Namespace机制为每一个应用单独创建一个叫“沙盒”的隔离环境，在“沙盒”里启动这些应用进程，实现一个虚机里面同时运行不同的应用，互不干涉。沙盒也就是后来的容器。 Docker解决了什么问题 Docker发布后，技术上跟Cloud Foundry的沙盒没有啥本质区别，但是最大不同就是容器的镜像。 问题出在Cloud Foundry的一键部署很方便，但是打包却比较麻烦，一旦用上PaaS，用户必须为每一种语言，框架，甚至版本维护一给打好的包，而这个打包过程，除了可执行文件和启动脚本外，需要修改好多配置才能跑起来，这个修改没有经验可用借鉴，全凭试错尝试。 Docker 镜像恰巧解决了这个问题。这个镜像就是一个压缩包，直接由一整套完整的操作系统的文件和目录组成的，有应用所需要的完整的依赖环境，内容可用和测试环境完全一样，所有不需要任何配置和修改直接保证了本地环境和云端环境的高读一致。拿着这个压缩包，使用某种技术创建一个“沙盒”环境，在沙盒里面解压缩，运行程序就可以了。这就是Docker的精髓。 PaaS最核心的打包系统一下子无用武之地了，抓狂的打包过程消失了。 提供一个下载好的操作系统文件和目录，制作一个压缩包 docker build \"镜像\" Docker创建一个“沙盒”，解压压缩包，运行自己的应用。这个沙盒也是通过Cgroups和Namespace技术来实现环境隔离的。 docker run \"镜像\" Docker公司是怎么把Docker搞火的 Docker公司的重要战略是“坚持把开发者群体放在至高无上的位置”，所以一开始Docker的推广是以开发者为主导的，简单的UI，有趣的demo，无论是你懂不懂后端，很简单就可以发布自己的应用，PaaS的受益者和最终用户，肯定都是开发者。Docker只是个开源项目的名称，dotCloud公司将自己的公司名字改为了Docker，鲸鱼的Logo也成为了商业商标。 Docker为啥发布Swarm项目 Docker项目的出现，让PaaS的定义由之前的CLoud Foundry描述的那样变成了一个由Docker镜像为标准的全新的概念。 Docker虽然解决了打包的问题，还不能叫做PaaS，因为PaaS的另一个重要功能是大规模部署。Docker在2014年的DockerCon上推出了自己的容器集群管理项目Swarm，预示着Docker公司想重新定义PaaS的愿望。 Docker的快速崛起后，CoreOS公司快速将容器融入自己的PaaS解决方案中，是当时DOcker项目的第二重要力量，但是随着Docker公司战略和对Docker项目的定位的改变，Docker公司想提供更多平台层的能力，向PaaS项目发展。显然和CoreOS的核心产品和战略冲突，2014年底CoreOS退出Docker项目，发布了自己Rocket(rkt)容器。 CoreOS是一系列开源项目的组合，包括Container Linux操作系统，Fleet作业调度工具，systemd进程管理，rkt容器。Swarm则是以一个完整的整体来对外管理集群，最大亮点是完全使用Docker项目的API来完成管理集群，比如： #单机Docker项目： docker run “我的镜像” #集群Docker项目： docker run -H “Swarm集群IP” “我的镜像” 编排概念的出现 Docker的崛起，2014-2015年催生了一个繁荣的Docker生态，Docker收购了Fig项目，后面改名为Compose项目。 编排在云计算领域是指通过工具或者配置来完成一组虚拟机以及相关资源的定义，配置，创建，删除等工作。 Fig项目首次提出了容器编排概念“Container Orchestration”。通过执行一条简单命令，将一个配置文件里面定义的不同的容器，按照他们的指定关联关系创建起来。 Mesos的转型 Mesos昨晚Berkeley主导的大数据套件之一，是当时大数据最受欢迎的资源管理项目，跟Yarn项目杀的难舍难分。大数据项目关注的是计算密集型离线业务，对应用打包和集群扩容托管没啥强烈需求，Hadoop，Spark等项目到现在也没有在容器上投入更大赌注。 Mesos的两层调度系统，天然可以支持PaaS业务，Mesos+Marathon项目很快成了Docker Swarm的有力竞争对手。Mesos拥有超大规模集群管理的经验，也有大规模生产环境在使用了，比如eBa，。Marathon提供了应用托管和负载均功能。Mesos公司提出了DC/OS的口号和产品，旨在使用户能够像管理一台机器一样管理一个万级别的物理集群，并且使用Docker容器在这个集群中自由部署应用。 Kubernetes的诞生 这个时候CoreOS完全被Docker压制，RedHat作为Docker项目早期重要贡献者也因为Docker公司的平台战略不满退出，OpenSift还勉强支撑，Mesos和Swarm是主要竞争对手。2014年6月Google发力，发布了Kubernetes项目，这个项目不仅挽救了CoreOS和RedHat，同时改变了整个容器市场的格局。 2014-2015年，整个容器社区热闹非凡，大量围绕Docker项目的网络，存储，监控，CI/CD，UI项目纷纷出台，也涌现出了Rancher，Tutun开源和商业上都取得成功的创业公司。Docker公司发布了Compose，Swarm和Machine三件套，Docker公司想从开源成功走向商业成功。 Google的容器项目也招架住Docker，Google本想提议关停自己容器项目，和Docker共同推出了一个中立的容器运行时库（container runtime）作为Docker项目的核心依赖。DOcker没有同意削弱自己地位的建议，推出了自己的容器运行时库Libcontainer。由于比较匆忙，代码可读性差，可维护不强，被社区长期诟病。 2015年6月，各个玩家开始切割Docker项目的话语权，手段也很经典。由Docker牵头，CoreOS，Google，RedHat等公司宣布，Docker将LibContainer捐出，改名RunC项目，交由一个完全中立的基金会管理，然后以RunC为依据，共同指定一套容器和镜像的标准和规范。 这套标准规范就是OCI （open Container Initiative）。提出将容器运行和镜像的实现从Dokcer项目完全剥离出来，一方面改善了Docker在这一块一家独大的现状，另一方面各个玩家可以不依赖Docker项目构建各自的平台能力。这是一群玩家根据各自利益干涉的一个妥协结果。Docker公司虽然时OCI的发起者和创始成员，但是很少在标准指定上扮演关键角色，也没有动力去推进这些所谓的标准，这就是为啥OCI组织效率持续低下的根本原因。 Docker不担心OCI的威胁，是因为Docker项目的容器生态的事实标准，社区足够庞大。但是斗争转移到容器之上的平台层，即PaaS，Docker公司就没有多大优势了。这个领域Google和RedHat有着深厚的技术积累，CoreOS这样的创业公司也有像Etcd这样的开源基础设施项目，Docker只有一个Swarm。 Google，RedHat等开源基础设施玩家，共同牵头发起了一个CNCF（Cloud Native Computing Foundation）的基金会。目的就是希望以Kubernetes项目为基础，建立一个由开源基础设施厂商主导的，按照独立基金会运行的平台级社区，对抗Docker公司为核心的容器商业生态。 Kubernetes的竞争对手为Swarm和Mesos，Swarm擅长和Docker生态无缝集成，Mesos擅长大规模集群的调度和管理。Kubernetes另开辟径，将Borg和Omega系统的内部特性落到Kubernetes项目上，就是Pod，Sidecar等超前的功能和设计模式。这些都是Google公司在这个领域多年的经验积累沉淀。RedHat和Google达成联盟，为这个项目投入了很多贡献，保证了自己的影响力。 Mesos的Apache社区比较封闭，虽然成熟，但是缺乏创新，Swarm虽然强调Docker Native，但是杀伤力不大。Kubernetes项目耳目一新的设计理念和号召力，构建出了一个与众不同的容器编排和管理生态，迅速崛起，Github社区各项指标一骑绝尘，Swarm远远被甩身后。 CNCF社区添加了像Prometheus，Fluentd，CNI等一系列容器知名生态工具和项目，大量的公司和创业团队开始专门针对CNCF社区而非Docker公司制定推广策略。 Docker为了应对竞争，2016年宣布放弃Swarm项目，将容器编排和集群管理功能内置到Docker项目中，虽然可以使得Docker项目的边界扩大到一个完整的PaaS项目范畴，但是增加了技术复杂度和维护难度。 Kubernetes反其道行之，在社区推进“民主化”架构，从API到容器运行时的每一层，都为开发者提供可扩展的插件机制，鼓励通过代码方式介入到Kubernetes的每一个阶段。催生了大量基于Kubernetes API和扩展接口的二次创业工作，社区在2016年之后得到了空前的发展。不同于之前局限于打包发布的PaaS化路线，这一次是围绕Kubernetes项目为核心的百花争鸣。Docker公司不得不面对这次豪赌的失败，开始放弃开源社区，专注于自己的商业化转型。 2017年开始，Docker公司将DOcker项目的容器运行时部分Containerd捐赠给了CNCF社区，标志着Docker项目完全升级为一个PaaS平台，Docker宣布将Docker项目改名为Moby，交给社区自行维护。 2017年10月，Docker宣布在自己主打产品Docker企业版内置Kubernetes项目。 2018年1月20日，RedHat收购CoreOS。 2018年3月28日，Docker公司的CTO Solomon Hykes 宣布辞职。 后面这段历史原作者将的酣畅淋漓，基本照搬过来的。大家自行体会这几年这个领域的风云变化和各大玩家的角逐。 总结 容器技术兴起源于PaaS技术的普及 Docker项目通过“容器镜像”，解决了应用打包这个根本性难题 容器本身没有价值，有价值的是“容器编排” powered by GitbookUpdated: 2019-01-21 17:23:25 "},"content/kubernetes.html":{"url":"content/kubernetes.html","title":"Kubernetes","keywords":"","body":"容器的本质 容器实际上是一个Linux Namespace、Linux Cgroups和rootfs三种技术构造出来的进程的隔离环境。 容器的静态试图：一组联合挂载在/var/lib/docker/...上的rootfs，即“容器镜像”Container Image。 容器的动态试图：一个由Namespace和Cgroup构成的隔离环境，即“容器运行时”Container Runtime。 作为一个开发者，不关心运行时，容器镜像才是真正承载容器信息进程传递的。容器编排由此出现，容器走向容器云。 Kubernetes的本质 kubernetes 架构 Kubernetes项目的理论基础要比工程实践走得靠前很多，归功于Google2015年4越发布的Borg论文。Borg承载了Google公司整个基础设施的核心依赖，在整个基础设施技术栈的最底层。 图片来源：http://malteschwarzkopf.de/research/assets/google-stack.pdf Kubernetes跟Borg非常相似，由Master+Node组成。Master控制节点： kube-apiserver 负责API服务 kube-scheduler 负责调度 kube-controller-manager 负责容器编排 整个集群数据持久化由kube-apiserver处理后保存在Etcd中。 Node计算节点最核心组件kubelet，负责同容器运行时（比如docker项目）打交道。 CRI （Container Runtime Interface）定义了容器运行时各种核心操作，比如启动一个容器需要的所有参数等。只要符合这个容器运行时能够运行标准的容器镜像，都可以通过CRI接入到Kubernetes项目。 OCI 容器运行时规范同底层的Linux操作系统进行交互，把CRI 请求翻译成Linux 操作系统的调用（操作Namespace和Cgroups等） CNI （Container Networking Interface） 网络插件。 CSI （Container Storage Interface） 存储插件。 Kubelet通过gRPC协议同一个Device Plugin的插件进行交互，这个插件时Kubernetes项目用来管理GPU等宿主机物理设备的主要组件，也是基于Kubernetes项目进行机器学习等工作关注的功能。 Kubernetes 概念 Docker Swarm + Compose项目是通过“link”，来解决多个容器的关联关系的。 Docker会在不同容器内以环境变量注入的方式传递IP，端口信息来实现多个容器的关联。 Kubernetes是用“Pod”来共享Network Namespace、同一组数据卷，从而达到高效交互信息的目的。通过“Service”的服务来给应用提供访问入口。 Pod 绑定 Service服务，Service服务声明IP地址等信息，作为Pod的代理入口Portal，替代Pod对外暴露一个固定的网络地址。 Kubernetes围绕Pod不断扩展： Deployment Pod的多实例管理器。 Secret Credential信息以Secret的方式存入Etcd，启动Pod时挂载进容器。 Job 只运行一次性的Pod。 DaemonSet 每个宿主机只能运行一个副本的守护进程服务。 CronJob 定时任务。 Kubernetes项目中，推崇使用“声明式API”的方式： 通过一个编排对象，比如Pod，Job等来描述管理的应用。 通过服务对象，比如Service，Secret等负责具体平台级功能。 编排对象和服务对象都是API对象。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/install.html":{"url":"docker/install.html","title":"Install","keywords":"","body":"Install Docker docker-io 是以前早期的版本，版本号是 1.，而 docker-ce 是新的版本，分为社区版 docker-ce 和企业版 docker-ee，版本号是 17. 。 Docker CE 17.03，可理解为Docker 1.13.1的Bug修复版本。因此，从Docker 1.13升级到Docker CE 17.03风险相对是较小的。 官网install install docker on Ubuntu https://docs.docker.com/install/linux/docker-ce/ubuntu/#install-docker-ce-1 install docker on Centos https://docs.docker.com/install/linux/docker-ce/centos/#install-docker-ce-1 [root@master]# yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 [root@master]# yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo Loaded plugins: fastestmirror adding repo from: https://download.docker.com/linux/centos/docker-ce.repo grabbing file https://download.docker.com/linux/centos/docker-ce.repo to /etc/yum.repos.d/docker-ce.repo Could not fetch/save url https://download.docker.com/linux/centos/docker-ce.repo to file /etc/yum.repos.d/docker-ce.repo: [Errno 12] Timeout on https://download.docker.com/linux/centos/docker-ce.repo: (28, 'Operation timed out after 30001 milliseconds with 0 out of 0 bytes received') Note: 由于有GW的缘故，一般都会install 失败。 其他源install 清华源 install https://mirror.tuna.tsinghua.edu.cn/help/docker-ce/ aliyuan install https://blog.csdn.net/doegoo/article/details/80062132 [root@master]# sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo Docker Registry aliyuan registry https://cr.console.aliyun.com/?spm=a2c4e.11153940.blogcont29941.9.4cc569d6IVqmDa#/accelerator [root@master]# sudo mkdir -p /etc/docker [root@master]# sudo tee /etc/docker/daemon.json Docker Hub cn [root@master]# sudo tee /etc/docker/daemon.json Private Registry [root@master]# cat /etc/docker/daemon.json { \"registry-mirrors\": [\"https://registry.docker-cn.com\"], \"insecure-registries\": [\"192.0.0.0/8\"] } # or [root@master]# cat /usr/lib/systemd/system/docker.service ExecStart=/usr/bin/dockerd --insecure-registry 192.168.137.100:30060 Issues Issue 1 [root@master]# yum install docker-ce-17.03.1.ce Error: Package: docker-ce-17.03.1.ce-1.el7.centos.x86_64 (docker-ce-stable) Requires: docker-ce-selinux >= 17.03.1.ce-1.el7.centos Available: docker-ce-selinux-17.03.0.ce-1.el7.centos.noarch (docker-ce-stable) docker-ce-selinux = 17.03.0.ce-1.el7.centos Available: docker-ce-selinux-17.03.1.ce-1.el7.centos.noarch (docker-ce-stable) docker-ce-selinux = 17.03.1.ce-1.el7.centos Available: docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch (docker-ce-stable) docker-ce-selinux = 17.03.2.ce-1.el7.centos You could try using --skip-broken to work around the problem You could try running: rpm -Va --nofiles --nodigest 解决： 需要先安装： yum install http://mirrors.aliyun.com/docker-ce/linux/centos/7/x86_64/stable/Packages/docker-ce-selinux-17.03.2.ce-1.el7.centos.noarch.rpm 再安装： yum install docker-ce-17.03.2.ce powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/command.html":{"url":"docker/command.html","title":"Docker Commands","keywords":"","body":"Docker Dommands docker images https://docs.docker.com/v1.11/engine/reference/commandline/images/ [root@yzw kolla]# docker images -f \"label=kolla_version\" --format \"{{.Repository}}:{{.Tag}}\" | grep -E source-base kolla-test/centos-source-base:1.1.1 [root@yzw kolla]# [root@yzw kolla]# docker images -f \"label=kolla_version\" -q 7cff68b428c8 f8140ec6ff6d [root@yzw kolla]# docker images --format \"table {{.Repository}}\\t{{.ID}}\\t{{.Tag}}\" REPOSITORY IMAGE ID TAG kolla-test/centos-source-neutron-server-opendaylight 7cff68b428c8 1.1.1 kolla-test/centos-source-neutron-server-ovn f8140ec6ff6d 1.1.1 docker system https://docs.docker.com/engine/reference/commandline/system/#description root@yzw-vm:/home/yzw# docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 2 0 131.3MB 131.3MB (100%) Containers 0 0 0B 0B Local Volumes 4 0 19.85MB 19.85MB (100%) Build Cache 0 0 0B 0B root@yzw-vm:/home/yzw# docker system prune --help Usage: docker system prune [OPTIONS] Remove unused data Options: -a, --all Remove all unused images not just dangling ones --filter filter Provide filter values (e.g. 'label==') -f, --force Do not prompt for confirmation --volumes Prune volumes #running的容器的镜像不会被删，stop的容器和镜像一块被删 [root@kolla ~]# docker system prune -af --filter 'label=kolla_version=5.0.4' Deleted Containers: 6d597715565466e545c27e23a65da172bb74a909e95f747452d748d7cf072714 ... 镜像叫悬挂 dangling 镜像 APP Commands yum server docker run --name nginx_yum -v /data/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v /data/yum:/opt:ro -p 8082:80 --restart always -d nginx file server docker run --name nginx_file -v /data/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v /data/file/:/opt:ro -p 8083:80 --restart always -d nginx pip server docker run --name pypiserver -v /data/pip/:/packages -p 8084:3141 --restart always -d pypiserver docker registry docker run -d -p 80:5000 -v /media/sf_share/registry:/var/lib/registry --restart always --name registry registry:latest curl -X GET http://localhost:80/v2/_catalog docker run -d --name registry_frontend -e ENV_DOCKER_REGISTRY_HOST=172.17.0.3 -e ENV_DOCKER_REGISTRY_PORT=5000 -p 8081:80 --restart always konradkleine/docker-registry-frontend:v2 docker run -it -d -p 8081:8080 --name registry_web --link registry -e REGISTRY_URL=http://registry:5000/v2 -e REGISTRY_NAME=localhost:5000 hyper/docker-registry-web docker run --name nginx_web -v /media/sf_share/nginx/nginx.conf:/etc/nginx/nginx.conf:ro -v /media/sf_share/nginx/web:/opt:ro -p 8080:80 --restart always -d nginx nginx 访问403 原因目录没有权限 /etc/fstab share /media/sf_share vboxsf defaults 0 0 jenkins docker run -u root -d -p 8080:8080 -p 50000:50000 -v /srv/jenkins:/var/jenkins_home -v /var/run/docker.sock:/var/run/docker.sock --env JAVA_OPTS=\"-Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Shanghai\" --name jenkins --restart always jenkinsci/blueocean gitlab docker run --detach --hostname 192.168.137.102 --publish 20443:443 --publish 20080:80 --publish 20022:22 --name gitlab --restart always --volume /srv/gitlab/config:/etc/gitlab --volume /srv/gitlab/logs:/var/log/gitlab --volume /srv/gitlab/data:/var/opt/gitlab gitlab/gitlab-ce 修改配置文件 /srv/gitlab/config/gitlab.rb external_url 'http://192.168.137.102:20080' nginx['listen_port'] = 80 gitlab_rails['gitlab_shell_ssh_port'] = 20022 gitlab_rails['gitlab_ssh_host'] = '192.168.137.102' docker exec -it gitlab /bin/bash gitlab-ctl reconfigure gitlab-ctl restart powered by GitbookUpdated: 2019-01-25 09:23:19 "},"docker/cgroups-namepspace.html":{"url":"docker/cgroups-namepspace.html","title":"Namespace && Cgroup","keywords":"","body":"概述 容器其实就是一种沙盒技术的，沙盒就像集装箱一样，把应用装起来，应用于应用之间，有了边界，互不干扰，而且方便被搬来搬去，这就是PaaS最理想的状态。 程序在操作系统的表现就是进程，容器技术的核心功能就是通过约束和修改进程的动态表现，创造出一个“边界”。Linux操作系统中，Cgroups技术是用来制造约束的手段，Namespace技术是用来修改进程视图的主要方法。 隔离技术（Namespace） 举个例子：运行busybox容器，执行/bin/sh，在跑个sleep。 $ docker run -it busybox /bin/sh / # / # ps PID USER TIME COMMAND 1 root 0:00 /bin/sh 7 root 0:00 ps / # / # sleep 100 & / # ps PID USER TIME COMMAND 1 root 0:00 /bin/sh 9 root 0:00 sleep 100 10 root 0:00 ps / # 宿主机上的进程 [root@yzw zhiweiyin]# ps auxf USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1122 0.3 1.7 729148 66444 ? Ssl 08:14 0:10 /usr/bin/dockerd root 1318 0.5 0.7 574660 30976 ? Ssl 08:14 0:16 \\_ docker-containerd --config /var/run/docker/containerd/containerd.toml root 4511 0.0 0.0 7488 3156 ? Sl 08:58 0:00 \\_ docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby root 4529 0.0 0.0 1252 264 pts/0 Ss 08:58 0:00 \\_ /bin/sh root 4671 0.0 0.0 1232 4 pts/0 S+ 09:01 0:00 \\_ sleep 100 容器里面sh的进程号是1，但是在宿主机上进程号是4529，且是docker-containerd-shim的子进程，sleep也是类似，这就是Namespace机制，容器里面屏蔽掉了其他进程和进程号。 Linux上创建线程是用clone,返回一个PID号。 int pid = clone(main_function,stack_size,SIGCHID,NULL) 当我们加上CLONE_NEWPID参数后，新创建的这个进程就会“看到”是一个全新的进程空间，在这个进程空间里PID是1。但是在主机上仍然是另外的PID号。多次执行clone会创建多个PID Namespace出来，每个进程里面PID都为1，看不到真正的宿主机上的PID，也看不到其他PID Namespace里的情况。 int pid = clone(main_function,stack_size,CLONE_NEWPID|SIGCHID,NULL) Mount Namespace 用于被隔离进程只看到当前Namespace里的挂载点信息。 Network Namespace 用于被隔离进程只看到当前Namespace里面的网络设备和配置。 这就是Linux容器实现的原理。创建容器进程时，指定了这个进程所需要启动的一组Namespace参数，这样容器就只能“看”到当前容器Namespace所限定的资源、文件、设置、状态、或者配置了，对于宿主机和其他不相干的进程都看不到了。容器其实是一种特殊的进程而已。 虚拟机 vs. 容器 先看两张图： 左边是虚拟机的工作原理，Hypervisor的软件通过硬件的虚拟化功能，模拟出了运行一个操作系统所以需要的各种硬件，然后在模拟的硬件上按照一个操作系统 Guest OS，软件进程跑在这个Guest OS里，也就只能看到这个OS里面的文件和目录，以及虚拟设备。所以虚拟机也能隔离作用，但是开销太大，太重了，一个最小的CentOS KVM虚机启动后差不多需要100-200M内存，所有对资源的使用都得经过序集合软件，又是一层损耗。 右边是容器的工作原理，Docker的位置是个应用一个级别的，和宿主机上其他应用进程是相同级别的，由宿主机统一管理，只不过被隔离的进程，额外设置了Namespace参数，Docker扮演的角色是辅助和管理，和Hypervisor完全不同。容器相对于虚机机来说几乎无损耗，也无需单独的Guest OS。但是容器也有弊端，所有容器之间隔离的不彻底，所有容器共用操作系统内核，内核有很多资源是不能被Namespace化的，比如时间。虚机机里面就可以随便这套了。所有应用容器化后什么可以做什么不可以做是需要考虑的一个问题，特别是对系统调用的使用上。 资源限制技术 Cgroups 通过Namespace技术虽然实现了容器的资源隔离，但是这个进程还是在宿主机上跑的，和其他进程共享CPU和内存的，有可能把宿主机上的CPU和内存吃完，Cgroups就是用来对进程设置资源限制的主要功能。 Linux Cgroups 全称 Linux Control Group，限制一个进程使用的资源上限，包括CPU，内存，磁盘，网络带宽等。还可以对进程进行优先级设置，审计，将进程挂起和恢复等操作。 Cgroups 给用户暴露出来的操作接口是文件系统，在/sys/fs/cgroup路径下，包括了各种限制资源类。 [root@yzw ~]# mount -t cgroup cgroup on /sys/fs/cgroup/systemd type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,xattr,release_agent=/usr/lib/systemd/systemd-cgroups-agent,name=systemd) cgroup on /sys/fs/cgroup/hugetlb type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,hugetlb) cgroup on /sys/fs/cgroup/freezer type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,freezer) cgroup on /sys/fs/cgroup/pids type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,pids) cgroup on /sys/fs/cgroup/cpu,cpuacct type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuacct,cpu) cgroup on /sys/fs/cgroup/blkio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,blkio) cgroup on /sys/fs/cgroup/devices type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,devices) cgroup on /sys/fs/cgroup/cpuset type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,cpuset) cgroup on /sys/fs/cgroup/net_cls,net_prio type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,net_prio,net_cls) cgroup on /sys/fs/cgroup/memory type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,memory) cgroup on /sys/fs/cgroup/perf_event type cgroup (rw,nosuid,nodev,noexec,relatime,seclabel,perf_event) [root@yzw cgroup]# ll /sys/fs/cgroup/ total 0 drwxr-xr-x. 5 root root 0 Sep 10 22:48 blkio lrwxrwxrwx. 1 root root 11 Sep 10 22:48 cpu -> cpu,cpuacct lrwxrwxrwx. 1 root root 11 Sep 10 22:48 cpuacct -> cpu,cpuacct drwxr-xr-x. 5 root root 0 Sep 10 22:48 cpu,cpuacct drwxr-xr-x. 3 root root 0 Sep 10 22:48 cpuset drwxr-xr-x. 5 root root 0 Sep 10 22:48 devices drwxr-xr-x. 3 root root 0 Sep 10 22:48 freezer drwxr-xr-x. 3 root root 0 Sep 10 22:48 hugetlb drwxr-xr-x. 5 root root 0 Sep 10 22:48 memory lrwxrwxrwx. 1 root root 16 Sep 10 22:48 net_cls -> net_cls,net_prio drwxr-xr-x. 3 root root 0 Sep 10 22:48 net_cls,net_prio lrwxrwxrwx. 1 root root 16 Sep 10 22:48 net_prio -> net_cls,net_prio drwxr-xr-x. 3 root root 0 Sep 10 22:48 perf_event drwxr-xr-x. 5 root root 0 Sep 10 22:48 pids drwxr-xr-x. 5 root root 0 Sep 10 22:48 systemd 举个例子：cfs_period和cfs_quota组合使用，限制进程在cfs_period的一段时间内，只能被分到总量为cfs_quota的CPU时间。在/sys/fs/cgroup/cpu目录下创建container文件夹，系统自动创建了一堆资源限制文件。这个目录就成为一个“控制组”。 [root@yzw cpu]# mkdir container [root@yzw cpu]# ls container/ cgroup.clone_children cgroup.procs cpuacct.usage cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_release cgroup.event_control cpuacct.stat cpuacct.usage_percpu cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 我们后台执行个死循环,top发现CPU1被吃完了，container控制组quota没有限制，period是100ms（100000us）。我们修改配置，quota写入20ms就是说100ms该控制组的进程只能使用20ms的CPU，这个进程只使用20%的CPU带宽，我们把限制的进程PID写入该控制组。再次top发现CPU1的使用率降下来了。 [root@yzw cpu]# while :; do : ;done & [1] 3592 [root@yzw cpu]# top %Cpu1 :100.0 us, 0.0 sy, 0.0 ni, 0.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st [root@yzw container]# cat cpu.cfs_period_us 100000 [root@yzw container]# cat cpu.cfs_quota_us -1 [root@yzw container]# echo 20000 > cpu.cfs_quota_us [root@yzw container]# echo 3592 > tasks [root@yzw container]# top %Cpu1 : 17.4 us, 0.0 sy, 0.0 ni, 82.6 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st Cgroups的设计比较简单粗暴，就是一个子系统目录加上一组资源限制文件的组合。Docker等容器项目来说就是在各个子系统下，为每个容器创建一个控制组（就是新建个目录），然后启动容器进程后，把容器进程的PID写入对应控制组的tasks文件中就可以了。 [root@yzw docker]# docker run -it --cpu-period=100000 --cpu-quota=20000 centos:7 /bin/bash [root@90c9fd20daf8 /]# [root@yzw ~]# cd /sys/fs/cgroup/cpu/docker/90c9fd20daf8f4f9ad384d349d37ca1542490d78daf4e3fe6b79f3ad6d7179bd/ [root@yzw 90c9fd20daf8f4f9ad384d349d37ca1542490d78daf4e3fe6b79f3ad6d7179bd]# cat cpu.cfs_quota_us 20000 [root@yzw 90c9fd20daf8f4f9ad384d349d37ca1542490d78daf4e3fe6b79f3ad6d7179bd]# cat cpu.cfs_period_us 100000 [root@yzw 90c9fd20daf8f4f9ad384d349d37ca1542490d78daf4e3fe6b79f3ad6d7179bd]# 总结 容器就是一个启动了多个Namespace的应用进程，这个进程能够使用的资源量，通过Cgroups配置限制。 容器技术中一个非常重要的概念，容器是一个单进程模型。 用户的应用进程就是容器里面PID=1的进程，其他后续创建的进程都是这个进程的子进程，意味着你没法运行两个不同的应用，除非找到一个公共的PID=1的程序来充当两个不同程序的父进程。好多人使用systemd或者superviord来充当容器的启动进程。 容器的设计希望容器和进程应用同生命周期，对后续编排很重要，如果容器是正常运行的，里面的应用挂了，处理起来就很麻烦。 issue 跟Namespace一样，Cgroups也有缺陷，提起最多的就是/proc文件系统的问题。/proc存储的是内核运行状态的一系列特殊文件，用户可以访问这些文件查看当前运行的进程的状态，比如CPU使用情况，内存使用情况，这些是top的主要数据来源。但是/proc不了解Cgroups的存在，就是容器里面读取的CPU核数，内存状态其实是宿主机的，会带来很多困惑和风险。lxcfs可以增强docker资源的可见性，可解决这个问题。做法是把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。从而使得应用获得正确的资源约束设定。kubernetes环境下，也能用，以ds 方式运行 lxcfs ，自动给容器注入争取的 proc 信息。 /proc文件系统的问题我好像遇到过这个坑..当时在容器上运行的java应用，由于当时jvm参数没正确配置上，就用默认的，而容器设置的内存为4g，最后oom了，当时用命令查看容器的内存占用情况，竟然发现内存竟然有60多g。 那应该显示的是宿主机的内存了，jvm按照宿主机内存大小分配的默认内存应该大于4g 所以还没full gc 就oom了 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/namespace.html":{"url":"docker/namespace.html","title":"Namespace","keywords":"","body":"Namespace Reference:http://man7.org/linux/man-pages/man7/namespaces.7.htmlhttps://lwn.net/Articles/531114/ Namespace是对全局系统资源的一种封装隔离，使得处于不同namespace的进程拥有独立的全局系统资源，改变一个namespace中的系统资源只会影响当前namespace里的进程，对其他namespace中的进程没有影响。 现在Linux支持的namespace有： Namespace Constant Isolates IPC CLONE_NEWIPC System V IPC, POSIX message queues Network CLONE_NEWNET Network devices, stacks, ports, etc. Mount CLONE_NEWNS Mount points PID CLONE_NEWPID Process IDs User CLONE_NEWUSER User and group IDs UTS CLONE_NEWUTS Hostname and NIS domain name 查询当前进程使用的namespace，都是以文件形式存在： yzw@yzw-vm:~$ sudo ls -l /proc/$$/ns total 0 lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 cgroup -> 'cgroup:[4026531835]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 ipc -> 'ipc:[4026531839]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 mnt -> 'mnt:[4026531840]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 net -> 'net:[4026531992]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 pid -> 'pid:[4026531836]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 pid_for_children -> 'pid:[4026531836]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 user -> 'user:[4026531837]' lrwxrwxrwx 1 yzw yzw 0 Dec 28 14:56 uts -> 'uts:[4026531838]' 每个Namespace的个数限制： yzw@yzw-vm:~$ ls -l /proc/sys/user/ total 0 -rw-r--r-- 1 root root 0 Dec 28 15:14 max_cgroup_namespaces -rw-r--r-- 1 root root 0 Dec 28 15:14 max_inotify_instances -rw-r--r-- 1 root root 0 Dec 28 15:14 max_inotify_watches -rw-r--r-- 1 root root 0 Dec 28 15:14 max_ipc_namespaces -rw-r--r-- 1 root root 0 Dec 28 15:14 max_mnt_namespaces -rw-r--r-- 1 root root 0 Dec 28 15:14 max_net_namespaces -rw-r--r-- 1 root root 0 Dec 28 15:14 max_pid_namespaces -rw-r--r-- 1 root root 0 Dec 28 15:14 max_user_namespaces -rw-r--r-- 1 root root 0 Dec 28 15:14 max_uts_namespaces namespace的API： clone创建新的进程，通过flag参数CLONE_NEW*(CLONE_NEWNET,CLONE_NEWIPC,CLONE_NEWCGROUP )创建新的ns，并将新的进程加入ns，当前进程保持不变。 setns将进程加入到已有的ns unshare将当前进程退出指定类型ns，加入到新创建的ns usenter在ns里面运行程序 当一个namespace的所有进程都退出，这个namespace就会被销毁。 Example setns Reference:http://man7.org/linux/man-pages/man2/setns.2.html int setns(int fd, int nstype)fd： namespace的文件句柄 即：/proc/[pid]/ns/ 中的ns文件。nstype：0，允许所有类型ns加入，CLONE_NEW* fd必须为指定类型ns。 #define _GNU_SOURCE #include #include #include #include #include #define errExit(msg) do { perror(msg);exit(EXIT_FAILURE);} while(0) int main(int argc, char *argv[]){ int fd; fd =open(argv[1],O_RDONLY); if (setns(fd,0) == -1){ errExit(\"setns\"); } execvp(argv[2],&argv[2]); errExit(\"execvp\"); } 将指定程序的进程加入指定ns root@yzw-vm:/home/yzw/docker# gcc -o setnc setns.c root@yzw-vm:/home/yzw/code/ns# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES c17865d18a7c centos:7.4.1708 \"/bin/bash\" 2 hours ago Up 2 hours upbeat_jennings root@yzw-vm:/home/yzw/code/ns# docker inspect --format ‘{{.State.Pid}}’ c17865d18a7c ‘3807’ root@yzw-vm:/home/yzw/code/ns# root@yzw-vm:/home/yzw/code/ns# ./setns /proc/3807/ns/mnt /bin/bash [root@yzw-vm /]# ls anaconda-post.log bin dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var [root@yzw-vm /]# root@yzw-vm:/home/yzw/code/ns# ./setns /proc/3807/ns/uts /bin/bash root@c17865d18a7c:/home/yzw/code/ns# Example unshare # 创建新的uts的ns，保存到uts-ns，运行hostname FOO 命令修改hostname root@yzw-vm:/home/yzw/code# touch uts-ns root@yzw-vm:/home/yzw/code# unshare --uts=/root/uts-ns hostname FOO root@yzw-vm:/home/yzw/code# hostname yzw-vm # 进入ns执行hostname命令 root@yzw-vm:/home/yzw/code# nsenter --uts=./uts-ns hostname FOO #或者进入ns执行bash，查看hostname root@yzw-vm:/home/yzw/code# nsenter --uts=./uts-ns /bin/bash root@FOO:/home/yzw/code# hostname FOO powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/exec-and-volume.html":{"url":"docker/exec-and-volume.html","title":"Docker exec and Docker volume","keywords":"","body":"docker exec 是怎么进入容器的？ 容器进程python2，docker exec又重新拉起来了进程跟python2进程都是docker-containerd-shim的子进程。docker exec又是怎么进入 python2进程的Namespace的呢? root@yzw-vm:/home/yzw/docker# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 9e4bdd819dc0 python:2.7-slim \"python2\" 3 seconds ago Up 1 second flamboyant_darwin root@yzw-vm:/home/yzw/docker# ps auxf ... root 1488 0.3 1.9 1405040 78288 ? Ssl 11:53 2:20 /usr/bin/dockerd -H fd:// root 1743 0.2 0.8 1319652 35928 ? Ssl 11:53 1:30 \\_ docker-containerd --config /var/run/docker/containerd/containerd.toml root 11223 0.0 0.1 7500 4080 ? Sl 23:07 0:00 \\_ docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/9e4bdd819dc0b462eb root 11243 0.0 0.1 30084 7764 pts/0 Ss+ 23:07 0:00 \\_ python2 root@yzw-vm:/home/yzw/docker# docker exec -it 9e4bdd819dc0 /bin/bash root@9e4bdd819dc0:/# root@yzw-vm:/home/yzw/docker# ps auxf ... root 1488 0.3 1.9 1405040 78208 ? Ssl 11:53 2:21 /usr/bin/dockerd -H fd:// root 1743 0.2 0.8 1319652 35928 ? Ssl 11:53 1:31 \\_ docker-containerd --config /var/run/docker/containerd/containerd.toml root 11223 0.0 0.0 7500 4020 ? Sl 23:07 0:00 \\_ docker-containerd-shim -namespace moby -workdir /var/lib/docker/containerd/daemon/io.containerd.runtime.v1.linux/moby/9e4bdd819dc0b462eb root 11243 0.0 0.1 30084 7764 pts/0 Ss+ 23:07 0:00 \\_ python2 root 11367 0.0 0.0 19952 3444 pts/1 Ss+ 23:12 0:00 \\_ /bin/bash Linux Namespace 创建了一个隔离的空间，但是一个进程的Namespace在宿主机上是以文件形式存在着的，进程的每一个namespace都链接到一个真是的文件。 一个进程，可以加入到某个进程已有的Namespace当中，达到进入这个进程所在容器的目的，就是exec的实现原理。 root@yzw-vm:/home/yzw/docker# docker inspect --format ‘{{.State.Pid}}’ 9e4bdd819dc0 ‘11243’ root@yzw-vm:/home/yzw/docker# ls -l /proc/11243/ns total 0 lrwxrwxrwx 1 root root 0 Sep 12 23:25 cgroup -> 'cgroup:[4026531835]' lrwxrwxrwx 1 root root 0 Sep 12 23:12 ipc -> 'ipc:[4026532239]' lrwxrwxrwx 1 root root 0 Sep 12 23:12 mnt -> 'mnt:[4026532237]' lrwxrwxrwx 1 root root 0 Sep 12 23:07 net -> 'net:[4026532242]' lrwxrwxrwx 1 root root 0 Sep 12 23:12 pid -> 'pid:[4026532240]' lrwxrwxrwx 1 root root 0 Sep 12 23:25 pid_for_children -> 'pid:[4026532240]' lrwxrwxrwx 1 root root 0 Sep 12 23:25 user -> 'user:[4026531837]' lrwxrwxrwx 1 root root 0 Sep 12 23:12 uts -> 'uts:[4026532238]' 这个操作依赖一个setns()的系统调用。将需要进入的Namespace的文件描述符fd交给setns()，就可以将当前进程加入到这个Namespace里面了。下面小程序传入2个参数，一个是Namespace的文件名，第二个是执行的程序。 #define _GNU_SOURCE #include #include #include #include #include #define errExit(msg) do { perror(msg);exit(EXIT_FAILURE);} while(0) int main(int argc, char *argv[]){ int fd; fd =open(argv[1],O_RDONLY); if (setns(fd,0) == -1){ errExit(\"setns\"); } execvp(argv[2],&argv[2]); errExit(\"execvp\"); } 编译后，传入mnt的namespace文件。 root@yzw-vm:/home/yzw/docker# gcc -o setnc setns.c root@yzw-vm:/home/yzw/docker# docker exec -it 9e4bdd819dc0 /bin/bash root@9e4bdd819dc0:/# cd /home/ root@9e4bdd819dc0:/home# ls root@9e4bdd819dc0:/home# touch abc root@9e4bdd819dc0:/home# ls abc root@yzw-vm:/home/yzw/docker# ./setnc /proc/11243/ns/mnt /bin/bash root@yzw-vm:/# ls bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@yzw-vm:/# ls /home/ abc root@yzw-vm:/# 查下新启的bin/bash的进程的PID的mnt Namespace，和python2进程的一样。 root@yzw-vm:/home/yzw/docker# ps axuf | grep /bin/bash root 11863 0.0 0.0 18204 3156 pts/4 S+ 23:57 0:00 | \\_ /bin/bash root 11866 0.0 0.0 21536 1084 pts/5 S+ 23:57 0:00 \\_ grep --color=auto /bin/bash root@yzw-vm:/home/yzw/docker# ls -l /proc/11863/ns/mnt lrwxrwxrwx 1 root root 0 Sep 12 23:57 /proc/11863/ns/mnt -> 'mnt:[4026532237]' docker volume 是怎么实现的？ docker volume又2种声明方式,比如把宿主机目录挂进容器/test目录： # 在宿主机/var/lib/docker/volumes/xxx/_data 挂到容器/test目录 $ docker run -v /test ... # 把宿主机/home目录挂到容器/test目录 $ docker run -v /home:/test ... docker怎么把宿主机上的目录挂到容器里面的？ 前面namespace总结中，当容器进程（dockerinit 容器初始化进程）被创建之后，尽管开启了Mount Namespace，但是它执行chroot或者pivot_root之前，容器进程一直可以看到宿主机上整个文件系统。宿主机上的容器镜像的各个层，在容器进程启动后，就会被联合挂载到/var/lib/docker/overlay2/xxx/merged目录中，这样容器的rootfs就准备好了。 容器启动进程dockerinit，而不是应用进程ENTRYPOINT+CMD,dockerinit负责完成根目录准备，挂载设备目录配置hostname一系列初始化工作，然后通过execv()，让应用程序取代自己，成为PID=1的进程。 只需要在rootfs准备好之后，chroot之前，把volume指定的目录，挂载到容器指定目录在宿主机上对应的目录（/var/lib/docker/overlay2/读写层/test），就可以了。这时候Mount Namespace已经开启，挂载后，容器里面是可见的，宿主机上看不到容器里面的挂载点，容器的隔离性不好被volume打破。 这里用到的挂载技术，就是Linux的绑定挂载 bind mount 机制，主要作用就是将一个目录/文件，而不是一个设备，挂载到一个指定目录上。并且，这时候你在该挂载点上进行的任何操作，发生在被挂载的目录或者文件上，而原挂载点的内容则会被隐藏起来，不受影响。 绑定挂载实际上是一个inode替换的过程，indoe可以理解为存放文件内容的“对象”，dentry，目录项，就是访问inode所使用的“指针”。 bind mount相当于/test的dentry，重定向到/home的inode。当修改/test目录，实际是修改/home目录的inode。一旦umount后，/test目录原先的内容会恢复。 这个/test目录的内容，是挂载在容器rootfs的可读写层，是不会被docker commit提交的，以为docker commint是发生在宿主机空间，由于mount namespace的隔离，不知道绑定挂载的存储，所有/test目录会被打包，出现在新的镜像里面的，但是里面是空的。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/user-in-docker.html":{"url":"docker/user-in-docker.html","title":"User","keywords":"","body":"Issue docker run 一个centos镜像，当通过non-root用户exec进入容器后，su切换到root用户时，不知道root密码。 yzw@yzw-vm:~$ docker run -it -d centos:7.4.1708 c091aee509c85153f738815618e6fd8eec20d2795104a687b2f2a5b222161a5f yzw@yzw-vm:~$ docker exec -it -u 1000 c091aee50 bash bash-4.2$ id uid=1000 gid=0(root) groups=0(root) bash-4.2$ su Password: su: Authentication failure UID and GID Docker 默认没有开启User Namespace，Docker和Host公用一套UID/GID。 Example 1 运行一个centos容器： yzw@yzw-vm:~$ docker run -it -d centos:7.4.1708 c091aee509c85153f738815618e6fd8eec20d2795104a687b2f2a5b222161a5f 在容器里面创建个test用户，UID=1000 yzw@yzw-vm:~$ docker exec -it c091aee50 bash [root@c091aee509c8 /]# useradd test [root@c091aee509c8 /]# cat /etc/passwd | grep test test:x:1000:1000::/home/test:/bin/bash 使用test用户进入容器，跑个sleep yzw@yzw-vm:~$ docker exec -it -u test c091aee509 bash [test@c091aee509c8 /]$ sleep 1000 使用默认root进入容器，sleep进程的User是test,UID=1000 yzw@yzw-vm:~$ docker exec -it c091aee509 bash [root@c091aee509c8 /]# ps aux | grep -v grep | grep sleep test 97 0.0 0.0 4328 716 pts/2 S+ 07:30 0:00 sleep 1000 host主机上，sleep进程的User是yzw，因为yzw的UID也是1000 yzw@yzw-vm:~$ ps aux | grep -v grep | grep sleep yzw 4121 0.0 0.0 4328 716 pts/2 S+ 15:30 0:00 sleep 1000 yzw@yzw-vm:~$ id uid=1000(yzw) gid=1000(yzw) groups=1000(yzw),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),998(docker),999(vboxsf) Example 2 在容器里面创建个test01用户，UID=1001 [root@c091aee509c8 /]# useradd test02 [root@c091aee509c8 /]# cat /etc/passwd | grep test02 test02:x:1001:1001::/home/test02:/bin/bash [root@c091aee509c8 /]# cat /etc/group | grep test02 test02:x:1001: 使用test01进入容器，跑个sleep yzw@yzw-vm:~$ docker exec -it -u test02 c091aee509 bash [test02@c091aee509c8 /]$ id uid=1001(test02) gid=1001(test02) groups=1001(test02) [test02@c091aee509c8 /]$ sleep 1000 使用默认root进入容器，sleep进程的User是test02,UID=1001 [root@c091aee509c8 /]# ps aux | grep -v grep | grep sleep test02 129 0.0 0.0 4328 612 pts/2 S+ 01:29 0:00 sleep 1000 host主机上，sleep进程的User是1001 yzw@yzw-vm:~$ ps aux | grep -v grep | grep sleep 1001 6438 0.0 0.0 4328 612 pts/2 S+ 09:29 0:00 sleep 1000 Example 3 使用一个没有使用的UID登陆容器,跑个sleep： # 只指定了UID，所有GID默认为0，可以通过-u [:]来指定GID。 yzw@yzw-vm:~$ docker exec -it -u 1003 c091aee509 sh sh-4.2$ id uid=1003 gid=0(root) groups=0(root) sh-4.2$ sleep 1000 用root登陆容器查看： [root@c091aee509c8 /]# ps aux | grep -v grep | grep sleep 1003 198 0.0 0.0 4328 660 pts/2 S+ 01:48 0:00 sleep 1000 [root@c091aee509c8 /]# [root@c091aee509c8 /]# [root@c091aee509c8 /]# cat /etc/passwd | grep 1003 [root@c091aee509c8 /]# cat /etc/group | grep 1003 Host上查看sleep进程： yzw@yzw-vm:~$ ps aux | grep -v grep | grep sleep 1003 7399 0.0 0.0 4328 660 pts/2 S+ 09:48 0:00 sleep 1000 yzw@yzw-vm:~$ cat /etc/passwd | grep 1003 yzw@yzw-vm:~$ cat /etc/group | grep 1003 Dockerfile指定用户 FROM ubuntu RUN groupadd -g 999 appuser && \\ useradd -r -u 999 -g appuser appuser USER appuser 安全 容器里面和主机root的权限是一样的，那安全问题怎么解决？Docker目前提供了2种方案：privileged和user namespace。 privileged Docker默认时关闭特权模式的，需要的时候直接 docker run --privileged=true $ docker run --help --privileged Give extended privileges to this container 这样全部的capabilities都被试能。 Full container capabilities (–privileged) The –privileged flag gives all capabilities to the container, and it also lifts all the limitations enforced by the device cgroup controller. In other words, the container can then do almost everything that the host can do. This flag exists to allow special use-cases, like running Docker within Docker. 还可以通过 docker run --cap-add/drop 加和减指定的capabilities $ docker run --help --cap-add list Add Linux capabilities --cap-drop list Drop Linux capabilities Docker 默认不开启特权模式，默认只支持了一些基本的capabilities，从而限制了容器里面root用户权限。默认支持的有：CAP_CHOWN 、 CAP_DAC_OVERRIDE 、 CAP_FSETID 、 CAP_MKNOD 、 FOWNER 、 NET_RAW 、 SETGID 、 SETUID 、 SETFCAP 、 SETPCAP 、 NET_BIND_SERVICE 、 SYS_CHROOT 、 KILL 和 AUDIT_WRITE。在这 14 项中几乎没有一项涉及到系统管理权限，比如 Docker 容器的 root 用户不具备 CAP_SYS_ADMIN，磁盘限额操作、mount 操作、创建进程新命名空间等均无法实现；比如由于没有 CAP_NET_ADMIN，网络方面的配置管理也将受到管制。 CAPSYS_ADMIN：CAP_SYS_ADMIN 实现一系列的系统管理权限，比如实现磁盘配额的 quotactl，实现文件系统挂载的 mount 权限；比如在 fork 子进程时，通过 clone 和 unshare 系统调用，使用 CLONE* 的 flag 参数来为子进程创建新的 namespaces；比如实现各种特权块设备以及文件系统的 ioctl 操作等。 CAP_NET_ADMIN：CAP_NET_ADMIN 实现一系列的网络管理权限，比如网络设备的配置，IP 防火墙，IP 伪装以及统计等功能；比如路由表的修改，TOS 的配置，混杂模式的配置等。 CAP_SETUID：CAP_SETUID 有能力对进程 UID 做出任何管控。 CAP_SYS_MODULE：CAP_SYS_MODULE 帮助 root 用户加载或者卸载相应的 Linux 内核模块。 CAP_SYS_NICE：CAP_SYS_NICE 有能力对任意进程修改其 NICE 值，同时支持对任何进程设置调度策略与优先级，还有在进程的 CPU 亲和性以及 I/O 调度方面有相应的配置权限 User Namespace 开启： # /etc/docker/daemon.json { \"userns-remap\": \"default\" } $ systemctl restart docker 验证 # 开启后系统创建了一个dockremap的用户 yzw@yzw-vm:~/code/ns$ id dockremap uid=125(dockremap) gid=130(dockremap) groups=130(dockremap) yzw@yzw-vm:~/code/ns$ sudo cat /etc/group | grep dockre dockremap:x:130: yzw@yzw-vm:~/code/ns$ sudo cat /etc/passwd | grep dockre dockremap:x:125:130::/home/dockremap:/bin/false # 新建了一个目录165536.165536,165536是dockremap映射出来的一个uid yzw@yzw-vm:~/code/ns$ sudo ls -ld /var/lib/docker/165536.165536 drwx------ 14 165536 165536 4096 Dec 29 00:39 /var/lib/docker/165536.165536 # 该目录和/var/lib/docker目录基本一样，用户隔离的文件都放这 yzw@yzw-vm:~/code/ns$ sudo ls -l /var/lib/docker/165536.165536 total 48 drwx------ 2 root root 4096 Dec 29 00:39 builder drwx------ 4 root root 4096 Dec 29 00:39 buildkit drwx------ 2 165536 165536 4096 Dec 29 00:39 containers drwx------ 3 root root 4096 Dec 29 00:39 image drwxr-x--- 3 root root 4096 Dec 29 00:39 network drwx------ 3 165536 165536 4096 Dec 29 00:39 overlay2 drwx------ 4 root root 4096 Dec 29 00:39 plugins drwx------ 2 root root 4096 Dec 29 00:39 runtimes drwx------ 2 root root 4096 Dec 29 00:39 swarm drwx------ 2 165536 165536 4096 Dec 29 00:39 tmp drwx------ 2 root root 4096 Dec 29 00:39 trust drwx------ 2 165536 165536 4096 Dec 29 00:39 volumes 使用 # 拉个容器跑个sleep yzw@yzw-vm:~/code/ns$ docker run -it -d centos bash 5028c0844804f6e5a5d57840448edeb596fa73a7d82d74eb40246ce9fc200e21 yzw@yzw-vm:~/code/ns$ docker exec -it 5028c084 bash [root@5028c0844804 /]# id uid=0(root) gid=0(root) groups=0(root) [root@5028c0844804 /]# sleep 1000 # host上，sleep的uid是165536，uid 165536 是用户 dockremap 的一个从属 ID，在宿主机中并没有什么特殊权限。然而容器中的用户却是 root， yzw@yzw-vm:~$ ps aux | grep sleep 165536 5249 0.0 0.0 4372 660 pts/1 S+ 00:53 0:00 sleep 1000 # 启动某个容器不使用user namespace，加--userns=host $ docker run -d --userns=host --name sleepme ubuntu sleep infinity 问题： 目前 docker 对它的支持还算不上完美，下面是已知的几个和现有功能不兼容的问题： 共享主机的 PID 或 NET namespace(--pid=host or --network=host) 外部的存储、数据卷驱动可能不兼容、不支持 user namespace 使用 --privileged 而不指定 --userns=host Reference:https://success.docker.com/article/introduction-to-user-namespaces-in-docker-enginehttps://docs.docker.com/engine/security/userns-remap/ https://medium.com/@mccode/understanding-how-uid-and-gid-work-in-docker-containers-c37a01d01cf QA 容器为什么不能识别到host的所有用户 因为/etc目录不是host的 容器exec进去为什么默认是root用户 dockerfile文件里面可以修改 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/user-and-group.html":{"url":"docker/user-and-group.html","title":"User and Group","keywords":"","body":"概述 reference: https://wiki.archlinux.org/index.php/Users_and_groups_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87) UNIX系统一切皆文件。GNU/Linux 通过用户和用户组实现访问控制 —— 包括对文件访问、设备使用的控制。 每一个文件都从属一个用户（属主）和一个用户组（属组），有三类访问权限：R，W，X。 yzw@yzw-vm:~$ ll drwxr-xr-x 2 yzw yzw 4096 Jun 19 2018 Desktop -rw-r--r-- 1 root root 168 Sep 12 22:08 Dockerfile 第一列 权限： 首字母d为目录，-为文件；后面每3个字符为一组，分别对应： 用户u，组g，其他用户o，-表示无此权限；读r，写w，执行x； 要给g添加权限读(r)和执行权限(x)就是：chmod g+rx 文件名 ---加号表示添加权限要取消其他用户的写(w)权限：chmod g-w 文件名 ---减号表示取消权限 第三列： User第四列： Goup User User分为： root用户，超级管理员。 虚拟用户，无登陆系统能力，系统运行必不可少的，比如bin，damen，ftp，nobody等。 普通实体用户，能登陆系统，只能操作自己的目录和内容，权限有限。 查看User信息 /etc/passwd:用户名：口令：UID：GID：注释性描述：主目录：登陆shell yzw@yzw-vm:~$ cat /etc/passwd root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin man:x:6:12:man:/var/cache/man:/usr/sbin/nologin nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin yzw:x:1000:1000:yzw,,,:/home/yzw:/bin/bash ... 密码口令为x，加密后保存在/etc/shadow /etc/shadow：用户名:加密口令:最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志 yzw@yzw-vm:~$ sudo cat /etc/shadow [sudo] password for yzw: root:!:17668:0:99999:7::: daemon:*:17647:0:99999:7::: man:*:17647:0:99999:7::: nobody:*:17647:0:99999:7::: yzw:$6$Kv6eIYhy$0ucI91b0KnzHl37y1WbfmMyxX.Jf7xSSukRBusXAM7sTaapYvS9HXWhDjgK9dozIJpl1FAzsGvWS5aqo.AVwL0:17668:0:99999:7::: ... 命令 useradd 或 adduesr：添加用户 $ useradd -g docker -G root,adm -u 1000 -s /bin/bash -d /home/yzw yzw $ useradd abc passwd:为用户设置密码,新增加的user不设置密码无法登陆的。 # root用户重置不需要输入旧密码，普通用户需要输入旧密码。 $ passwd yzw # -d 指定空密码 $ passwd -d yzw userdel删除用户,主要删除/etc/group shadow passwd等记录和用户目录 # -r 删除目录 $ userdel -r yzw gpasswd把user加到group，或从group删除 $ gpasswd -a yzw docker $ gpasswd -d yzw docker usermod：修改用户属性，如登录名、用户的home目录等 # 修改用户组,离开其他组，只加入这个组 $ usermod -G groupTest test123 # 添加多个组 $ usermode -G A,B,C yzw # 修改用户名称 $ usermode -l newname oldname pwcov： 同步用户从/etc/passwd 到/etc/shadowpwck： 校验用户配置文件/etc/passwd 和/etc/shadow 文件内容是否合法或完整；pwunconv：与pwconv功能相反，用来关闭用户的投影密码。它会把密码从shadow文件内，重回存到passwd文件里，然后删除 /etc/shadow 文件。finger：查看用户信息工具id：查看用户的UID、GID及所归属的用户组chfn：更改用户信息工具su：用户切换工具，默认切换到root，需要输入切换的用户密码sudo：用来以其他身份来执行命令，预设的身份为root。在/etc/sudoers中设置了可执行sudo指令的用户。若其未经授权的用户企图使用sudo，则会发出警告的邮件给管理员。用户使用sudo时，必须先输入用户自己的密码，之后有5分钟的有效期限，超过期限则必须重新输入密码。visudo： 用于编辑 /etc/sudoers 的命令；也可以不用这个命令，直接用vi 来编辑 /etc/sudoers 的效果是一样的。 Group Group就是具由相同特征的用户User的集合体。一个User可以属于多个Group，一个主Group，其他都是附属Group。 一个User都有一个同名的Group。 查询信息 /etc/group组名称：密码：GID：用户 yzw@yzw-vm:~$ cat /etc/group root:x:0: daemon:x:1: man:x:12: sudo:x:27:yzw yzw:x:1000: nobody:x:997: ... /etc/gshadow组名称：密码：管理员账号：所属账号 yzw@yzw-vm:~$ sudo cat /etc/gshadow [sudo] password for yzw: root:*:: daemon:*:: man:*:: yzw:!:: nobody:!!:: 命令 groupadd：添加用户组 $ groupadd -g 101 group2 groupdel：删除用户组groupmod：修改用户组信息groups：显示用户所属的用户组grpck：用于验证组文件的完整性grpconv：通过/etc/group和/etc/gshadow 的文件内容来同步或创建/etc/gshadow ，如果/etc/gshadow 不存在则创建；grpunconv：通过/etc/group 和/etc/gshadow 文件内容来同步或创建/etc/group ，然后删除gshadow文件； UID/EUID/RUID and GID/EGID/RGID UID/GID UID/GID是Kernel识别User/Group的标识，是唯一的， 但是一套UID/GID可以对应多个User/Group。 相同UID/GID的不同User/Group的权限是相同的。 root的UID和GID都是0，可以访问一切资源，叫privileged UID/GID。 大部分Linux系统的前100 UID/GID都是系统使用的，新用户从500或者1000开始，Ubuntu/Centos上新用户从1000开始。 yzw@yzw-vm:~$ id uid=1000(yzw) gid=1000(yzw) groups=1000(yzw),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),998(docker),999(vboxsf) 每一个process有：RUID/RGID Real UID/GID：identify process owner当用户使用用户名密码登陆系统后，旧有唯一确定的RUID了，就是这个用户的UID。创建用户的时候已经定义好了。 EUID/EGID Effective UID/GDID： used in access control decisions进程在运行中对文件的实际范围权限，通常就是RUID。 Saved UID/GID saved UID/GID：stores a previous UID so that it can be restored laterFSUID/FSGID used for access control to the file system yzw@yzw-vm:~$ ps -o pid,uid,euid,ruid,suid,egid,rgid,sgid,cmd PID UID EUID RUID SUID EGID RGID SGID CMD 3741 1000 1000 1000 1000 1000 1000 1000 bash 一个可执行文件（程序），在执行时，一般该文件只拥有调用这个文件的用户具有的权限。因为用户登陆系统后会执行Shell（bash或者sh），这是创建用户时就定义好的，这个用户执行的程序的父进程就是这个Shell，权限也就继承了父进程的权限。通过SUID/GID可以改变这种设置，这时候EUID就不是从父进程那继承来的了，就变成了文件本身的User/Group了。 SUID/SGID Set UID/GID：对外权限的开放。跟文件而不是用户绑定。 用来设置可执行文件在执行阶段具有文件所有者的权限。 普通用户执行su或者passwd时，这两个程序都会访问/etc/passwd和/etc/shadow文件，而这两个文件是需要root权限的。 yzw@yzw-vm:~$ ll /etc/passwd -rw-r--r-- 1 root root 2774 Dec 27 00:10 /etc/passwd yzw@yzw-vm:~$ ll /etc/shadow -rw-r----- 1 root shadow 1545 Dec 27 00:10 /etc/shadow su和passwd这两个可执行文件权限里面带了s： yzw@yzw-vm:~$ ll /bin/su -rwsr-xr-x 1 root root 44664 Jan 25 2018 /bin/su* yzw@yzw-vm:~$ ll /usr/bin/passwd -rwsr-xr-x 1 root root 59640 Jan 25 2018 /usr/bin/passwd* s 就是set-user-id标志。有这个标志表示，任何普通用户运行su或者passwd程序时，其EUID就是该程序的所有者。 用户yzw执行passwd命令时，yzw的Shell会fork一个子进程，子进程的RUID == EUID == yzw的UID,然后exec程序/usr/bin/passwd,exec 会根据/usr/bin/passwd的SUID 把子进程的 EUID 设置成 root, 此时这个进程就有了root权限，可以读/etc/shadow文件了，从而完成yzw密码修改。 通过chmod修改去掉/usr/bin/passwd的SUID权限： yzw@yzw-vm:~$ sudo chmod u-s /usr/bin/passwd [sudo] password for yzw: yzw@yzw-vm:~$ ll /usr/bin/passwd -rwxr-xr-x 1 root root 59640 Jan 25 2018 /usr/bin/passwd* 修改密码就会报权限错误： yzw@yzw-vm:~$ passwd Changing password for yzw. (current) UNIX password: Enter new UNIX password: You must choose a longer password passwd: Authentication token manipulation error passwd: password unchanged Reference:https://skednet.wordpress.com/2010/07/07/uid-euid-suid-fsuid/ When a process is created by FORK, the created process inherits its parent's UIDs When a process executes a NEW FILE via EXEC, that executing process keeps ITS OWN UIDs unless the SETUID in the new file is set. -- if the SETUID bit is set in the file we're EXECing, then --> process's EUID == file owner's UID --> process's SUID == file owner's UID To drop privilege temporarily, a process will remove the privileged UID from its EUID but keep it saved as its saved UID (SUID); later the process can restore the privileges by setting its EUID to its SUID. To drop privileges permanently a process removes the privileged UID from all three of its UIDs; thereafter the process cannot restore that privileged access Capabilities Linux 给普通用户尽可能低的权限，给root全部的权限，依赖单一账户执行特权操作的方式加大了系统风险，比如我们使用SUID知识需要一小部分特权，但是却获取了root的全部权限。引入了Capabilities的概念限制特权用户的权限。 Capability 有三种： effective（e）：当前有效的能力集，当执行某特权操作时，系统检查cap_effective对应位是否有效，不再检查进程的有效UID是是否位0. permitted（p）：当前进程允许使用的能力集，effective包含于permitted。 inherited（i）：可以被继承的能力集。比如执行exec允许其他命令时，能够被新命令继承的能力就在inherited集里面。 Ambient： 外界的，这是一个为非特权程序的跨 execve(2) 保留的权能集合。外界的权能集合服从不可变性，如果权能既不是被允许的也不是可继承的，则它也从不可能是外界的。 Capabilities 细分到线程，每个线程都有自己的Capabilities。 可执行文件也具有一定的Capabilities。 POSIX能力只能分配给进程，不能分配给文件。 如果进程的真实 uid 或有效 uid 是 0（根用户），或者文件是 setuid root，那么文件的可继承集和允许集就是满的。 如果进程的有效 uid 是根用户，或者文件是 setuid root，那么文件有效集就是满的。root@yzw-vm:/home/yzw# cat /proc/$$/task/10973/status | grep Cap CapInh: 0000000000000000 CapPrm: 0000003fffffffff CapEff: 0000003fffffffff CapBnd: 0000003fffffffff CapAmb: 0000000000000000 Liunx定义了很多特权操作和对应的Capability： CAP_CHOWN 改变文件的属性chown() CAP_KILL 发送kill()，signal()信号 CAP_SETUID 改变进程uid,setuid() CAP_SYS_PTRACE trace进程ptrace() CAP_SYS_TIME 系统时间settimeofday() ... 程序 Capabilities /bin/ping CAP_NET_RAW /bin/mount CAP_SYS_ADMIN /bin/su CAP_DAC_OVERRIDE,CAP_SETGID,CAP_SETUID /usr/bin/passwd CAP_CHOWN ,CAP_DAC_OVERRIDE ,CAP_FOWNER # ping 设置了SUID，所有非root用户可以使用 yzw@yzw-vm:~$ ll /bin/ping -rwsr-xr-x 1 root root 68520 Aug 29 16:25 /bin/ping* yzw@yzw-vm:~$ ping www.baidu.com PING www.a.shifen.com (220.181.111.37) 56(84) bytes of data. 64 bytes from 220.181.111.37 (220.181.111.37): icmp_seq=1 ttl=53 time=20.5 ms 64 bytes from 220.181.111.37 (220.181.111.37): icmp_seq=2 ttl=53 time=19.10 ms # 取消ping的SUID权限，非root用户就没权限了 yzw@yzw-vm:~$ sudo chmod u-s /bin/ping [sudo] password for yzw: yzw@yzw-vm:~$ ping www.baidu.com ping: socket: Operation not permitted # 给ping设置cap_new_raw的Capability，没有SUID但有权限了 yzw@yzw-vm:~$ sudo setcap cap_net_raw+ep /bin/ping yzw@yzw-vm:~$ ll /bin/ping -rwxr-xr-x 1 root root 68520 Aug 29 16:25 /bin/ping* yzw@yzw-vm:~$ sudo getcap /bin/ping /bin/ping = cap_net_raw+ep yzw@yzw-vm:~$ ping www.baidu.com PING www.a.shifen.com (220.181.111.37) 56(84) bytes of data. 64 bytes from 220.181.111.37 (220.181.111.37): icmp_seq=1 ttl=53 time=19.9 ms 64 bytes from 220.181.111.37 (220.181.111.37): icmp_seq=2 ttl=53 time=20.2 ms #删除能力 yzw@yzw-vm:~$ sudo setcap -r /bin/ping yzw@yzw-vm:~$ sudo setcap cap_net_raw-ep /bin/ping Reference:https://www.hrwhisper.me/introduction-to-linux-capability/https://rk700.github.io/2016/10/26/linux-capabilities/https://linux.die.net/man/7/capabilities User Namespace Linux User Namespace 为正在运行的进程提供安全相关的隔离，包括UID/GID，Capabilities，限制他们对系统资源的访问。 user namespace可以嵌套（目前内核控制最多32层），除了系统默认的user namespace外，所有的user namespace都有一个父user namespace，每个user namespace都可以有零到多个子user namespace。 当在一个进程中调用unshare或者clone创建新的user namespace时，当前进程原来所在的user namespace为父user namespace，新的user namespace为子user namespace. 在不同的user namespace中，同样一个用户的user ID 和group ID可以不一样，换句话说，一个用户可以在父user namespace中是普通用户，在子user namespace中是超级用户（超级用户只相对于子user namespace所拥有的资源，无法访问其他user namespace中需要超级用户才能访问资源） 从Linux 3.8开始，创建新的user namespace不需要root权限。 当前yzw用户信息 yzw@yzw-vm:~/code/ns$ id uid=1000(yzw) gid=1000(yzw) groups=1000(yzw),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),998(docker),999(vboxsf) yzw@yzw-vm:~/code/ns$ readlink /proc/$$/ns/user user:[4026531837] yzw@yzw-vm:~/code/ns$ cat /proc/$$/status | grep Cap CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000003fffffffff CapAmb: 0000000000000000 # 非root没有Cap，改不了hostname yzw@yzw-vm:~/code/ns$ hostname FOOO hostname: you must be root to change the host name 创建user namespace 需要指定跟它父ns的uid/gid进行映射，这样系统才能控制一个user ns里的用户在其他user ns里面的权限，比如向其他user ns里面进程发信号，访问其他user ns里用户权限的文件。noboy的uid/gid值是溢出值，在/proc/sys/kernel/overflowuid和 /proc/sys/kernel/overflowgid中定义。 yzw@yzw-vm:~/code/ns$ unshare --user /bin/bash nobody@yzw-vm:~/code/ns$ id uid=65534(nobody) gid=65534(nogroup) groups=65534(nogroup) 创建user namespace后，需要修改映射关系，映射ID的方法是添加配置到/proc/PID/uid_map和/proc/PID/gid_map（这里的PID是新user namespace中的进程ID，刚开始时这两个文件都是空的）。格式：ID-inside-ns ID-outside-ns length比如：0 1000 256 父user ns的1000-1256映射到新user ns的0-256 系统默认的user namespace没有父user namespace，但为了保持一致，kernel提供了一个虚拟的uid和gid map文件，看起来是这样子的: yzw@yzw-vm:~/code/ns$ cat /proc/$$/uid_map 0 0 4294967295 yzw@yzw-vm:~/code/ns$ cat /proc/$$/gid_map 0 0 4294967295 创建映射： #新创建user ns 的map文件时空的 yzw@yzw-vm:~/code/ns$ unshare --user /bin/bash nobody@yzw-vm:~/code/ns$ cat /proc/$$/uid_map nobody@yzw-vm:~/code/ns$ cat /proc/$$/gid_map nobody@yzw-vm:~/code/ns$ ls -l /proc/$$/uid_map -rw-r--r-- 1 nobody nogroup 0 Dec 28 23:54 /proc/2888/uid_map #nobody有读写权限，但还是不让写 nobody@yzw-vm:~/code/ns$ echo '0 1000 100' > /proc/$$/uid_map bash: echo: write error: Operation not permitted nobody@yzw-vm:~/code/ns$ echo $$ 2888 #新开个终端到yzw用户下 #因为这个文件需要CAP_SETUID和CAP_SETGID的权限，新user ns的bash进程没有 yzw@yzw-vm:~$ ll /proc/2888/uid_map /proc/2888/gid_map -rw-r--r-- 1 yzw yzw 0 Dec 28 23:54 /proc/2888/gid_map -rw-r--r-- 1 yzw yzw 0 Dec 29 00:11 /proc/2888/uid_map yzw@yzw-vm:~$ echo '0 1000 100' > /proc/2888/uid_map -bash: echo: write error: Operation not permitted yzw@yzw-vm:~$ echo '0 1000 100' > /proc/2888/gid_map -bash: echo: write error: Operation not permitted #给当前bash增加Cap yzw@yzw-vm:~$ sudo setcap cap_setgid,cap_setuid+ep /bin/bash yzw@yzw-vm:~$ getcap /bin/bash /bin/bash = cap_setgid,cap_setuid+ep yzw@yzw-vm:~$ cat /proc/$$/status | grep Cap CapInh: 0000000000000000 CapPrm: 0000000000000000 CapEff: 0000000000000000 CapBnd: 0000003fffffffff CapAmb: 0000000000000000 yzw@yzw-vm:~$ exec bash yzw@yzw-vm:~$ cat /proc/$$/status | grep Cap CapInh: 0000000000000000 CapPrm: 00000000000000c0 CapEff: 00000000000000c0 CapBnd: 0000003fffffffff CapAmb: 0000000000000000 yzw@yzw-vm:~$ echo '0 1000 100' > /proc/2888/gid_map yzw@yzw-vm:~$ echo '0 1000 100' > /proc/2888/uid_map #恢复Cap yzw@yzw-vm:~$ sudo setcap cap_setgid,cap_setuid-ep /bin/bash yzw@yzw-vm:~$ getcap /bin/bash /bin/bash = yzw@yzw-vm:~$ #在新建user ns下，重新载入bash，变成root用户了映射的是yzw用户 nobody@yzw-vm:~/code/ns$ exec /bin/bash root@yzw-vm:~/code/ns# id uid=0(root) gid=0(root) groups=0(root),65534(nogroup) root@yzw-vm:~/code/ns# root用户映射： #创建新的user ns，-r 是将root用户映射到yzw用户 yzw@yzw-vm:~/code/ns$ unshare --user -r /bin/bash root@yzw-vm:~/code/ns# readlink /proc/$$/ns/user user:[4026532301] # 新的user ns 里面root用户的bash进程拥有全部Cap root@yzw-vm:~/code/ns# cat /proc/$$/status | grep Cap CapInh: 0000000000000000 CapPrm: 0000003fffffffff CapEff: 0000003fffffffff CapBnd: 0000003fffffffff CapAmb: 0000000000000000 root@yzw-vm:~/code/ns# id uid=0(root) gid=0(root) groups=0(root),65534(nogroup) root@yzw-vm:~/code/ns# cat /proc/$$/uid_map 0 1000 1 root@yzw-vm:~/code/ns# cat /proc/$$/gid_map 0 1000 1 root@yzw-vm:~/code/ns# hostname FOOOO # 但是root用户依然改不了uts，因为yzw用户Cap都是空，没有改hostname权限 root@yzw-vm:~/code/ns# hostname FOOOO hostname: you must be root to change the host name 使用root用户创建user ns，在子ns里面root依然不能改hostname。不管怎么映射，当用子 user namespace 的用户访问父 user namespace 的资源的时候，它启动的进程的 capability 都为空，所以这里子 user namespace 的 root 用户在父 user namespace 中就相当于一个普通的用户。 root@yzw-vm:/home/yzw/code/ns# unshare --user -r /bin/bash root@yzw-vm:/home/yzw/code/ns# hostname FOO hostname: you must be root to change the host name root@yzw-vm:/home/yzw/code/ns# Reference:https://segmentfault.com/a/1190000006913195 https://segmentfault.com/a/1190000006913499https://lwn.net/Articles/532593/https://lwn.net/Articles/540087/ powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/non-root-user.html":{"url":"docker/non-root-user.html","title":"Non-root User","keywords":"","body":"非root用户无法运行docker命令 Reference:https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user 使用非root用户运行docker命令的时候会提示没有权限，需要加上sudo： yzw@yzw-vm:~$ docker ps Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/json: dial unix /var/run/docker.sock: connect: permission denied 是因为当前用户没有访问 /var/run/docker.sock 的权限： yzw@yzw-vm:~$ ll /var/run/docker.sock srw-rw---- 1 root docker 0 Dec 26 10:24 /var/run/docker.sock= yzw@yzw-vm:~$ cat /etc/group | grep docker docker:x:998: yzw@yzw-vm:~$ id uid=1000(yzw) gid=1000(yzw) groups=1000(yzw),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),999(vboxsf) 需要将当前用户加入到docker组里面： yzw@yzw-vm:~$ sudo gpasswd --add yzw docker Adding user yzw to group docker # 或者 sudo usermod -aG docker $USER yzw@yzw-vm:~$ cat /etc/group | grep docker docker:x:998:yzw 当前用户log out再 log back： yzw@yzw-vm:~$ id uid=1000(yzw) gid=1000(yzw) groups=1000(yzw),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),116(lpadmin),126(sambashare),998(docker),999(vboxsf) yzw@yzw-vm:~$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES powered by GitbookUpdated: 2019-01-21 17:23:25 "},"docker/docker-image.html":{"url":"docker/docker-image.html","title":"Images","keywords":"","body":"mount namespace “左耳朵耗子” 叔的一篇将docker的Namespace的文章，文章地址：https://coolshell.cn/articles/17010.html ,里面有个小程序: #define _GNU_SOURCE #include #include #include #include #include #include #include /* 定义一个给 clone 用的栈，栈大小1M */ #define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = { \"/bin/bash\", NULL }; int container_main(void* arg) { printf(\"Container [%5d] - inside the container!\\n\", getpid()); execv(container_args[0], container_args); printf(\"Something's wrong!\\n\"); return 1; } int main() { printf(\"Parent [%5d] - start a container!\\n\", getpid()); /* 启用Mount Namespace - 增加CLONE_NEWNS参数 */ int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWUTS | CLONE_NEWPID | CLONE_NEWNS | SIGCHLD, NULL); waitpid(container_pid, NULL, 0); printf(\"Parent - container stopped!\\n\"); return 0; } clone创建一个新的子进程，启动mount namespace（CLONE_NEWNS标志），子进程启动/bin/bash程序，这个shell就运行在mount namespace的隔离环境里。编译下： [root@yzw test]# gcc -o container container.c [root@yzw test]# ./container Parent [19645] - start a container! Container [ 1] - inside the container! [root@yzw test]# ls /tmp/ test 发现依然能看到很多宿主机的文件。为什么mount namespace开启后，容器里面和宿主机的文件完全一样呢？因为mount namespace修改的是容器进程对文件系统“挂载点”的认知，只有在“挂载”操作之后才能起作用，之前新创建的容器会继承宿主机的挂载点。我们在上面程序bash操作前重新以tmpfs内存盘的格式mount /tmp目录： int container_main(void* arg) { printf(\"Container [%5d] - inside the container!\\n\", getpid()); //如果机器根目录的挂载类型是shared，得重新挂载根目录 //mount(\"\",\"/\",NULL,MS_PRIVATE,\"\") mount(\"none\",\"/tmp\",\"tmpfs\",0,\"\") execv(container_args[0], container_args); printf(\"Something's wrong!\\n\"); return 1; } 编译执行结果： [root@yzw test]# gcc -o container container.c [root@yzw test]# ./container Parent [19861] - start a container! Container [ 1] - inside the container! [root@yzw test]# ls /tmp/ [root@yzw test]# [root@yzw test]# mount -l | grep tmpfs ... none on /tmp type tmpfs (rw,relatime,seclabel) 在宿主机上： # 按理说应该是看不见的，在虚拟机里还能看到，后面再调查哈为啥？？ tmp目录下已经和宿主机的tmp目录内容不一致了。 当新创一个容器时，在容器进程启动前重新挂载根目录，由于mount namespace的存在，这个目录对宿主机不可见，容器里面的文件系统就是一个隔离的环境了。 issue 重新挂载/tmp目录的实验执行完成后，在宿主机上居然可以看到这个挂载信息。。这是怎么回事呢？实际上，大家自己装的虚拟机，或者云上的虚拟机的根目录，很多都是以share方式的挂载的。这时候，你在容器里做mount也会继承share方式。这样就会把容器内挂载传播到宿主机上。解决这个问题，你可以在重新挂载/tmp之前，在容器内先执行一句：mount(“”, “/“, NULL, MS_PRIVATE, “”) 这样，容器内的根目录就是private挂载的了。（没起作用?） chroot chroot可以改变进程的根目录到指定目录，把bash和ls程序以及运行需要的so文件拷贝过来: [root@yzw tmp]# mkdir -p test/{bin,lib64,lib} [root@yzw tmp]# cd test/ [root@yzw test]# cp /bin/{bash,ls} bin/ [root@yzw test]# T=/tmp/test [root@yzw test]# list=\"$(ldd /bin/ls | egrep -o '/lib.*\\.[0-9]')\" [root@yzw test]# for i in $list; do cp -v \"$i\" \"${T}${i}\";done [root@yzw test]# list=\"$(ldd /bin/bash | egrep -o '/lib.*\\.[0-9]')\" [root@yzw test]# for i in $list; do cp -v \"$i\" \"${T}${i}\";done [root@yzw tmp]# chroot /tmp/test /bin/bash bash-4.2# ls / bin lib lib64 bash进程的根目录被修改了，感知不到宿主机的目录，这种方式和mount namespace效果类似，mount namespace就是基于chroot改进而来的。 镜像 为了让容器的这个根目录真实，我们一般会把给根目录挂载一个完整的操作系统的文件系统，比如ubuntu的ISO。这样启动之后，我们ls根目录就是整个ubuntu的所有目录和文件了。这个挂载在容器根目录，用来为容器j进程提供隔离后执行环境的文件系统，就是容器镜像，还有关于专业的名称，rootfs（根文件系统）。 [root@yzw ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ededc6515049 busybox \"/bin/sh\" 2 hours ago Up 2 hours cranky_lichterman [root@yzw ~]# docker exec -it ededc6515049 /bin/sh / # ls bin dev etc home proc root sys tmp usr var 进入容器执行的sh，ls程序，就是容器/bin目录下的程序，和宿主机上的sh，ls完全不一样。 docker项目最核心的的原理就是为带创建的用户进程： 启动namespace设置； 设置指定的cgroup参数； 切换进程的根目录；docker 优先使用pivot_root系统调用，如果不支持才使用chroot。 rootfs只是一个操作系统包含的文件，配置和目录，并不包括内核。linux操作系统中，内核和这些是分开的，开机时将内核加载到内存中。所有同一个宿主机上的容器公用同一个内核。如果容器应用要修改内核的配置啥的会影响这个宿主机的，这点不如虚机隔离性好。 容器的一致性 不管是本地，还是云端，还是任何一个机器，用户只要解压打包好的rootfs，这个应用就能跑起来。解决了PaaS打包的大问题。 如果一个rootfs做好，另外一个应用像在这个上面进行修改，再发布，这样维护起来比较麻烦，不具备扩展性。docker公司引入了layer的概念，用户制作镜像的每一个步骤，都会生成一个layer，就是一个增量的rootfs。 Docker 的storage driver除了支持AUFS外，还支持devicemapper，overlayer，btrfs等，现在Docker CE默认为overlayer2，如果后端是direct-LVM的话，会默认为devicemapper。通过docker info可以查询到： [root@yzw ~]# docker info Containers: 1 Running: 1 Paused: 0 Stopped: 0 Images: 109 Server Version: 18.06.1-ce Storage Driver: overlay2 Backing Filesystem: extfs Supports d_type: true Native Overlay Diff: true AUFS 联合文件系统（union file system） unionFS最主要的功能就是可以将多个不同位置的目录联合挂载在同一个目录下。AUFS全称Another UnionFS，后面又叫Alternative UnionFS，再后就又叫Advance UnionFS，只能在ubuntu和Debian上使用，因为没有被Linus合入linux主分干，下面例子在Ubuntu上跑的。 root@yzw-vm:/tmp# tree . ├── A │ ├── a │ └── x ├── B │ ├── b │ └── x └── C 3 directories, 4 files root@yzw-vm:/tmp# cat A/x a root@yzw-vm:/tmp# cat B/x b root@yzw-vm:/tmp# mount -t aufs -o dirs=./A:./B none ./C root@yzw-vm:/tmp# tree . ├── A │ ├── a │ └── x ├── B │ ├── b │ └── x └── C ├── a ├── b └── x 3 directories, 7 files root@yzw-vm:/tmp# cat C/x a root@yzw-vm:/tmp# df -h Filesystem Size Used Avail Use% Mounted on ... none 59G 22G 34G 40% /tmp/C root@yzw-vm:/tmp# echo \"c\" > C/x root@yzw-vm:/tmp# cat C/x c root@yzw-vm:/tmp# cat A/x c root@yzw-vm:/tmp# cat B/x b 合并后的C目录，x只有一份，通过“写时复制策略实现镜像的共享和最小化磁盘开销。对C中的文件进行修改，A，B 中对应文件也会生效(相同文件存在覆盖除外)。 AUFS工作在文件的层次上，也就是说AUFS对文件的操作需要将整个文件复制到读写层内，哪怕只是文件的一小部分被改变，也需要复制整个文件。这在一定成度上会影响容器的性能，尤其是当要复制的文件很大，文件在栈的下面几层或文件在目录中很深的位置时，对性能的影响会很显著。 通过“写时复制（CoW）”策略实现镜像的共享和最小化磁盘开销。任何对于底层文件系统分层的更改都会被“向上拷贝”到文件系统的一个临时、工作、或高层的分层里面。这个可写的层然后可以被看做是一个“改动（diff）”，能将之应用到下层只读的层，而这些层很可能作为底层被很多容器的进程中共享。 一个Docker中使用分层文件系统的好处就是，1000个运行着bash的ubuntu:latest容器的副本，会共享一个底层的镜像，而并不会产生1000个文件系统的副本（vfs是个例外，请参考下边vfs部分）。并且同样重要的是，对于aufs和overlay的实现，用来读取或执行共享库的共享内存也在所有运行的容器之间共享，大大的减少了通用库如'libc'的内存占用。 issue 目录联合挂载时，如果A和B目录里的x文件内容不一样，这时如何处理？ aufs是一层一层往上盖的，所以我给的例子里，A里面的x会覆盖B里面的x. root@yzw-vm:/tmp# mount -t aufs -o dirs=./B:./A none ./C root@yzw-vm:/tmp# tree . ├── A │ ├── a │ └── x ├── B │ ├── b │ └── x └── C ├── a ├── b └── x 3 directories, 7 files root@yzw-vm:/tmp# cat C/x b 没有内核所以rootfs会比较小，请问一般安装的linux系统内核文件在哪里存放呢？ 首先，docker镜像比较小不只是因为没有内核，内核本身其实不大。大小的差异主要因为我们平常看见的虚拟机镜像实际上是整个磁盘的快照。其次，一般情况下，内核放下安装盘里，解压到磁盘上，加载到内存中。有兴趣可以读这里：https://blog.csdn.net/gatieme/article/details/50914250 devicemapper 早期Docker在Debian，Ubuntu系统中默认使用AUFS，RedHat系统用devicemapper。devicemapper是红帽系主推的。 devicemapper是基于块设备的，不是基于文件的。它质量上有优点也有缺点，如果安装/配置过程中没有特别格外注意的话，可能导致和其他选项比较起来性能低下、质量不高。鉴于overlay和overlay2受到了Fedora和RHEL最新的内核的支持，并且拥有SELinux的支持，除非在Red Hat场景中有某种必须使用devicemapper的需求，我想随着用户的成熟他们会转向overlay的怀抱。 overlay2 overlay2是什么 Overlay是一个联合文件系统，它的概念较之aufs的分支模型更为简单。Overlay通过三个概念来实现它的文件系统：一个“下层目录（lower-dir）”，一个“上层目录（upper-dir）”，将着两个layers联合挂载后就是一个做为文件系统合并视图的“合并（merged）”目录。镜像层和容器曾可以有相同的文件，这中情况下，upperdir中的文件覆盖lowerdir中的文件。 受限于只有一个“下层目录”，需要额外的工作来让“下层目录”递归嵌套（下层目录自己又是另外一个overlay的联合），或者按照Docker的实现，将所有位于下层的内容都硬链接到“下层目录”中。正是这种可能潜在的inode爆炸式增长（因为有大量的分层和硬连接）阻碍了很多人采用Overlay。Overlay2支持多个下层目录，最多128个，解决了Overlay的inode耗尽的问题，继承了Overlay很多优点，包括包括在同一个引擎的多个容器间从同一个分层中加载内库从而达到内存共享。 Docker 配置使用overlay2：https://docs.docker.com/storage/storagedriver/overlayfs-driver/#configure-docker-with-the-overlay-or-overlay2-storage-driver 举个例子 看下ubuntu镜像由5层构成： root@yzw-vm:~# docker pull ubuntu:latest latest: Pulling from library/ubuntu 124c757242f8: Pull complete 9d866f8bde2a: Pull complete fa3f2f277e67: Pull complete 398d32b153e8: Pull complete afde35469481: Pull complete Digest: sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378 Status: Downloaded newer image for ubuntu:latest root@yzw-vm:~# docker image inspect ubuntu [ ... \"GraphDriver\": { \"Data\": { \"LowerDir\": \"/var/lib/docker/overlay2/c9f2c1bb3948f7d2ccd2b84e13b317b0a41fb47c35b89bf6d2a7b19f3e81b339/diff:/var/lib/docker/overlay2/b0f87a2c50b9b290269ee5ee55ce06dae5b877dc740cbc9ce3103476818d3438/diff:/var/lib/docker/overlay2/098182c0afb45de2f0f8911a95d3bcb282e130dbeea4f9ea8c1f0899d267b314/diff:/var/lib/docker/overlay2/a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412/diff\", \"MergedDir\": \"/var/lib/docker/overlay2/ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0/merged\", \"UpperDir\": \"/var/lib/docker/overlay2/ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0/diff\", \"WorkDir\": \"/var/lib/docker/overlay2/ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0/work\" }, \"Name\": \"overlay2\" }, \"RootFS\": { \"Type\": \"layers\", \"Layers\": [ \"sha256:a30b835850bfd4c7e9495edf7085cedfad918219227c7157ff71e8afe2661f63\", \"sha256:6267b420796f78004358a36a2dd7ea24640e0d2cd9bbfdba43bb0c140ce73567\", \"sha256:f73b2816c52ac5f8c1f64a1b309b70ff4318d11adff253da4320eee4b3236373\", \"sha256:6a061ee02432e1472146296de3f6dab653f57c109316fa178b40a5052e695e41\", \"sha256:8d7ea83e3c626d5ef1e6a05de454c3fe8b7a567db96293cb094e71930dba387d\" ] }, ... 在/var/lib/docker/overylay2目录下： root@yzw-vm:/var/lib/docker/overlay2# ls -lt total 156 drwx------ 4 root root 4096 Sep 12 11:25 ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0 drwx------ 2 root root 4096 Sep 12 11:25 l drwx------ 4 root root 4096 Sep 12 11:25 c9f2c1bb3948f7d2ccd2b84e13b317b0a41fb47c35b89bf6d2a7b19f3e81b339 drwx------ 4 root root 4096 Sep 12 11:25 b0f87a2c50b9b290269ee5ee55ce06dae5b877dc740cbc9ce3103476818d3438 drwx------ 4 root root 4096 Sep 12 11:25 098182c0afb45de2f0f8911a95d3bcb282e130dbeea4f9ea8c1f0899d267b314 drwx------ 3 root root 4096 Sep 12 11:25 a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412 ... l目录包含了很多软连接，使用短名称指向了其他层。短名称用于避免mount参数时达到页面大小的限制。 root@yzw-vm:/var/lib/docker/overlay2# ls -lt l/ total 152 lrwxrwxrwx 1 root root 72 Sep 12 11:25 AKSNN53ER6ZYNOBNG66LKFO64B -> ../ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0/diff lrwxrwxrwx 1 root root 72 Sep 12 11:25 INN4Q6OLITXODWESIJWDCOU7W5 -> ../c9f2c1bb3948f7d2ccd2b84e13b317b0a41fb47c35b89bf6d2a7b19f3e81b339/diff lrwxrwxrwx 1 root root 72 Sep 12 11:25 USMKDHE2W2PPV64EAYHRKEJQDX -> ../b0f87a2c50b9b290269ee5ee55ce06dae5b877dc740cbc9ce3103476818d3438/diff lrwxrwxrwx 1 root root 72 Sep 12 11:25 JGVKVFIFNC4A2SCJW2MTBLPCJK -> ../098182c0afb45de2f0f8911a95d3bcb282e130dbeea4f9ea8c1f0899d267b314/diff lrwxrwxrwx 1 root root 72 Sep 12 11:25 2XXDFCYHLGAYYD4NHPEYWAQRAN -> ../a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412/diff link文件里是l目录下这个目录的短名称，diff目录就是镜像内容，lower文件指出了该层的组成，由高层到低层；work目录？？？ 最底层目录只有diff目录和link文件。其他底层目录除了diff，link外还有lower文件和work目录。 root@yzw-vm:/var/lib/docker/overlay2# ls ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0/ diff link lower work root@yzw-vm:/var/lib/docker/overlay2# ls c9f2c1bb3948f7d2ccd2b84e13b317b0a41fb47c35b89bf6d2a7b19f3e81b339/ diff link lower work root@yzw-vm:/var/lib/docker/overlay2# ls b0f87a2c50b9b290269ee5ee55ce06dae5b877dc740cbc9ce3103476818d3438/ diff link lower work root@yzw-vm:/var/lib/docker/overlay2# ls 098182c0afb45de2f0f8911a95d3bcb282e130dbeea4f9ea8c1f0899d267b314/ diff link lower work root@yzw-vm:/var/lib/docker/overlay2# ls a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412/ diff link root@yzw-vm:/var/lib/docker/overlay2# ls a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412/diff/ bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@yzw-vm:/var/lib/docker/overlay2# cat a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412/link 2XXDFCYHLGAYYD4NHPEYWAQRAN root@yzw-vm:/var/lib/docker/overlay2# cat ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0/lower l/INN4Q6OLITXODWESIJWDCOU7W5:l/USMKDHE2W2PPV64EAYHRKEJQDX:l/JGVKVFIFNC4A2SCJW2MTBLPCJK:l/2XXDFCYHLGAYYD4NHPEYWAQRAN 容器运行起来后多了2个目录，读写层和初始层： merged（挂载点）： 容器根目录。/var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/merged upperdir（容器层）：初始时这个读写层时空的。/var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/diff lowerdir（镜像层）：总共6层，除了image的5层外，多了一个init层。/var/lib/docker/overlay2/l/LC4OCZQCW7HCEKLXMY4AIYBT7U/var/lib/docker/overlay2/l/AKSNN53ER6ZYNOBNG66LKFO64B/var/lib/docker/overlay2/l/INN4Q6OLITXODWESIJWDCOU7W5/var/lib/docker/overlay2/l/USMKDHE2W2PPV64EAYHRKEJQDX/var/lib/docker/overlay2/l/JGVKVFIFNC4A2SCJW2MTBLPCJK/var/lib/docker/overlay2/l/2XXDFCYHLGAYYD4NHPEYWAQRAN workdir：是用来完成如copy-on_write的操作。/var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/work init层，这个位于只读层和读写层之间，docker项目单独生成的一个内部层，专门用来存/etc/hosts，/etc/resolv.conf等信息，这些原本属于ubuntu镜像一部分，但是用户往往需要在启动容器时写入一些指定的值比如hostname，需要在可读性层对他们进行修改。 这些修改支队当前容器有效，在docker commit时，不会把这一层提交掉，只会提交读写层。 /etc/hosts, /etc/resolv.conf和/etc/hostname，容器中的这三个文件不存在于镜像，而是存在于/var/lib/docker/containers/，在启动容器的时候，通过mount的形式将这些文件挂载到容器内部。 修改容器里面/etc/hostname的内容，会在/var/lib/docker/containers/xxx/hostname看到修改。 容器内部的所有修改都在可读写层，docker commit和push保存的也是这个可读写层。 root@yzw-vm:/var/lib/docker/overlay2# docker run -it -d ubuntu /bin/bash 13d8a0e5162bfa3e087d20eef15a3805136ba2c2d6413356078ada71886639d4 root@yzw-vm:/var/lib/docker/overlay2# df -h Filesystem Size Used Avail Use% Mounted on ... tmpfs 395M 28K 395M 1% /run/user/120 tmpfs 395M 28K 395M 1% /run/user/1000 tmpfs 395M 0 395M 0% /run/user/0 overlay 59G 22G 35G 38% /var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/merged shm 64M 0 64M 0% /var/lib/docker/containers/13d8a0e5162bfa3e087d20eef15a3805136ba2c2d6413356078ada71886639d4/mounts/shm root@yzw-vm:/var/lib/docker/overlay2# mount | grep overlay overlay on /var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/merged type overlay (rw,relatime,lowerdir=/var/lib/docker/overlay2/l/LC4OCZQCW7HCEKLXMY4AIYBT7U:/var/lib/docker/overlay2/l/AKSNN53ER6ZYNOBNG66LKFO64B:/var/lib/docker/overlay2/l/INN4Q6OLITXODWESIJWDCOU7W5:/var/lib/docker/overlay2/l/USMKDHE2W2PPV64EAYHRKEJQDX:/var/lib/docker/overlay2/l/JGVKVFIFNC4A2SCJW2MTBLPCJK:/var/lib/docker/overlay2/l/2XXDFCYHLGAYYD4NHPEYWAQRAN,upperdir=/var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/diff,workdir=/var/lib/docker/overlay2/e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/work) 在容器内部可以看到overlay和shm两个文件挂载到根目录和/dev/shm目录 root@13d8a0e5162b:~# df -h Filesystem Size Used Avail Use% Mounted on overlay 59G 22G 35G 38% / tmpfs 64M 0 64M 0% /dev tmpfs 2.0G 0 2.0G 0% /sys/fs/cgroup /dev/sda1 59G 22G 35G 38% /etc/hosts shm 64M 0 64M 0% /dev/shm tmpfs 2.0G 0 2.0G 0% /proc/acpi tmpfs 2.0G 0 2.0G 0% /proc/scsi tmpfs 2.0G 0 2.0G 0% /sys/firmware 各个layer目录： root@yzw-vm:/var/lib/docker/overlay2# ls -lt total 32 drwx------ 5 root root 4096 Sep 12 14:23 e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a drwx------ 4 root root 4096 Sep 12 14:23 e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init drwx------ 2 root root 4096 Sep 12 14:23 l drwx------ 4 root root 4096 Sep 12 11:25 ff432dd74bd7f4b941c667517c450bb75e7af61eda23a005d75db6e9b812d0e0 drwx------ 4 root root 4096 Sep 12 11:25 c9f2c1bb3948f7d2ccd2b84e13b317b0a41fb47c35b89bf6d2a7b19f3e81b339 drwx------ 4 root root 4096 Sep 12 11:25 b0f87a2c50b9b290269ee5ee55ce06dae5b877dc740cbc9ce3103476818d3438 drwx------ 4 root root 4096 Sep 12 11:25 098182c0afb45de2f0f8911a95d3bcb282e130dbeea4f9ea8c1f0899d267b314 drwx------ 3 root root 4096 Sep 12 11:25 a708ff445d90980fd58da00dd9b2fc2917ef61cb3b1aaed9bda7ea44aba12412 root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a diff link lower merged work root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/diff/ root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/merged/ bin boot dev etc home lib lib64 media mnt opt proc root run sbin srv sys tmp usr var root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/work/ work root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/work/work/ root@yzw-vm:/var/lib/docker/overlay2# cat e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/link A4DRDBGDJJI2XIR7VGQ4T2WOFF root@yzw-vm:/var/lib/docker/overlay2# cat e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a/lower l/LC4OCZQCW7HCEKLXMY4AIYBT7U:l/AKSNN53ER6ZYNOBNG66LKFO64B:l/INN4Q6OLITXODWESIJWDCOU7W5:l/USMKDHE2W2PPV64EAYHRKEJQDX:l/JGVKVFIFNC4A2SCJW2MTBLPCJK:l/2XXDFCYHLGAYYD4NHPEYWAQRAN root@yzw-vm:/var/lib/docker/overlay2# root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init/ diff link lower work root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init/diff/ dev etc root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init/work/ work root@yzw-vm:/var/lib/docker/overlay2# ls e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init/work/work/ root@yzw-vm:/var/lib/docker/overlay2# root@yzw-vm:/var/lib/docker/overlay2# cat e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init/link LC4OCZQCW7HCEKLXMY4AIYBT7U root@yzw-vm:/var/lib/docker/overlay2# cat e6f0c6c4c30099c6903e0c65fdeb46de4f9ed4d1d71338a54480f66f0b00010a-init/lower l/AKSNN53ER6ZYNOBNG66LKFO64B:l/INN4Q6OLITXODWESIJWDCOU7W5:l/USMKDHE2W2PPV64EAYHRKEJQDX:l/JGVKVFIFNC4A2SCJW2MTBLPCJK:l/2XXDFCYHLGAYYD4NHPEYWAQRAN root@yzw-vm:/var/lib/docker/overlay2# 容器使用overlay读写 读容器层不存在的文件：从镜像层中读取，有小的性能损耗。 读只存在于容器层的文件：直接读取，不拷贝，无额外性能损耗。 读同时存在于容器和镜像层的文件：直接读取容器层文件，因为容器层隐藏了镜像层的同名文件。无额外性能损耗。 写一个文件： 容器写一个已经存在的文件，容器层不存在这个文件，overlay会将整个文件拷贝到容器层进行修改，拷贝只发生在第一次写入文件时，后面就直接容器层写了。OverlayFS只工作在两层中。这比AUFS要在多层镜像中查找时性能要好。 删除文件和目录：删除文件时，容器层会镜像层创建一个whiteout文件，而镜像层的文件并没有被删，whiteout文件会隐藏它。例如删除一个foo文件，容器层创建一个.wh.foo的文件，当这两层被联合挂载之后，foo文件就会被.wh.foo文件遮挡起来消失了。翻译成“白障”。 删除目录时，容器层会创建一个不透明目录，和whiteout文件隐藏镜像层文件类似。 重命名目录：只有在源路径和目的路径都在顶层容器层时，才允许执行rename操作。否则，会返回EXDEV。因此，你的应用需要能够处理EXDEV，并且回滚操作，执行替代的“拷贝和删除”策略。（没理解） 性能 引用: https://arkingc.github.io/2017/05/05/docker-filesystem-overlay/ 一般来说，overlay/overlay2很快，几乎肯定比aufs和devicemapper快。在某些特定场景下，还可能比btrfs快。 此外，还有几点overlay/overlay2驱动性能相关的注意事项： 页缓存：OverlayFS支持页缓存共享，意味着多个容器访问相同的文件能够共享一个单一的page cache entry。 使得overlay/overlay2驱动能高效使用内存，是PaaS以及其它高密度场景一个好的选择。 copy_up：对镜像层大文件进行写操作时，copy-on-write会给写操作带来大量延迟。 inode 限制：使用overlay会引起过度的inode消耗，消耗会随着主机上的镜像和容器的增加而增加。 拥有大量镜像的主机在大量容器启动和停止时可能会耗尽inodes。 不幸的是你只能在文件系统创建时指定inode数，因此你可能需要考虑将/var/lib/docker放在另一个独立的设备上， 或者在创建文件系统时手动修改inode值。而overlay2则没有这样的问题。 RPM和Yum：OverlayFS仅实现了POSIX标准的一部分，某些操作还会违反POSIX标准，copy_up操作就是其中一个。 下面是提升OverlayFS驱动性能的最佳实践。 SSD：为了获得最佳性能，一个通常的想法是使用诸如SSD这类更快的存储设备； 使用数据卷： 数据卷提供了最好的以及最可预见的性能。 因为绕过了存储驱动，因此不会存在瘦供给和copy-on-write带来的潜在性能开销。 因此，写操作较频繁的数据应该放在数据卷上。 Dockerfile python:2.7-slim基础镜像一共有4层layer。 root@yzw-vm:/home/yzw/docker# docker images REPOSITORY TAG IMAGE ID CREATED SIZE python 2.7-slim c9cde4658340 7 days ago 120MB root@yzw-vm:/home/yzw/docker# ls -l /var/lib/docker/overlay2/ total 20 drwx------ 4 root root 4096 Sep 12 22:07 0111758e6c9c78c7ae33a181232bf15aa1fc9c314d1b9cacd9506541065a7379 drwx------ 2 root root 4096 Sep 12 22:07 l drwx------ 4 root root 4096 Sep 12 22:07 4560378751a6edaa1b7ed848d632310eaf96431d9643792a5d2462b315263c0e drwx------ 4 root root 4096 Sep 12 22:07 f6a44ab116768ddf5ba69b2f9601a627ececb2d9d20b6a5956bf97818fd46691 drwx------ 3 root root 4096 Sep 12 22:07 897596fd5f4efb3890c0801cb3813374fb9c40513f172bf2bf05f3f0c9e37d4b 基于python:2.7-slim基础镜像build一个新的镜像 root@yzw-vm:/home/yzw/docker# cat Dockerfile FROM python:2.7-slim WORKDIR /app ADD . /app RUN pip install --trusted-host pypi.python.org -r requirements.txt EXPOSE 80 ENV NAME World CMD [\"python\",\"app.py\"] root@yzw-vm:/home/yzw/docker# docker build -t helloworld . ... root@yzw-vm:/home/yzw/docker# docker images REPOSITORY TAG IMAGE ID CREATED SIZE helloword latest 7c6f3f79a9d1 30 minutes ago 131MB python 2.7-slim c9cde4658340 7 days ago 120MB 在python基础镜像的基础上多出了3个layer，WORKDIR，ADD，RUN每一个操作会生成一个layer root@yzw-vm:/var/lib/docker/overlay2# ls -lt total 32 drwx------ 2 root root 4096 Sep 12 22:10 l drwx------ 4 root root 4096 Sep 12 22:10 4463cacc12fd606d136c25228be202afba963954ea3b90b9aa22e760230eb559 drwx------ 4 root root 4096 Sep 12 22:09 1c6de6d90e60da84614b9ffe7147ccc3465e9d301445161f1c8d0b8bdb3e8044 drwx------ 4 root root 4096 Sep 12 22:09 44744bbff15cfb1a73c168cee6d23b4e99458d5c4a72199a310016d7ebd4e28d drwx------ 4 root root 4096 Sep 12 22:07 0111758e6c9c78c7ae33a181232bf15aa1fc9c314d1b9cacd9506541065a7379 drwx------ 4 root root 4096 Sep 12 22:07 4560378751a6edaa1b7ed848d632310eaf96431d9643792a5d2462b315263c0e drwx------ 4 root root 4096 Sep 12 22:07 f6a44ab116768ddf5ba69b2f9601a627ececb2d9d20b6a5956bf97818fd46691 drwx------ 3 root root 4096 Sep 12 22:07 897596fd5f4efb3890c0801cb3813374fb9c40513f172bf2bf05f3f0c9e37d4b root@yzw-vm:/var/lib/docker/overlay2# ls -lt 44744bbff15cfb1a73c168cee6d23b4e99458d5c4a72199a310016d7ebd4e28d/diff/app total 0 root@yzw-vm:/var/lib/docker/overlay2# ls -lt 1c6de6d90e60da84614b9ffe7147ccc3465e9d301445161f1c8d0b8bdb3e8044/diff/app/ total 12 -rw-r--r-- 1 root root 168 Sep 12 22:08 Dockerfile -rw-r--r-- 1 root root 6 Sep 12 22:01 requirements.txt -rw-rw-r-- 1 root root 324 Sep 12 21:59 app.py root@yzw-vm:/var/lib/docker/overlay2# ls -lt 4463cacc12fd606d136c25228be202afba963954ea3b90b9aa22e760230eb559/diff total 12 drwxrwxrwt 2 root root 4096 Sep 12 22:10 tmp drwx------ 3 root root 4096 Sep 12 22:09 root drwxr-xr-x 3 root root 4096 Aug 31 08:00 usr powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/kubeadm-v1.13.0.html":{"url":"kubernetes/kubeadm-v1.13.0.html","title":"Kubeadm Install","keywords":"","body":"install and enable docker-ce https://docs.docker.com/install/linux/docker-ce/centos/#install-docker-ce-1 setup http_proxy global proxy is only used to install app by yum. $ export http_proxy=http://192.168.137.1:7777 $ export https_proxy=http:/192.168.137.1:7777 setup docker proxy $ vim /usr/lib/systemd/system/docker.service [Service] ... Environment=\"HTTP_PROXY=http://192.168.137.1:7777/\" \"HTTPS_PROXY=http://192.168.137.1:7777\" \"NO_PROXY=localhost,127.0.0.1,registry.docker-cn.com\" $ systemctl daemon-reload $ systemctl restart docker $ systemctl show --property=Environment docker Environment=HTTP_PROXY=http://192.168.137.1:7777/ HTTPS_PROXY=https://192.168.137.1:7777 NO_PROXY=localhost,127.0.0.1,registry.docker-cn.com $ docker pull k8s.gcr.io/kube-apiserver-amd64:v1.13.0 v1.13.0: Pulling from kube-apiserver-amd64 73e3e9d78c61: Pull complete bef6770497e3: Pull complete Digest: sha256:f88cb526ae4346a682d759397c085d6aba829748b862db8feeca5ff99330482f Status: Downloaded newer image for k8s.gcr.io/kube-apiserver-amd64:v1.13.0 install kubectl kubeadm kubelet https://kubernetes.io/docs/setup/independent/install-kubeadm/ $ cat /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kube* EOF $ setenforce 0 $ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config $ systemctl stop firewalld $ systemctl disable firewalld $ yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes $ systemctl enable kubelet && systemctl start kubelet install master kubeadm init [root@node11 ~]# kubeadm init --pod-network-cidr=111.111.0.0/16 [init] Using Kubernetes version: v1.13.0 [preflight] Running pre-flight checks [WARNING Hostname]: hostname \"node11\" could not be reached [WARNING Hostname]: hostname \"node11\": lookup node11 on 192.168.137.1:53: server misbehaving [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [node11 localhost] and IPs [192.168.137.111 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [node11 localhost] and IPs [192.168.137.111 127.0.0.1 ::1] [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [node11 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.137.111] [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 25.513024 seconds [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.13\" in namespace kube-system with the configuration for the kubelets in the cluster [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"node11\" as an annotation [mark-control-plane] Marking the node node11 as control-plane by adding the label \"node-role.kubernetes.io/master=''\" [mark-control-plane] Marking the node node11 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule] [bootstrap-token] Using token: o86g4m.6neumrafpc9n0zcw [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.137.111:6443 --token o86g4m.6neumrafpc9n0zcw --discovery-token-ca-cert-hash sha256:9b1f749a9dd839529b995bbd77576daaa3e3a4edcde2e234f4fd379428fe4341 [root@node11 ~]# mkdir -p $HOME/.kube [root@node11 ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@node11 ~]# chown $(id -u):$(id -g) $HOME/.kube/config issue [root@node11 ~]# kubeadm init --pod-network-cidr=111.111.0.0/16 [init] Using Kubernetes version: v1.13.0 [preflight] Running pre-flight checks [WARNING Firewalld]: firewalld is active, please ensure ports [6443 10250] are open or your cluster may not function correctly [WARNING HTTPProxy]: Connection to \"https://192.168.137.111\" uses proxy \"http://192.168.137.1:7777\". If that is not intended, adjust your proxy settings [WARNING HTTPProxyCIDR]: connection to \"10.96.0.0/12\" uses proxy \"http://192.168.137.1:7777\". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration [WARNING HTTPProxyCIDR]: connection to \"111.111.0.0/16\" uses proxy \"http://192.168.137.1:7777\". This may lead to malfunctional cluster setup. Make sure that Pod and Services IP ranges specified correctly as exceptions in proxy configuration [WARNING Hostname]: hostname \"node11\" could not be reached [WARNING Hostname]: hostname \"node11\": lookup node11 on 192.168.137.1:53: server misbehaving error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2 [ERROR Swap]: running with swap on is not supported. Please disable swap [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` ### resolve # 1 add cpu # 2 swapoff -a && sed -i '/swap/d' /etc/fstab install calico Installing with the Kubernetes API datastore—50 nodes or less https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-networkhttps://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/calico # kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml # kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml The default CALICO_IPV4POOL_CIDR in calico.yaml is \"192.168.0.0/16\", because we pass --pod-network-cidr=192.168.0.0/16 to kubeadm by default.if pod-network-cidr is not default value, you need to change the value of CALICO_IPV4POOL_CIDR in calico.yaml. Installing with the Kubernetes API datastore—more than 50 nodes https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/calico # kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-node-xjhgm 2/2 Running 0 4m38s kube-system coredns-86c58d9df4-hm94l 1/1 Running 0 3h32m kube-system coredns-86c58d9df4-mztp6 1/1 Running 0 3h32m kube-system etcd-node11 1/1 Running 0 3h31m kube-system kube-apiserver-node11 1/1 Running 0 3h31m kube-system kube-controller-manager-node11 1/1 Running 0 3h31m kube-system kube-proxy-nvg9v 1/1 Running 0 3h32m kube-system kube-scheduler-node11 1/1 Running 0 3h31m # kubectl get nodes NAME STATUS ROLES AGE VERSION node11 Ready master 3h51m v1.13.0 install nodes on nodes # kubeadm join 192.168.137.111:6443 --token o86g4m.6neumrafpc9n0zcw --discovery-token-ca-cert-hash sha256:9b1f749a9dd839529b995bbd77576daaa3e3a4edcde2e234f4fd379428fe4341 [preflight] Running pre-flight checks [WARNING Hostname]: hostname \"node12\" could not be reached [WARNING Hostname]: hostname \"node12\": lookup node12 on 114.114.114.114:53: no such host [discovery] Trying to connect to API Server \"192.168.137.111:6443\" [discovery] Created cluster-info discovery client, requesting info from \"https://192.168.137.111:6443\" [discovery] Requesting info from \"https://192.168.137.111:6443\" again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"192.168.137.111:6443\" [discovery] Successfully established connection with API Server \"192.168.137.111:6443\" [join] Reading configuration from the cluster... [join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet] Downloading configuration for the kubelet from the \"kubelet-config-1.13\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"node12\" as an annotation issue failed to request cluster info # kubeadm join 192.168.137.111:6443 --token o86g4m.6neumrafpc9n0zcw --discovery-token-ca-cert-hash sha256:9b1f749a9dd839529b995bbd77576daaa3e3a4edcde2e234f4fd379428fe4341 [preflight] Running pre-flight checks [WARNING Hostname]: hostname \"node12\" could not be reached [WARNING Hostname]: hostname \"node12\": lookup node12 on 114.114.114.114:53: no such host [WARNING HTTPProxy]: Connection to \"https://192.168.137.111\" uses proxy \"http://192.168.137.1:7777\". If that is not intended, adjust your proxy settings [discovery] Trying to connect to API Server \"192.168.137.111:6443\" [discovery] Created cluster-info discovery client, requesting info from \"https://192.168.137.111:6443\" [discovery] Failed to request cluster info, will try again: [Get https://192.168.137.111:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: Forbidden] [discovery] Failed to request cluster info, will try again: [Get https://192.168.137.111:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: Forbidden] resolve https://github.com/kubernetes/kubeadm/issues/299 # unset http_proxy # unset https_proxy calico node CrashLoopBackOff # kubectl get pods --all-namespaces NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-node-4mq2w 1/2 CrashLoopBackOff 6 16m kube-system calico-node-m6s96 1/2 CrashLoopBackOff 7 19m # kubectl describe pod calico-node-m6s96 -n kube-system Name: calico-node-m6s96 ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 21m default-scheduler Successfully assigned kube-system/calico-node-m6s96 to node12 Normal Pulling 21m kubelet, node12 pulling image \"quay.io/calico/cni:v3.3.2\" Normal Started 16m kubelet, node12 Started container Normal Pulled 16m kubelet, node12 Successfully pulled image \"quay.io/calico/cni:v3.3.2\" Normal Created 16m kubelet, node12 Created container Normal Created 15m (x2 over 21m) kubelet, node12 Created container Normal Started 15m (x2 over 21m) kubelet, node12 Started container Normal Pulled 15m (x2 over 21m) kubelet, node12 Container image \"quay.io/calico/node:v3.3.2\" already present on machine Normal Killing 15m kubelet, node12 Killing container with id docker://calico-node:Container failed liveness probe.. Container will be killed and recreated. Warning Unhealthy 14m (x7 over 16m) kubelet, node12 Liveness probe failed: Get http://localhost:9099/liveness: dial tcp [::1]:9099: connect: connection refused Warning Unhealthy 6m14s (x51 over 16m) kubelet, node12 Readiness probe failed: calico/node is not ready: felix is not ready: Get http://localhost:9099/readiness: dial tcp [::1]:9099: connect: connection refused Warning BackOff 66s (x30 over 9m22s) kubelet, node12 Back-off restarting failed container resolve not resolved yet. powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/kube.html":{"url":"kubernetes/kube.html","title":"kube","keywords":"","body":"API Server 访问API server kubectl 命令访问 #打开调试开关 kubectl -v=8 get pods kubectl get --raw /api/v1/namespaces REST API client库 swagger-ui apiserver 启动参数 --insecure-bind-address=0.0.0.0 The IP address on which to serve the --insecure-port (set to 0.0.0.0 for all interfaces). (default 127.0.0.1) --insecure-port=8080 --enable-swagger-ui=true 浏览器打开： http://192.168.137.101:8080/swagger-ui/ http://192.168.137.101:8080/swagger.json http://192.168.137.101:8080/swaggerapi 访问控制 Kubernetes API 的每个请求都会经过多阶段的访问控制之后才会被接受，这包括认证、授权以及准入控制（Admission Control）等。 认证 失败返回401 授权 失败返回403 资源配额 http://docs.kubernetes.org.cn/750.html kube-schedule 负责分配调度 Pod 到集群内的节点上，它监听 kube-apiserver，查询还未分配 Node 的 Pod，然后根据调度策略为这些 Pod 分配节点（更新 Pod 的 NodeName 字段） 指定Node节点调度 nodeSelector 只调度到匹配指定 label 的 Node 上 # node 打label kubectl label nodes node-01 disktype=ssd # pod 设置 spec: nodeSelector: disktype: ssd 参考 https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ nodeAffinity podAffinity taints 用于node NoSchedule：新的 Pod 不调度到该 Node 上，不影响正在运行的 Pod PreferNoSchedule：soft 版的 NoSchedule，尽量不调度到该 Node 上 NoExecute：新的 Pod 不调度到该 Node 上，并且删除（evict）已在运行的 Pod。Pod 可以增加一个时间（tolerationSeconds） #设置 taint kubectl taint nodes node1 key1=value1:NoSchedule kubectl taint nodes node1 key1=value1:NoExecute kubectl taint nodes node1 key2=value2:NoSchedule #删除 taint kubectl taint nodes node1 key1:NoSchedule- kubectl taint nodes node1 key1:NoExecute- kubectl taint nodes node1 key2:NoSchedule- #查询 [root@lzg-test-dnw55arvno6m-master-0 kubernetes]# kubectl describe nodes lzg-test-dnw55arvno6m-master-0 Name: lzg-test-dnw55arvno6m-master-0 Role: Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux dedicated=master failure-domain.beta.kubernetes.io/zone=nova kubernetes.io/hostname=lzg-test-dnw55arvno6m-master-0 Annotations: node-mgnt-ip=172.16.106.102 node.alpha.kubernetes.io/ttl=0 volumes.kubernetes.io/controller-managed-attach-detach=true Taints: dedicated=master:NoSchedule tolerations 用于pod 配了tolerations 可以被分配到taints的节点，也可以分配到其他节点，如果您希望这些 pod 只能被分配到上述专用节点，那么您还需要给这些专用节点另外添加一个和上述 taint 类似的 label。 tolerations: - key: \"key1\" operator: \"Equal\" value: \"value1\" effect: \"NoExecute\" tolerationSeconds: 3600 tolerations: - key: \"node.alpha.kubernetes.io/unreachable\" operator: \"Exists\" effect: \"NoExecute\" tolerationSeconds: 6000 参考： https://k8smeetup.github.io/docs/concepts/configuration/taint-and-toleration/ http://blog.51cto.com/nosmoking/2097380 https://blog.csdn.net/yevvzi/article/details/77966171 kubectl cordon/uncordon node 禁止/取消禁止pod调度到该节点 kubectl drain node 驱逐该节点上的所有pod 参考 https://kubernetes.feisky.xyz/zh/components/scheduler.html https://jimmysong.io/kubernetes-handbook/concepts/node.html https://jimmysong.io/kubernetes-handbook/concepts/taint-and-toleration.html https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/ https://k8smeetup.github.io/docs/concepts/configuration/taint-and-toleration/ kube-controller-manager controller manager由一系列控制器组成 Replication Controller Node Controller CronJob Controller Daemon Controller Deployment Controller Endpoint Controller Garbage Collector Namespace Controller Job Controller Pod AutoScaler RelicaSet Service Controller ServiceAccount Controller StatefulSet Controller Volume Controller Resource quota Controller 高可用 在启动时设置 --leader-elect=true 后，controller manager 会使用多节点选主的方式选择主节点。只有主节点才会调用 StartControllers() 启动所有控制器，而其他从节点则仅执行选主算法。 参考 https://kubernetes.feisky.xyz/zh/components/controller-manager.html kubelet 每个节点上都运行一个 kubelet 服务进程，默认监听 10250 端口，接收并执行 master 发来的指令，管理 Pod 及 Pod 中的容器。每个 kubelet 进程会在 API Server 上注册节点自身信息，定期向 master 节点汇报节点的资源使用情况，并通过 cAdvisor 监控节点和容器的资源。 端口 10250端口 cAdvisor访问端口4149： http://192.168.137.101:4194/containers/ 10255端口 静态pod 配置清单位置：--pod-manifest-path=/etc/kubernetes/manifests kubelet将会周期扫描这个目录，根据这个目录下出现或消失的YAML/JSON文件来创建或删除静态pod 我们不能通过API服务器来删除静态pod（例如，通过 kubectl 命令），kebelet不会删除它。 参考 https://kubernetes.io/docs/tasks/administer-cluster/static-pod/ powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/kubectl.html":{"url":"kubernetes/kubectl.html","title":"Kubectl","keywords":"","body":"kubectl bash completion https://kubernetes.io/docs/tasks/tools/install-kubectl/#enabling-shell-autocompletion install and configure on centos $ yum install -y bash-completion $ source > ~/.bashrc for more details, consult ==kubectl completion -h== issue $ kubectl bash: _get_comp_words_by_ref: command not found solution $ source /etc/bash_completion or $ source /etc/profile.d/bash_completion.sh and then $ source usage powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/kube-apiserver.html":{"url":"kubernetes/kube-apiserver.html","title":"Kube-apiserver","keywords":"","body":"API Server 提供了k8s各类资源对象（pod,RC,Service等）的增删改查及watch等REST接口，是整个系统的数据总线和数据中心。 集群管理的API接口（包括授权，认证，访问控制等） 其他模块间数据通信的枢纽（只有API Server才能直接操作etcd） 资源配额控制入口； 启动参数 $ cat /etc/kubernetes/manifests/kube-apiserver.manifest ... command: - /hyperkube - apiserver - --advertise-address=192.168.137.101 - --etcd-servers=https://192.168.137.101:2379,https://192.168.137.102:2379,https://192.168.137.103:2379 - --etcd-quorum-read=true - --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem - --etcd-certfile=/etc/ssl/etcd/ssl/node-node2.pem - --etcd-keyfile=/etc/ssl/etcd/ssl/node-node2-key.pem - --insecure-bind-address=127.0.0.1 - --apiserver-count=2 - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota - --service-cluster-ip-range=10.233.0.0/18 - --service-node-port-range=30000-32767 - --client-ca-file=/etc/kubernetes/ssl/ca.pem - --basic-auth-file=/etc/kubernetes/users/known_users.csv - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem - --token-auth-file=/etc/kubernetes/tokens/known_tokens.csv - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem - --secure-port=6443 - --insecure-port=8080 - --storage-backend=etcd3 - --v=2 - --allow-privileged=true - --anonymous-auth=False API说明 API groups core group. path: /api/v1, aipVersion:v1 other group. path: /apis/$GROUP_NAME/$VERSION, apiVersion: $GROUP_NAME/$VERSION ## api [root@node1 ~]# curl localhost:8080/ { \"paths\": [ \"/api\", \"/api/v1\", \"/apis\", \"/apis/\", \"/apis/apiextensions.k8s.io\", \"/apis/apiextensions.k8s.io/v1beta1\", \"/apis/apiregistration.k8s.io\", \"/apis/apiregistration.k8s.io/v1beta1\", \"/apis/apps\", \"/apis/apps/v1beta1\", ... ## version [root@node1 ~]# curl localhost:8080/version ## core group [root@node1 ~]# curl localhost:8080/api [root@node1 ~]# curl localhost:8080/api/v1 [root@node1 ~]# curl localhost:8080/api/v1/pods [root@node1 ~]# curl localhost:8080/api/v1/pods/status ## other group [root@node1 ~]# curl localhost:8080/apis [root@node1 ~]# curl localhost:8080/apis/apiextensions.k8s.io [root@node1 ~]# curl localhost:8080/apis/apiextensions.k8s.io/v1beta1 启用禁用API groups 默认API groups都是是启动的,通用的group的资源也是默认启动的，可以通过apiserver启动参数 --runtime-config来设置group以及其资源。 --runtime-config=batch/v1=false,batch/v2alpha1 --runtime-config=extensions/v1beta1/deployments=false,extensions/v1beta1/ingress=false 修改后需要重启apiserver和controller-manager。 参考 https://kubernetes.io/docs/concepts/overview/kubernetes-api/ 访问API Server 默认情况下，kube-apiserver进程在本机通过： insecure-bind-address:insecure-port（127.0.0.1:8080或者localhost:8080）端口提供非安全认证的REST服务； secure-port（6443）端口提供HTTPS安全服务；==（用的哪个IP？？）== kubectl 命令访问 kubectrl 跟api service 也是通过REST方式通信的。 打开调试开关 ``` [root@node1 ~]# kubectl -v=8 get pods I0704 09:57:25.641993 8087 loader.go:357] Config loaded from file /root/.kube/config I0704 09:57:25.754547 8087 round_trippers.go:383] GET https://192.168.137.101:6443/api I0704 09:57:25.754574 8087 round_trippers.go:390] Request Headers: I0704 09:57:25.754588 8087 round_trippers.go:393] Accept: application/json, / I0704 09:57:25.754600 8087 round_trippers.go:393] User-Agent: kubectl/v1.7.5+coreos.0 (linux/amd64) kubernetes/070d238 I0704 09:57:25.766195 8087 round_trippers.go:408] Response Status: 200 OK in 11 milliseconds I0704 09:57:25.766224 8087 round_trippers.go:411] Response Headers: I0704 09:57:25.766229 8087 round_trippers.go:414] Date: Wed, 04 Jul 2018 01:57:25 GMT I0704 09:57:25.766234 8087 round_trippers.go:414] Content-Type: application/json I0704 09:57:25.766237 8087 round_trippers.go:414] Content-Length: 138 I0704 09:57:25.766272 8087 request.go:991] Response Body: {\"kind\":\"APIVersions\",\"versions\":[\"v1\"],\"serverAddressByClientCIDRs\":[{\"clientCIDR\":\"0.0.0.0/0\",\"serverAddress\":\"192.168.137.101:6443\"}]} I0704 09:57:25.766389 8087 round_trippers.go:383] GET https://192.168.137.101:6443/apis I0704 09:57:25.766395 8087 round_trippers.go:390] Request Headers: I0704 09:57:25.766400 8087 round_trippers.go:393] Accept: application/json, / I0704 09:57:25.766419 8087 round_trippers.go:393] User-Agent: kubectl/v1.7.5+coreos.0 (linux/amd64) kubernetes/070d238 I0704 09:57:25.767277 8087 round_trippers.go:408] Response Status: 200 OK in 0 milliseconds I0704 09:57:25.767367 8087 round_trippers.go:411] Response Headers: I0704 09:57:25.767372 8087 round_trippers.go:414] Content-Type: application/json I0704 09:57:25.767376 8087 round_trippers.go:414] Content-Length: 3274 I0704 09:57:25.767381 8087 round_trippers.go:414] Date: Wed, 04 Jul 2018 01:57:25 GMT * kubectl 直接访问API 版本信息 [root@node1 ~]# kubectl get --raw /api {\"kind\":\"APIVersions\",\"versions\":[\"v1\"],\"serverAddressByClientCIDRs\":[{\"clientCIDR\":\"0.0.0.0/0\",\"serverAddress\":\"192.168.137.101:6443\"}]} 支持的资源 [root@node1 ~]# kubectl get --raw /api/v1 集群资源 [root@node1 ~]# kubectl get --raw /api/v1/pods [root@node1 ~]# kubectl get --raw /api/v1/services [root@node1 ~]# kubectl get --raw /api/v1/replicationcontrollers #### REST API > **api-reference:** > https://v1-7.docs.kubernetes.io/docs/api-reference/v1.7/ ##### swagger-ui * apiserver 启动参数 > * ***--insecure-bind-address=0.0.0.0*** The IP address on which to serve the --insecure-port (set to 0.0.0.0 for all interfaces). (default 127.0.0.1) > * ***--insecure-port=8080*** > * ***--enable-swagger-ui=true*** * 浏览器打开： > * ***http://192.168.137.101:8080/swagger-ui/*** > * ***http://192.168.137.101:8080/swagger.json*** > * ***http://192.168.137.101:8080/swaggerapi*** ##### API访问 Kubernetes API 的每个请求都会经过多阶段的访问控制之后才会被接受，这包括认证、授权以及准入控制（Admission Control）等。 * 认证 失败返回401 * 授权 失败返回403 [root@node1 ~]# curl https://node1:6443/api/v1/nodes Unauthorized [root@node1 ~]# curl localhost:8080/api { \"kind\": \"APIVersions\", \"versions\": [ \"v1\" ], \"serverAddressByClientCIDRs\": [ { \"clientCIDR\": \"0.0.0.0/0\", \"serverAddress\": \"192.168.137.101:6443\" } ] CA [root@node test]# curl https://node1:6443/api/v1/nodes --cert ./test.pem --key ./test-key.pem --cacert ./ca.pem token [root@node3 test]# curl -k --header \"Authorization: Bearer UPVzqw3LVaqpPCrijfuc2rwKadjvGNUq\" https://node1:6443/api/v1/nodes password [root@node3 test]# ./kubectl --server=https://node1:6443 get nodes Please enter Username: kube Please enter Password: * ##### client库 通过编程的方式访问，这种方式有两种使用场景： * pod需要发现同属于一个service的其它pod副本的信息来组建集群（如elasticsearch集群）； * 需要通过调用api来我开发基于k8s的管理平台； pod中的进程通过API server的service来访问API Server。 [root@node1 ~]# kubectl get services NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes 10.233.0.1 443/TCP 27d client-go ：https://github.com/kubernetes/client-go ##### kubectl proxy 启动内部代理，然后访问内部代理进行调用，可以进行访问控制，如限制要访问的接口，增加允许访问的client的白名单。 8001端口启动代理，拒绝客户端访问pods [root@node1 ~]# kubectl proxy --reject-paths=\"^/api/v1/pods\" --port=8001 --v=2 [root@node1 ~]# curl localhost:8001/api/v1/pods Unauthorized [root@node1 ~]# #### proxy API接口 API Server把收到的REST请求转发到某个node上的kubelet进程的REST端口上，由kubelet进程负责响应。 这里获取的数据来源于node，并非ETCD，会有时间上偏差。 [root@node1 ~]# curl localhost:8080/api/v1/proxy/nodes/node3/pods/ [root@node1 ~]# curl localhost:8080/api/v1/proxy/nodes/node3/stats/ [root@node1 ~]# curl localhost:8080/api/v1/proxy/nodes/node3/spec/ ### 资源配额 ### 其他模块通信 * kubelet --> apiserver (定期调用REST接口汇报自身状态，API server更新到etcd中， watch监听pod信息，监听到有pod被调度到本节点，或者本节点pod被删或者修改后执行相应操作) * controller-manager --> apiserver (node controller 通过watch接口实时监控Node的信息，并做相应的处理。) * scheduler --> apiserver （watch侦听新建pod的信息， 检索符合要求的node列表，并进行调度逻辑的执行） ==为缓解apiserver的压力， 各个模块会缓存从apiserver获取的数据， 某些情况下会直接使用缓存的数据（什么情况使用缓存， 如何保证数据的一致性？）== [root@node3 node1_6443]# ll /root/.kube/cache/discovery/node1_6443/ total 64 drwxr-xr-x. 3 root root 4096 Jun 27 16:35 apiextensions.k8s.io drwxr-xr-x. 3 root root 4096 Jun 27 16:35 apiregistration.k8s.io drwxr-xr-x. 3 root root 4096 Jun 27 16:35 apps drwxr-xr-x. 4 root root 4096 Jun 27 16:35 authentication.k8s.io drwxr-xr-x. 4 root root 4096 Jun 27 16:35 authorization.k8s.io drwxr-xr-x. 3 root root 4096 Jun 27 16:35 autoscaling drwxr-xr-x. 3 root root 4096 Jun 27 16:35 batch drwxr-xr-x. 3 root root 4096 Jun 27 16:35 certificates.k8s.io drwxr-xr-x. 3 root root 4096 Jun 27 16:35 extensions drwxr-xr-x. 3 root root 4096 Jun 27 16:35 networking.k8s.io drwxr-xr-x. 3 root root 4096 Jun 27 16:35 policy drwxr-xr-x. 4 root root 4096 Jun 27 16:35 rbac.authorization.k8s.io -rwxr-xr-x. 1 root root 3426 Jun 27 16:35 servergroups.json drwxr-xr-x. 3 root root 4096 Jun 27 16:35 settings.k8s.io drwxr-xr-x. 4 root root 4096 Jun 27 16:35 storage.k8s.io drwxr-xr-x. 2 root root 4096 Jun 27 16:35 v1 ``` 参考 http://docs.kubernetes.org.cn/750.html http://www.huweihuang.com/article/kubernetes/core-principle/kubernetes-core-principle-api-server/ https://github.com/feiskyer/kubernetes-handbook/blob/master/zh/components/apiserver.md 源码分析 command github.com/spf13/pflag http go-restful github.com/emicklei/go-restful etcd powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/kube-dns.html":{"url":"kubernetes/kube-dns.html","title":"Kube-dns","keywords":"","body":"kube NDS Kubernetes DNS pod 中包括 3 个容器： kubedns：kubedns 进程监视 Kubernetes master 中的 Service 和 Endpoint 的变化，并维护内存查找结构来服务DNS请求。 dnsmasq：dnsmasq 容器添加 DNS 缓存以提高性能。 sidecar：sidecar 容器在执行双重健康检查（针对 dnsmasq 和 kubedns）时提供单个健康检查端点（监听在10054端口）。 DNS pod 具有静态 IP 并作为 Kubernetes 服务暴露出来。该静态 IP 分配后，kubelet 会将使用 --cluster-dns = 标志配置的 DNS 传递给每个容器。 DNS 名称也需要域名。本地域可以使用标志 --cluster-domain = 在 kubelet 中配置。 [root@node1 nginx]# kubectl get svc --all-namespaces -o wide NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR default kubernetes 10.233.0.1 443/TCP 5d kube-system kube-dns 10.233.0.3 53/UDP,53/TCP 13m k8s-app=kube-dns 不管那种部署很是，kubernetes 对外提供的 DNS 服务是一致的。每个 service 都会有对应的 DNS 记录，kubernetes 保存 DNS 记录的格式如下： ..svc. 在 pod 中可以通过 service_name.namespace.svc.domain 来访问任何的服务，也可以使用缩写 service_name.namespace，如果 pod 和 service 在同一个 namespace，甚至可以直接使用 service_name。 NOTE：正常的 service 域名会被解析成 service vip，而 headless service 域名会被直接解析成背后的 pods ip。 虽然不会经常用到，但是 pod 也会有对应的 DNS 记录，格式是 pod-ip-address..pod.，其中 pod-ip-address 为 pod ip 地址的用 - 符号隔开的格式，比如 pod ip 地址是 1.2.3.4 ，那么对应的域名就是 1-2-3-4.default.pod.cluster.local。 有一个名称为 \"my-service\" 的 Service，它在 Kubernetes 集群中名为 \"my-ns\" 的 Namespace中，为 \"my-service.my-ns\" 创建了一条 DNS 记录。 在名称为 \"my-ns\" 的 Namespace 中的 Pod 应该能够简单地通过名称查询找到 \"my-service\"。 在另一个 Namespace 中的 Pod 必须限定名称为 \"my-service.my-ns\"。 这些名称查询的结果是 Cluster IP。 [root@node1 nginx]# kubectl get svc --all-namespaces -o wide NAMESPACE NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR default kubernetes 10.233.0.1 443/TCP 5d kube-system kube-dns 10.233.0.3 53/UDP,53/TCP 13m k8s-app=kube-dns [root@node1 nginx]# kubectl exec -it busybox cat /etc/resolv.conf nameserver 10.233.0.3 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5 [root@node1 nginx]# [root@node1 nginx]# kubectl exec -it busybox -- nslookup kubernetes.default.svc.cluster.local Server: 10.233.0.3 Address 1: 10.233.0.3 kube-dns.kube-system.svc.cluster.local Name: kubernetes.default.svc.cluster.local Address 1: 10.233.0.1 kubernetes.default.svc.cluster.local [root@node1 nginx]# kubectl exec -it busybox -- nslookup kubernetes Server: 10.233.0.3 Address 1: 10.233.0.3 kube-dns.kube-system.svc.cluster.local Name: kubernetes Address 1: 10.233.0.1 kubernetes.default.svc.cluster.local [root@node1 nginx]# kubectl exec -it busybox -- nslookup kube-dns.kube-system Server: 10.233.0.3 Address 1: 10.233.0.3 kube-dns.kube-system.svc.cluster.local Name: kube-dns.kube-system Address 1: 10.233.0.3 kube-dns.kube-system.svc.cluster.local apiVersion: v1 kind: Service metadata: name: default-subdomain spec: selector: name: busybox clusterIP: None ports: - name: foo # Actually, no port is needed. port: 1234 targetPort: 1234 --- apiVersion: v1 kind: Pod metadata: name: busybox1 labels: name: busybox spec: hostname: busybox-1 subdomain: default-subdomain containers: - image: busybox command: - sleep - \"3600\" name: busybox --- apiVersion: v1 kind: Pod metadata: name: busybox2 labels: name: busybox spec: hostname: busybox-2 subdomain: default-subdomain containers: - image: busybox command: - sleep - \"3600\" name: busybox [root@node1 ~]# kubectl exec -it busybox1 -- nslookup default-subdomain Server: 10.233.0.3 Address 1: 10.233.0.3 kube-dns.kube-system.svc.cluster.local Name: default-subdomain Address 1: 10.233.71.15 busybox-1.default-subdomain.default.svc.cluster.local Address 2: 10.233.71.16 busybox-2.default-subdomain.default.svc.cluster.local [root@node1 ~]# kubectl exec -it busybox1 ping busybox-2.default-subdomain.default.svc.cluster.local PING busybox-2.default-subdomain.default.svc.cluster.local (10.233.71.16): 56 data bytes 64 bytes from 10.233.71.16: seq=0 ttl=63 time=0.056 ms 64 bytes from 10.233.71.16: seq=1 ttl=63 time=0.052 ms 64 bytes from 10.233.71.16: seq=2 ttl=63 time=0.054 ms ^C --- busybox-2.default-subdomain.default.svc.cluster.local ping statistics --- 3 packets transmitted, 3 packets received, 0% packet loss round-trip min/avg/max = 0.052/0.054/0.056 ms 参考 https://jimmysong.io/kubernetes-handbook/practice/configuring-dns.html http://cizixs.com/2017/04/11/kubernetes-intro-kube-dns http://www.cnblogs.com/iiiiher/p/7891713.html https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/#dns-policy powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/pod.html":{"url":"kubernetes/pod.html","title":"Pod","keywords":"","body":"Overview https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/ K8s-OSContainer-ProcessPod-Process Group Pod是一个逻辑概念，是一组共享某些资源的容器。Pod里的所有容器，共享同一个Network Namespace。 凡是调度，网络，存储，安全相关的属性都是Pod级别的。 Networking Each Pod is assigned a unique IP address. Every container in a Pod shares the network namespace, including the IP address and network ports. Containers inside a Pod can communicate with one another using localhost. When containers in a Pod communicate with entities outside the Pod, they must coordinate how they use the shared network resources (such as ports). Storage A Pod can specify a set of shared storage volumes. All containers in the Pod can access the shared volumes, allowing those containers to share data. Volumes also allow persistent data in a Pod to survive in case one of the containers within needs to be restarted. infra container k8s.gcr.io/pause Spec apiVersion: v1 kind: Pod metadata: name: two-containers spec: nodeSelector: disktype:ssd hostAliases: - ip: \"1.1.1.1\" hostname: - \"foo.remote\" - \"bar.remote\" shareProcessNamespace:true restartPolicy: Never volumes: - name: shared-data hostPath: path: /data containers: - name: nginx-container image: nginx lifecycle: postStart: exec: command: [\"/bin/sh\",\"-c\",\"echo hello > /usr/share/message\"] preStop: exec: command: [\"/usr/sbin/nginx\",\"-s\",\"quit\"] volumeMounts: - name: shared-data mountPath: /usr/share/nginx/html - name: debian-container image: debian volumeMounts: - name: shared-data mountPath: /pod-data command: [\"/bin/sh\"] args: [\"-c\", \"echo Hello from the debian container > /pod-data/index.html\"] nodeSelcetor： Pod和Node绑定的字段 HostAliases：定义了/etc/hosts的你内容 shareProcessNamespace：共享PID namespaces lifecycle: Container lifecycle hooks. postStart: Container启动后立即执行的操作。不严格保证在ENTRYPOINT后。 preStop：容器被kill前执行的操作。同步的，会阻塞kill流程，完成后才能被kill。 https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/ status Pending 已创建保存到ETCD，但没有被顺利创建。 Running 已调度成功，至少一个容器在运行中。 Ready 已running并且可以对外提供服务了。 initContainer Pod can have one or more Init Container, which are run before the app Containers are started. They always run to completion. Each one must complete successfully before the next one is started. Example apiVersion: v1 kind: Pod metadata: name: javaweb-2 spec: initContainers: - image: geektime/sample:v2 name: war command: [\"cp\", \"/sample.war\", \"/app\"] volumeMounts: - mountPath: /app name: app-volume containers: - image: geektime/tomcat:7.0 name: tomcat command: [\"sh\",\"-c\",\"/root/apache-tomcat-7.0.42-v2/bin/start.sh\"] volumeMounts: - mountPath: /root/apache-tomcat-7.0.42-v2/webapps name: app-volume ports: - containerPort: 8080 hostPort: 8001 volumes: - name: app-volume emptyDir: {} Design Pattern for Container-based Distribute Systems https://www.usenix.org/conference/hotcloud16/workshop-program/presentation/burns sidecar pattern like using Init Container Projected Volume https://kubernetes.io/docs/concepts/storage/volumes/#projectedhttps://kubernetes.io/docs/tasks/configure-pod-container/configure-projected-volume-storage/ Secret ConfigMap Downward API ServiceAccountToken pods/storage/projected.yaml apiVersion: v1 kind: Pod metadata: name: test-projected-volume spec: containers: - name: test-projected-volume image: busybox args: - sleep - \"86400\" volumeMounts: - name: all-in-one mountPath: \"/projected-volume\" readOnly: true volumes: - name: all-in-one projected: sources: - secret: name: user - secret: name: pass Liveness and Readiness Probes https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/ The kubelet uses liveness probes to know when to restart a Container. The kubelet uses readiness probes to know when a Container is ready to start accepting traffic. liveness example: pods/probe/exec-liveness.yaml apiVersion: v1 kind: Pod metadata: labels: test: liveness name: liveness-exec spec: containers: - name: liveness image: k8s.gcr.io/busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 liveness can be definded by command,HTTP request,TCP, named prot. Readiness probes are configured similarly to liveness probes. The only difference is that you use the readinessProbe field instead of the livenessProbe field. readinessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 restartPolicy https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states Pod的restartPolicy.Kubernetes没有stop，有restart，但实际上是重新创建。 Pod恢复过程拥有都发生在当前节点上，不会跑到其他节点上去。 一旦Pod和一个节点绑定，除非这个绑定关系发生变化，否则它永远都不会离开这个节点。当这个节点宕机了，这个Pod也不会主动迁移的。 Pod的restartPolicy指定的策略允许重启一场的容器（比如，always），那么这个Pod就会保持running状态，并进行容器重启，否正就会进入Failed状态。 包含多个容器的Pod，只要它里面所有的容器都进入异常状态后，Pod才会进入Failed状态。在此之前，Pod都是running状态。此时Ready会显示正常容器的个数。 PodPreset https://kubernetes.io/docs/concepts/workloads/pods/podpreset/https://kubernetes.io/docs/tasks/inject-data-application/podpreset/ enable PodPreset modify /etc/kubernetes/manifests/kube-apiserver.yaml - command: ... - --enable-admission-plugins=NodeRestriction,PodPreset - --runtime-config=settings.k8s.io/v1alpha1=true powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/deployment.html":{"url":"kubernetes/deployment.html","title":"Deployment","keywords":"","body":"Controller Pattern Controller都放在 pkg/controller 目录下，都遵循Kubernetes项目中的一个通用编排模式：控制循环（control loop） for { 实际状态 := 获取集群中对象 X 的实际状态（Actual State） 期望状态 := 获取集群中对象 X 的期望状态（Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } } 实际状态一般来自于Kubernetes集群本身。期望状态一般来自于用户提交的YAML文件。 Deployment 控制器的实现步骤： Deployment 控制器从Etcd里面获取所有携带“app:nginx”标签的Pod，统计他们的数量，这是实际状态； Deployment 对象的replicas字段是期望状态； Deployment 控制器讲两个状态进行对比，根据对比结果，确定删除还是创建Pod。 这种对比操作叫调谐（Reconcile），这哥调谐的过程，称作调谐循环（Reconcile Loop）或者同步循环（Sync Loop）。 Deployment https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ Deployment用在Stateless Application场景。 Deployment是个两层控制器。 上半部分是控制器定义，包括期望状态，下半部分是被控制对象的模板叫PodTemplete组成。 Deployment 控制器实际操作的是replicaSet对象，而不是pod对象。 在所有API对象的 Metadata 里面，都有一个字段叫作 ownerReference，保持当前这个API对象的拥有者Owner的信息。 deployment创建了replicaSet，replicateSet创建了pod： [root@node11 ~]# kubectl get pods -o yaml nginx-deployment-5896fbb489-4966s apiVersion: v1 kind: Pod metadata: creationTimestamp: \"2018-12-12T07:13:22Z\" generateName: nginx-deployment-5896fbb489- labels: app: nginx pod-template-hash: 5896fbb489 name: nginx-deployment-5896fbb489-4966s namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: nginx-deployment-5896fbb489 uid: 6913b1a4-fddd-11e8-a06c-080027c2b927 resourceVersion: \"253112\" selfLink: /api/v1/namespaces/default/pods/nginx-deployment-5896fbb489-4966s uid: 6916c238-fddd-11e8-a06c-080027c2b927 [root@node11 ~]# kubectl get replicaset NAME DESIRED CURRENT READY AGE nginx-deployment-5896fbb489 2 2 2 43h [root@node11 ~]# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 2/2 2 2 44h [root@node11 ~]# kubectl get replicaset nginx-deployment-5896fbb489 -o yaml apiVersion: extensions/v1beta1 kind: ReplicaSet metadata: annotations: deployment.kubernetes.io/desired-replicas: \"2\" deployment.kubernetes.io/max-replicas: \"3\" deployment.kubernetes.io/revision: \"2\" creationTimestamp: \"2018-12-12T07:13:22Z\" generation: 2 labels: app: nginx pod-template-hash: 5896fbb489 name: nginx-deployment-5896fbb489 namespace: default ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: Deployment name: nginx-deployment uid: 0ab68b8c-fddd-11e8-a06c-080027c2b927 resourceVersion: \"177670\" selfLink: /apis/extensions/v1beta1/namespaces/default/replicasets/nginx-deployment-5896fbb489 uid: 6913b1a4-fddd-11e8-a06c-080027c2b927 ReplicaSet会把这个随机字符串加到它控制的所有Pod的标签里后面带的字符串叫pod-template-hash，ReplicaSet会把这个随机字符串家到它控制的所有Pod的标签里，包子这些Pod不会跟集群里其他Pod混淆。 ReplicaSet会把pod-template-hash加到自己的Lable里面。 [root@node11 ~]# kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-5896fbb489 4 4 4 44h [root@node11 ~]# kubectl describe rs nginx-deployment-5896fbb489 Name: nginx-deployment-5896fbb489 Namespace: default Selector: app=nginx,pod-template-hash=5896fbb489 Labels: app=nginx pod-template-hash=5896fbb489 Annotations: deployment.kubernetes.io/desired-replicas: 4 deployment.kubernetes.io/max-replicas: 5 deployment.kubernetes.io/revision: 2 Controlled By: Deployment/nginx-deployment [root@node11 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deployment-5896fbb489-4966s 1/1 Running 1 44h nginx-deployment-5896fbb489-5ckfb 1/1 Running 0 7m39s Operation scale [root@node11 ~]# kubectl scale deployment nginx-deployment --replicas=4 deployment.extensions/nginx-deployment scaled edit [root@node11 ~]# kubectl edit deployment/nginx-deployment deployment.extensions/nginx-deployment edited [root@node11 ~]# kubectl get deployments ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 82s deployment-controller Scaled up replica set nginx-deployment-6fc57fccb6 to 1 Normal ScalingReplicaSet 82s deployment-controller Scaled down replica set nginx-deployment-5896fbb489 to 3 Normal ScalingReplicaSet 82s deployment-controller Scaled up replica set nginx-deployment-6fc57fccb6 to 2 Normal ScalingReplicaSet 80s deployment-controller Scaled down replica set nginx-deployment-5896fbb489 to 2 Normal ScalingReplicaSet 80s deployment-controller Scaled up replica set nginx-deployment-6fc57fccb6 to 3 Normal ScalingReplicaSet 80s deployment-controller Scaled down replica set nginx-deployment-5896fbb489 to 1 Normal ScalingReplicaSet 80s deployment-controller Scaled up replica set nginx-deployment-6fc57fccb6 to 4 Normal ScalingReplicaSet 78s deployment-controller Scaled down replica set nginx-deployment-5896fbb489 to 0 RollingUpdateStrategy default [root@node11 ~]# kubectl describe deployment nginx-deployment ... StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge configure apiVersion: apps/v1 kind: Deployment metadata: name: nginx-deployment labels: app: nginx spec: ... strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 1 Deployment控制的是ReplicaSet的数目和属性。一个版本对应的是一个ReplicaSet，这个版本应用的Pod数量，又RecplicaSet来保证。 通过kubectl set image更新image到1.91版本，这个版本不存在，新的ReplicaSet(hash=79dccf98ff)出错停止了，它创建了2个Pod，都没有READY，旧ReplicaSet(hash=76bf4969df)水平收缩，也自动停止了，一个旧Pod被删。 [root@node11 ~]# kubectl get deployments.apps nginx-deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 4/4 4 4 7s [root@node11 ~]# [root@node11 ~]# kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-76bf4969df 4 4 4 19s [root@node11 ~]# [root@node11 ~]# kubectl set image deployment nginx-deployment nginx=nginx:1.91 --record=true deployment.extensions/nginx-deployment image updated [root@node11 ~]# kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-76bf4969df 3 3 3 106s nginx-deployment-79dccf98ff 2 2 0 5s [root@node11 ~]# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 3/4 2 3 113s [root@node11 ~]# kubectl rollout status deployment nginx-deployment Waiting for deployment \"nginx-deployment\" rollout to finish: 2 out of 4 new replicas have been updated... 通过kubectl rollout undo 可以回滚到上一个版本 [root@node11 ~]# kubectl rollout undo deployment/nginx-deployment deployment.extensions/nginx-deployment rolled back [root@node11 ~]# kubectl get deployment NAME READY UP-TO-DATE AVAILABLE AGE nginx-deployment 4/4 4 4 8m51s [root@node11 ~]# kubectl get rs NAME DESIRED CURRENT READY AGE nginx-deployment-76bf4969df 4 4 4 8m54s nginx-deployment-79dccf98ff 0 0 0 7m13s 通过kubectl rollout history 可以查看Deployment变更的版本，指定了--record都会被记录下来。可以查看具体版本，和回滚到具体版本。 [root@node11 ~]# kubectl rollout history deployment nginx-deployment deployment.extensions/nginx-deployment REVISION CHANGE-CAUSE 2 kubectl set image deployment nginx-deployment nginx=nginx:1.91 --record=true 3 [root@node11 ~]# kubectl rollout history deployment nginx-deployment --revision=2 deployment.extensions/nginx-deployment with revision #2 Pod Template: Labels: app=nginx pod-template-hash=79dccf98ff Annotations: kubernetes.io/change-cause: kubectl set image deployment nginx-deployment nginx=nginx:1.91 --record=true Containers: nginx: Image: nginx:1.91 Port: 80/TCP Host Port: 0/TCP Environment: Mounts: Volumes: [root@node11 ~]# kubectl rollout undo deployment nginx-deployment --to-revision=2 每次对Deployment进行更新操作，都会生成一个新的ReplicaSet对象。可以通过pause Deployment后进行更新操作，再resume，这样就只生成一个新的ReplicaSet。 $ kubectl rollout pause deployment/nginx-deployment deployment.extensions/nginx-deployment paused $ kubectl edit xxx /set image ... $ kubectl rollout resume deploy/nginx-deployment deployment.extensions/nginx-deployment resumed Blue-Green / Canary Deployment https://github.com/ContainerSolutions/k8s-deployment-strategies/tree/master/canary NOTE 不同namespace相同的对象，是完全不同的对象。 Deployment 只允许容器的restartPolicy=Always 将Deployment的spec.revisionHistoryLimit设置为0，就不能再回滚了。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/daemonset.html":{"url":"kubernetes/daemonset.html","title":"DaemonSet","keywords":"","body":"DaemonSet DaemonSet的主要作用，让你在Kubernetes集群里，运行一个Daemon Pod。 这个Pod运行在每一个Node上； 每个Node上只又一个这样的Pod实例； 当有新的节点加入后，这个Pod会在新的节点上被创建出来，当旧节点被删除后，上面的Pod也会被回收。 apiVersion: apps/v1 kind: DaemonSet metadata: name: fluentd-elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 resources: limits: memory: 200Mi requests: cpu: 100m memory: 200Mi volumeMounts: - name: varlog mountPath: /var/log - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true terminationGracePeriodSeconds: 30 volumes: - name: varlog hostPath: path: /var/log - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers DaemonSet是依靠toleration实现的。 nodeAffinity tolerations 在指定的Node上创建新的Pod。 apiVersion: v1 kind: Pod metadata: name: with-node-affinity spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: metadata.name operator: In values: - node-geektime DaemonSet Controller会在创建Pod的时候，自动在这个Pod的API对象里，加上nodeAffinity定义。 需要绑定的节点名称，就是当前正在遍历的这个Node。 DaemonSet 还会给这个Pod自动加上一个与调度相关的字段tolerations。这个字段会 “容忍” （toleration）某些 “污点”（Taint）。 apiVersion: v1 kind: Pod metadata: name: with-toleration spec: tolerations: - key: node.kubernetes.io/unschedulable operator: Exists effect: NoSchedule 正常情况下，被标记了 unscheduleable “污点”的Node，是不会有任何Pod被调度上去的（effect: NoSchedule）。 DeamonSet不需要修改用户的YAML文件里的Pod模板，而是在向Kubernetes发起请求之前，直接修改根据模板生成的Pod对象。 tolerations“容忍” 所有被标记为 unschedulable “污点”的Node，允许被调度。 DeamonSet自动地给被管理的Pod加上了这个特殊的toleration，就使得这些Pod可以忽略这个限制，继而保证每个节点上都会被调度一个Pod。如果这个节点又给故障的话，这个Pod可能会启动失败，而DaemonSet则始终尝试下去，直到Pod启动成功。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/statefulset.html":{"url":"kubernetes/statefulset.html","title":"StatefulSet","keywords":"","body":"StatefulSet StatefulSet is the workload API object used to manage stateful applications. https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ 应用状态分类： 拓扑状态 多个实例启动有先后顺序，比如先A再B，如果A，B都删掉了，再次创建出来的时候也必须遵守这个顺序。新创建的Pod必须和原来的网络标识意义，这样原先访问者才能使用同样的方法，访问到这个新的Pod。 存储状态 多个实例分别绑定不同的存储数据。重新创建后还需继续可访问之前的存储数据。 Demo1 apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web --- apiVersion: v1 kind: Service metadata: name: nginx labels: app: nginx spec: ports: - port: 80 name: web clusterIP: None selector: app: nginx [root@node11 ~]# kubectl apply -f statefulset.yaml statefulset.apps/web created service/nginx created [root@node11 ~]# kubectl get service nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP None 80/TCP 15s [root@node11 ~]# kubectl get statefulsets.apps NAME READY AGE web 2/2 31s [root@node11 ~]# kubectl get pods -w -l app=nginx NAME READY STATUS RESTARTS AGE web-0 1/1 Running 0 61s web-1 1/1 Running 0 60s [root@node11 ~]# kubectl describe statefulsets web ... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal SuccessfulCreate 98s statefulset-controller create Pod web-0 in StatefulSet web successful Normal SuccessfulCreate 97s statefulset-controller create Pod web-1 in StatefulSet web successful StatefulSet 先后创建Pod，分别命名web-0，web-1。 查询下每个pod的hostname，于Pod名称相同： [root@node11 ~]# kubectl exec web-0 -- sh -c 'hostname' web-0 [root@node11 ~]# kubectl exec web-1 -- sh -c 'hostname' web-1 使用nslookup 查询 DNS [root@node11 ~]# kubectl run -i --tty --image busybox:1.28.4 dns-test --restart=Never --rm /bin/sh If you don't see a command prompt, try pressing enter. / # nslookup web-0.nginx.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-0.nginx.default.svc.cluster.local Address 1: 192.168.72.22 web-0.nginx.default.svc.cluster.local / # nslookup web-1.nginx.default.svc.cluster.local Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: web-1.nginx.default.svc.cluster.local Address 1: 192.168.226.81 web-1.nginx.default.svc.cluster.local / # ping web-0.nginx.default.svc.cluster.local PING web-0.nginx.default.svc.cluster.local (192.168.72.22): 56 data bytes 64 bytes from 192.168.72.22: seq=0 ttl=63 time=0.056 ms 64 bytes from 192.168.72.22: seq=1 ttl=63 time=0.126 ms 有状态应用必须通过DNS或者hostname的方式来访问，IP会变化的。 当删除掉这两个有状态应用的Pod，Kubernetes会按照原先的顺序，创建出两个新的Pod，并且分配了于原来相同的“网络身份”。通过这种严格的对应规则，StatefulSet就保证了Pod网络标识的稳定性。 Demo2 apiVersion: apps/v1 kind: StatefulSet metadata: name: web spec: serviceName: \"nginx\" replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.9.1 ports: - containerPort: 80 name: web volumeMounts: - name: www mountPath: /usr/share/nginx/html volumeClaimTemplates: - metadata: name: www spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi 当我们创建StatefulSet后，Kubernetes集群里面会多2个PVC，这些PVC以 “--” 的方式命名。 当Pod被删除后，对应的PVC和PV并不会被删除，Kubernetes会按照顺序重新恢复Pod，并找到之前的的PVC，进而和这个PVC绑定的PV。 通过这种方式实现应用存储状态的管理。 Demo3 https://github.com/oracle/kubernetes-website/blob/master/docs/tasks/run-application/mysql-statefulset.yaml update $ kubectl patch statefulset mysql --type='json' -p='[{\"op\": \"replace\", \"path\": \"/spec/template/spec/containers/0/image\", \"value\":\"mysql:5.7.23\"}]' statefulset.apps/mysql patched StatefulSet Controller会按照Pod编号相反的顺序，从最后一个Pod开始，逐一更新每个Pod。sepc.updateStrategy.rollingUpdate的partition字段，可以更精细的控制“滚动更新”，比如Canary Deploy。 $ kubectl patch statefulset mysql -p '{\"spec\":{\"updateStrategy\":{\"type\":\"RollingUpdate\",\"rollingUpdate\":{\"partition\":2}}}}' statefulset.apps/mysql patched 使用path或者edit修改partition字段，只有序号大于或者等于2的Pod会被更新到这个版本，并且，删除或者重启序号小于2的Pod，等它再次启动，依然会保持之前的版本，不会被升级到新版本。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/job-cronJob.html":{"url":"kubernetes/job-cronJob.html","title":"Job CronJob","keywords":"","body":"Job Batch Job: 计算业务/离线业务，跑完就退出； Long Running Job：长作业。 apiVersion: batch/v1 kind: Job metadata: name: pi spec: template: spec: containers: - name: pi image: resouer/ubuntu-bc command: [\"sh\", \"-c\", \"echo 'scale=10000; 4*a(1)' | bc -l \"] restartPolicy: Never backoffLimit: 4 restartPolicy 在Job里面只允许被设置为Never和OnFailure；而在Deployment对象里，只需运行被设置为Always。 restartPolicy为Never，当Job失败会不断尝试创建一个新的Pod。 restartPolicy为OnFailure，当Job失败后，会尝试重启Pod，不会重新创建。 spec.backoffLimit字段可以限制创建的次数，Job重新创建的间隔是指数增长的。 spec.activeDeadlineSeconds是设置最长运行时间，超过这个时间就会被终止。 spec.parallelism，Job任意时间最多可以启动多少个Pod同时运行。 spec.completions，Job完成的Pod数目，就是最终用户需要的Pod数目。 Demo Demo 1 外部管理器+Job模板 最通用的方式： apiVersion: batch/v1 kind: Job metadata: name: process-item-$ITEM labels: jobgroup: jobexample spec: template: metadata: name: jobexample labels: jobgroup: jobexample spec: containers: - name: c image: busybox command: [\"sh\", \"-c\", \"echo Processing item $ITEM && sleep 5\"] restartPolicy: Never 外部脚本或者程序控制$ITEM变量，这些Job都又同一个标签。 $ mkdir ./jobs $ for i in apple banana cherry do cat job-tmpl.yaml | sed \"s/\\$ITEM/$i/\" > ./jobs/job-$i.yaml done demo 2 拥有固定任务数目的并行Job apiVersion: batch/v1 kind: Job metadata: name: job-wq-1 spec: completions: 8 parallelism: 2 template: metadata: name: job-wq-1 spec: containers: - name: c image: myrepo/job-wq-1 env: - name: BROKER_URL value: amqp://guest:guest@rabbitmq-service:5672 - name: QUEUE value: job1 restartPolicy: OnFailure 这种方式，只关心指定任务数completions，不关心parallelism数。 任务可以从工作队列里面获取，生成者-消费者模式。 /* job-wq-1 的伪代码 */ queue := newQueue($BROKER_URL, $QUEUE) task := queue.Pop() process(task) exit CronJob CronJob 定时任务，是一个Job对象控制器。 apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: \"*/1 * * * *\" jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure */1 * * * * 从0开始，每1个单位时间执行一次，分，时，日，月，星期 concurrencyPolicy=Allow，默认情况，Job可以同时存在； concurrencyPolicy=Forbid，不会创建新Pod，该创建周期被跳过； concurrencyPolicy=Replace，新产生的Job会替换旧的、没执行完的Job。 spec.startingDeadlineSeconds，多少秒里，miss的数目达到100次，这个Job就不会再被创建。 （Job创建失败，就会被记miss） powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/authentication-authorization-admission.html":{"url":"kubernetes/authentication-authorization-admission.html","title":"Authentication Authorization Admission","keywords":"","body":"认证Authenticating & 授权Authorization & 准入Admission Control 账户分为两类： User Account：普通用户被假定为由外部独立服务管理。管理员分发私钥，用户存储（如Keystone或Google帐户），甚至包含用户名和密码列表的文件。在这方面，kubernetes没有代表普通用户帐户的对象。无法通过API调用的方式向集群中添加普通用户。用户账号为全局设计的。命名必须在一个集群的所有命名空间中唯一。这个账户是给人用的。 Service Account：service account是由kubernetes API管理的帐户。它们都绑定到了特定的 namespace，并由api-server自动创建，或者通过API调用手动创建。service account关联了一套凭证，存储在Secret，这些凭证同时被挂载到pod中，从而允许pod与kubernetes API之间的调用。服务账号是在命名空间里的。这个账户是给pod中的进程用的。 概述 k8s通过kube-apiserver组件对外提供REST服务，有两类客户端：普通用户和集群内的pod; k8s默认https安全端口6443，一个API请求到达该端口后，要经过认证，授权，准入控制，实际API请求; K8s默认非安全端口8080（只能本机访问，绑定的是localhost）; 认证：对客户端的认证，Authenticaton verifies who you are; 授权：对不同用户不同的访问权限，Authorization verifies what you are authorized to do; 准入：Admission Control 有一个准入控制列表，我们可以通过命令行设置选择执行哪几个准入控制器。只有所有的准入控制器都检查通过之后，apiserver 才执行该请求，否则返回拒绝。 认证 apiserver认证配置: [root@node1 manifests]# cat /etc/kubernetes/manifests/kube-apiserver.manifest apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system labels: k8s-app: kube-apiserver kubespray: v2 annotations: kubespray.etcd-cert/serial: \"{'stderr_lines': [], u'changed': True, u'end': u'2018-06-06 22:41:46.202119', 'failed': False, u'stdout': u'B2EDC360B1BF0E77', u'cmd': u'openssl x509 -in /etc/ssl/etcd/ssl/node-node1.pem -noout -serial | cut -d= -f2', u'rc': 0, u'start': u'2018-06-06 22:41:46.185670', u'stderr': u'', u'delta': u'0:00:00.016449', 'stdout_lines': [u'B2EDC360B1BF0E77']}\" kubespray.apiserver-cert/serial: \"D72C56208C860176\" spec: hostNetwork: true dnsPolicy: ClusterFirst containers: - name: kube-apiserver image: yinzw/hyperkube:v1.7.5_coreos.0 imagePullPolicy: IfNotPresent resources: limits: cpu: 800m memory: 2000M requests: cpu: 100m memory: 256M command: - /hyperkube - apiserver - --advertise-address=192.168.137.101 - --etcd-servers=https://192.168.137.101:2379,https://192.168.137.102:2379,https://192.168.137.103:2379 - --etcd-quorum-read=true - --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem - --etcd-certfile=/etc/ssl/etcd/ssl/node-node1.pem - --etcd-keyfile=/etc/ssl/etcd/ssl/node-node1-key.pem - --insecure-bind-address=127.0.0.1 - --apiserver-count=2 - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota - --service-cluster-ip-range=10.233.0.0/18 - --service-node-port-range=30000-32767 - --client-ca-file=/etc/kubernetes/ssl/ca.pem - --basic-auth-file=/etc/kubernetes/users/known_users.csv - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem - --token-auth-file=/etc/kubernetes/tokens/known_tokens.csv - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem - --secure-port=6443 - --insecure-port=8080 - --storage-backend=etcd3 - --v=2 - --allow-privileged=true - --anonymous-auth=False --client-ca-file: 指定CA根证书文件为 /etc/kubernetes/ssl/ca.pem ，内置CA公钥用于验证某证书是否是CA签发的证书; --tls-private-key-file: 指定ApiServer私钥文件为 /etc/kubernetes/ssl/apiserver-key.pem; --tls-cert-file: 指定ApiServer证书文件为 /etc/kubernetes/ssl/apiserver.pem; --token-auth-file 静态token认证; --basic-auth-file 用户密码认证; user account认证有三种：CA证书，token，Base认证，可以配多种 sa 认证 开启了https认证，访问集群，提示unauthorized CA认证 [root@node3 kubernetes]# curl https://node1:6443/api/v1/nodes Unauthorized [root@node3 kubernetes]# curl https://node1:6443/api/v1/nodes --cert /etc/kubernetes/ssl/node-node3.pem --key /etc/kubernetes/ssl/node-node3-key.pem { \"kind\": \"NodeList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/nodes\", \"resourceVersion\": \"434825\" }, \"items\": [ { \"metadata\": { \"name\": \"node1\", \"selfLink\": \"/api/v1/nodes/node1\", ... [root@node3 ssl]# curl https://node1:6443/api/v1/nodes --cert /etc/kubernetes/ssl/kube-proxy-node3.pem --key /etc/kubernetes/ssl/kube-proxy-node3-key.pem { \"kind\": \"NodeList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/nodes\", \"resourceVersion\": \"435193\" }, \"items\": [ { \"metadata\": { \"name\": \"node1\", \"selfLink\": \"/api/v1/nodes/node1\", \"uid\": \"de4e2334-699b-11e8-ba8f-0800277b75c4\", \"resourceVersion\": \"435188\", \"creationTimestamp\": \"2018-06-06T15:11:20Z\", ... [root@node2 kubernetes]# curl https://node1:6443/api/v1/nodes --cert /etc/kubernetes/ssl/kube-scheduler.pem --key /etc/kubernetes/ssl/kube-scheduler-key.pem { \"kind\": \"NodeList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/nodes\", \"resourceVersion\": \"435809\" }, \"items\": [ { \"metadata\": { \"name\": \"node1\", \"selfLink\": \"/api/v1/nodes/node1\", kubelet 的配置 [root@node3 kubernetes]# cat node-kubeconfig.yaml apiVersion: v1 kind: Config clusters: - name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pem server: https://localhost:6443 users: - name: kubelet user: client-certificate: /etc/kubernetes/ssl/node-node3.pem client-key: /etc/kubernetes/ssl/node-node3-key.pem contexts: - context: cluster: local user: kubelet name: kubelet-cluster.local current-context: kubelet-cluster.local kubeproxy 的配置 [root@node3 kubernetes]# cat kube-proxy-kubeconfig.yaml apiVersion: v1 kind: Config clusters: - name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pem server: https://localhost:6443 users: - name: kube-proxy user: client-certificate: /etc/kubernetes/ssl/kube-proxy-node3.pem client-key: /etc/kubernetes/ssl/kube-proxy-node3-key.pem contexts: - context: cluster: local user: kube-proxy name: kube-proxy-cluster.local current-context: kube-proxy-cluster.local kube-scheduler 的配置 [root@node2 kubernetes]# cat kube-scheduler-kubeconfig.yaml apiVersion: v1 kind: Config clusters: - name: local cluster: certificate-authority: /etc/kubernetes/ssl/ca.pem server: https://127.0.0.1:6443 users: - name: kube-scheduler user: client-certificate: /etc/kubernetes/ssl/kube-scheduler.pem client-key: /etc/kubernetes/ssl/kube-scheduler-key.pem contexts: - context: cluster: local user: kube-scheduler name: kube-scheduler-cluster.local current-context: kube-scheduler-cluster.local 手动生成证书和key，可以在集群其他节点上使用 [root@node1 ssl]# openssl genrsa -out test-key.pem 2048 Generating RSA private key, 2048 bit long modulus ......+++ ...+++ e is 65537 (0x10001) [root@node1 ssl]# openssl req -new -key test-key.pem -out test.csr -subj \"/CN=test\" [root@node1 ssl]# [root@node1 ssl]# openssl x509 -req -in test.csr -CA ca.pem -CAkey ca-key.pem -CAcreateserial -out test.pem -days 3650 -extensions v3_req Signature ok subject=/CN=test Getting CA Private Key [root@node1 ssl]# scp test* node3:/root/test root@node3's password: test.csr 100% 883 1.7MB/s 00:00 test-key.pem 100% 1679 3.4MB/s 00:00 test.pem [root@node3 test]# curl https://node1:6443/api/v1/nodes --cert ./test.pem --key ./test-key.pem { \"kind\": \"NodeList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/nodes\", \"resourceVersion\": \"439941\" }, 将证书拷贝到非集群节点上，报错 [root@node test]# curl https://node1:6443/api/v1/nodes --cert ./test.pem --key ./test-key.pem curl: (60) Peer's certificate has an invalid signature. More details here: http://curl.haxx.se/docs/sslcerts.html curl performs SSL certificate verification by default, using a \"bundle\" of Certificate Authority (CA) public keys (CA certs). If the default bundle file isn't adequate, you can specify an alternate file using the --cacert option. If this HTTPS server uses a certificate signed by a CA represented in the bundle, the certificate verification probably failed due to a problem with the certificate (it might be expired, or the name might not match the domain name in the URL). If you'd like to turn off curl's verification of the certificate, use the -k (or --insecure) option. 需要将ca根证书拷贝过来 [root@node test]# curl https://node1:6443/api/v1/nodes --cert ./test.pem --key ./test-key.pem --cacert ./ca.pem { \"kind\": \"NodeList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/nodes\", \"resourceVersion\": \"441981\" }, \"items\": [ { \"metadata\": { \"name\": \"node1\", \"selfLink\": \"/api/v1/nodes/node1\", 为什么集群节点不需要ca根证书？因为集群ca证书已经在ca-bundle.crt里面了，curl时不用带cacert。但是将ca证书加到ca-bundle里面了也不行，不知道为啥？ [root@node3 ssl]# cd /etc/pki/tls/certs/ [root@node3 certs]# ls ca-bundle.crt ca-bundle.trust.crt make-dummy-cert Makefile renew-dummy-cert [root@node3 certs]# cat ca-bundle.crt | grep MIIC9zCCAd+gAwIBAgIJAP+vWZXVLCBqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV MIIC9zCCAd+gAwIBAgIJAP+vWZXVLCBqMA0GCSqGSIb3DQEBCwUAMBIxEDAOBgNV token 认证 和静态密码一样，这种方法也是名存实亡，不推荐使用。 查看token 格式：token,user,uid,\"group1,group2,group3\" NOTE：如果该静态token文件更改的话，需要重启apiserver [root@node1 tokens]# ls known_tokens.csv system:kubectl-node1.token system:kubectl-node2.token system:kubelet-node1.token system:kubelet-node2.token system:kubelet-node3.token [root@node1 tokens]# cat known_tokens.csv UPVzqw3LVaqpPCrijfuc2rwKadjvGNUq,system:kubectl-node1,system:kubectl-node1 uPjAGKBVwFCN9Iyam7Y3150tgrGmXekm,system:kubectl-node2,system:kubectl-node2 fPv22M1DYnVDXpXKPPlzMaZDLgmGZCKp,system:kubelet-node1,system:kubelet-node1 GwpmzUtoIbODKavaNXoNqf4P8XcaUz0K,system:kubelet-node2,system:kubelet-node2 MjkEgEkG8L9vcmy1zEdF68OXRkriyONL,system:kubelet-node3,system:kubelet-node3 [root@node1 tokens]# cat system\\:kubectl-node1.token UPVzqw3LVaqpPCrijfuc2rwKadjvGNUq 访问1 [root@node3 test]# curl -k --header \"Authorization: Bearer UPVzqw3LVaqpPCrijfuc2rwKadjvGNUq\" https://node1:6443/api/v1/nodes { \"kind\": \"NodeList\", \"apiVersion\": \"v1\", \"metadata\": { \"selfLink\": \"/api/v1/nodes\", \"resourceVersion\": \"446733\" }, \"items\": [ { \"metadata\": { \"name\": \"node1\", 访问2 [root@node3 test]# ./kubectl --server=https://node1:6443 get nodes Please enter Username: admin Please enter Password: **** error: You must be logged in to the server (the server has asked for the client to provide credentials (get nodes)) [root@node3 test]# ./kubectl --server=https://node1:6443 --token=UPVzqw3LVaqpPCrijfuc2rwKadjvGNUq --insecure-skip-tls-verify=true get nodes NAME STATUS AGE VERSION node1 Ready 20d v1.7.5+coreos.0 node2 Ready 20d v1.7.5+coreos.0 node3 Ready 20d v1.7.5+coreos.0 password file认证 这种方式很不灵活，也不安全，可以说名存实亡，不推荐使用。 查看 格式: password,user,uid,\"group1,group2,group3\" [root@node1 users]# ls known_users.csv [root@node1 users]# cat known_users.csv ekxj0taObdTNTXc,kube,admin,\"system:masters\" 使用 [root@node3 test]# ./kubectl --server=https://node1:6443 get nodes Please enter Username: kube Please enter Password: *************** NAME STATUS AGE VERSION node1 Ready 20d v1.7.5+coreos.0 node2 Ready 20d v1.7.5+coreos.0 node3 Ready 20d v1.7.5+coreos.0 [root@node3 test]# 参考 https://kubernetes.feisky.xyz/zh/plugins/auth.html#%20%E8%AE%A4%E8%AF%81 SA service-account 授权 AlwaysDeny：表示拒绝所有的请求，该配置一般用于测试; AlwaysAllow：表示接收所有请求，如果集群不需要授权，则可以采取这个策略; ABAC：基于属性的访问控制，表示基于配置的授权规则去匹配用户请求，判断是否有权限； RBAC：基于角色的访问控制，允许管理员通过 api 动态配置授权策略。 参考 https://jimmysong.io/kubernetes-handbook/guide/kubectl-user-authentication-authorization.html RBAC RBAC 介绍 Role: 某个ns下的资源 ClusterRole: 集群范围内的资源(node, endpoint pods) RoleBinding ClusterRoleBinding: 将role中定义的权限授予users、groups、service accounts 参考 https://mritd.me/2018/03/20/use-rbac-to-control-kubectl-permissions/ https://blog.qikqiak.com/post/add-authorization-for-kubernetes-dashboard/ 配置 [root@node1 manifests]# cat kube-apiserver.manifest ... spec: command: - --authorization-mode=Node,RBAC 查询roles clusterroles [root@node1 manifests]# kubectl get roles --all-namespaces NAMESPACE NAME AGE kube-public system:controller:bootstrap-signer 22d kube-system extension-apiserver-authentication-reader 22d kube-system kubernetes-dashboard-minimal 22d kube-system system::leader-locking-kube-controller-manager 22d kube-system system::leader-locking-kube-scheduler 22d kube-system system:controller:bootstrap-signer 22d kube-system system:controller:cloud-provider 22d kube-system system:controller:token-cleaner 22d [root@node1 manifests]# kubectl get clusterroles NAME AGE admin 22d calico-node 22d cluster-admin 22d cluster-proportional-autoscaler 22d edit 22d kubernetes-dashboard-anonymous 22d system:aggregate-to-admin 22d system:aggregate-to-edit 22d system:aggregate-to-view 22d system:auth-delegator 22d ... 查询 rolebinding clusterrolebinding RoleBinding把Role绑定到账户主体Subject，让Subject继承Role所在namespace下的权限。 ClusterRoleBinding把ClusterRole绑定到Subject，让Subject集成ClusterRole在整个集群中的权限。 账户主体Subject在这里还是叫“用户”吧，包含组group，用户user和ServiceAccount。 [root@node1 manifests]# kubectl get rolebinding --all-namespaces NAMESPACE NAME AGE kube-public system:controller:bootstrap-signer 22d kube-system kubernetes-dashboard-minimal 22d kube-system system::leader-locking-kube-controller-manager 22d kube-system system::leader-locking-kube-scheduler 22d kube-system system:controller:bootstrap-signer 22d kube-system system:controller:cloud-provider 22d kube-system system:controller:token-cleaner 22d [root@node1 manifests]# kubectl get clusterrolebinding --all-namespaces NAME AGE calico-node 22d cluster-admin 22d cluster-proportional-autoscaler 22d kubernetes-dashboard-anonymous 22d kubespray:system:node 22d system:aws-cloud-provider 22d system:basic-user 22d system:controller:attachdetach-controller 22d ... 准入 当前可配置的准入控制器主要有： AlwaysAdmit：允许所有请求 AlwaysDeny：拒绝所有请求 AlwaysPullImages：在启动容器之前总是去下载镜像 ServiceAccount：将 secret 信息挂载到 pod 中，比如 service account token，registry key 等 ResourceQuota 和 LimitRanger：实现配额控制 SecurityContextDeny：禁止创建设置了 Security Context 的 pod 通过kubeconfig配置访问多集群 kubeconfig介绍 kubeconfig文件用于组织关于集群、用户、命名空间和认证机制的信息。命令行工具kubectl从 kubeconfig文件中得到它要选择的集群以及跟集群api-server交互的信息。 默认情况下，kubectl会从$HOME/.kube目录下查找文件名为config的文件。可以通过设置环境变量KUBECONFIG或者通过设置--kubeconfig去指定其它kubeconfig文件。 Context context指定了kubectl命令运行的上下文环境，kubectl与当前context中指定的集群和命名空间进行通信，并且使用当前context中包含的用户凭证。 每个context都是一个由（集群、命名空间、用户）描述的三元组。可以使用kubectl config use-context去设置当前的context。 [root@node1 ~]# kubectl config view apiVersion: v1 clusters: - cluster: certificate-authority-data: REDACTED server: https://192.168.137.101:6443 name: kubernetes contexts: - context: cluster: kubernetes user: kubernetes-admin name: kubernetes-admin@kubernetes current-context: kubernetes-admin@kubernetes kind: Config preferences: {} users: - name: kubernetes-admin user: client-certificate-data: REDACTED client-key-data: REDACTED [root@node1 ~]# kubectl config get-contexts CURRENT NAME CLUSTER AUTHINFO NAMESPACE * kubernetes-admin@kubernetes kubernetes kubernetes-admin 参考 https://kubernetes.io/docs/reference/access-authn-authz/authentication/ https://jimmysong.io/kubernetes-handbook/guide/authentication.html https://zhangchenchen.github.io/2017/08/17/kubernetes-authentication-authorization-admission-control/ http://www.cnblogs.com/breg/p/5923604.html https://www.kubernetes.org.cn/1995.html https://jimmysong.io/kubernetes-handbook/guide/managing-tls-in-a-cluster.html https://jimmysong.io/kubernetes-handbook/practice/create-tls-and-secret-key.html https://jimmysong.io/kubernetes-handbook/guide/auth-with-kubeconfig-or-token.html https://blog.frognew.com/2017/04/kubernetes-1.6-rbac.html powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/tls-ca.html":{"url":"kubernetes/tls-ca.html","title":"TLS CA","keywords":"","body":"ca-bundle.crt What is ca-bundle.crt? ca-bundle.crt is a file that contains well known root CAcertificates. [root@node3 certs]# curl -v https://www.baidu.com/ * About to connect() to www.baidu.com port 443 (#0) * Trying 61.135.169.121... * Connected to www.baidu.com (61.135.169.121) port 443 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * CAfile: /etc/pki/tls/certs/ca-bundle.crt CApath: none * SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 * Server certificate: * subject: CN=baidu.com,OU=service operation department.,O=\"BeiJing Baidu Netcom Science Technology Co., Ltd\",L=beijing,ST=beijing,C=CN * start date: Jun 29 00:00:00 2017 GMT * expire date: Aug 17 23:59:59 2018 GMT * common name: baidu.com * issuer: CN=Symantec Class 3 Secure Server CA - G4,OU=Symantec Trust Network,O=Symantec Corporation,C=US > GET / HTTP/1.1 > User-Agent: curl/7.29.0 > Host: www.baidu.com > Accept: */* > centos 更新ca-bundle. cp foo.crt /etc/pki/ca-trust/source/anchors/ update-ca-trust extract powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/secret.html":{"url":"kubernetes/secret.html","title":"Secret","keywords":"","body":"Secret Secrete有三种类型： Opaque (default): 任意字符串。 kubernetes.io/service-account-token: 给 service account 用的。 kubernetes.io/dockercfg: 给Docker registry 用的，用户下载 docker 镜像认证使用。 Opaque base64编码格式，用于存储密码，密钥等，可以通过base64 -decode解码，加密性弱。 命令创建secret [root@node1 ~]# echo -n 'my-test' > ./username.txt [root@node1 ~]# echo -n '123456' > ./password.txt [root@node1 ~]# kubectl create secret generic test-1 --from-file=./username.txt --from-file=./password.txt secret \"test-1\" created 手工创建secret [root@node1 ~]# echo -n 'my-test' | base64 bXktdGVzdA== [root@node1 ~]# echo -n '123456' | base64 MTIzNDU2 [root@node1 ~]# [root@node1 ~]# cat test-1.yaml apiVersion: v1 kind: Secret metadata: name: mysecret type: Opaque data: username: YWRtaW4= password: MWYyZDFlMmU2N2Rm [root@node1 ~]# [root@node1 ~]# kubectl apply -f test-1.yaml secret \"mysecret\" created 查询 [root@node1 ~]# kubectl get secret NAME TYPE DATA AGE default-token-xz2pl kubernetes.io/service-account-token 3 18d mysecret Opaque 2 2h sa-test-secret kubernetes.io/service-account-token 3 9d sa-test-token-xqb7q kubernetes.io/service-account-token 3 9d test-1 Opaque 2 2h [root@node1 ~]# [root@node1 ~]# kubectl describe secret test-1 Name: test-1 Namespace: default Labels: Annotations: Type: Opaque Data ==== password.txt: 6 bytes username.txt: 7 bytes [root@node1 ~]# [root@node1 ~]# kubectl get secret test-1 -o yaml apiVersion: v1 data: password.txt: MTIzNDU2 username.txt: bXktdGVzdA== kind: Secret metadata: creationTimestamp: 2018-06-25T02:52:12Z name: test-1 namespace: default resourceVersion: \"356787\" selfLink: /api/v1/namespaces/default/secrets/test-1 uid: c25fd9bf-7822-11e8-a0b4-0800277b75c4 type: Opaque [root@node1 ~]# kubectl get secret mysecret -o yaml apiVersion: v1 data: password: MWYyZDFlMmU2N2Rm username: YWRtaW4= kind: Secret metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: | {\"apiVersion\":\"v1\",\"data\":{\"password\":\"MWYyZDFlMmU2N2Rm\",\"username\":\"YWRtaW4=\"},\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"mysecret\",\"namespace\":\"default\"},\"type\":\"Opaque\"} creationTimestamp: 2018-06-25T03:06:21Z name: mysecret namespace: default resourceVersion: \"357857\" selfLink: /api/v1/namespaces/default/secrets/mysecret uid: bc8b2691-7824-11e8-a0b4-0800277b75c4 type: Opaque 使用secret 通过加到卷来访问secret [root@node1 ~]# cat busybox3.yaml apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: busybox image: busybox:latest command: - sleep - \"3600\" volumeMounts: - name: secret-volume mountPath: /etc/secret-volume readOnly: true volumes: - name: secret-volume secret: secretName: mysecret [root@node1 ~]# [root@node1 ~]# kubectl create -f busybox3.yaml pod \"busybox\" created [root@node1 ~]# [root@node1 ~]# kubectl exec busybox -it ls /etc/secret-volume password username [root@node1 ~]# 通过环境变量访问secret* [root@node1 ~]# cat busybox3.yaml apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: busybox image: busybox:latest command: - sleep - \"3600\" env: - name: SECRET_USERNAME valueFrom: secretKeyRef: name: mysecret key: username - name: SECRET_PASSWORD valueFrom: secretKeyRef: name: mysecret key: password [root@node1 ~]# kubectl create -f busybox3.yaml pod \"busybox\" created 多个pods可以使用同一个secret 可以设置挂载目录权限 可以通过secret.items来重新定义挂载目录 secret存在etcd里，可以被自动刷新，kubelet定时维护容器的卷 default 每个pod都会自动挂载一个default-token-xxx的volume的secret到/var/run/secrets/kubernetes.io/serviceaccount目录，正好是默认Service Account对应的ServiceAccountToken [root@node11 ~]# kubectl describe pod nginx-deployment-5896fbb489-4966s ... Containers: nginx: Container ID: docker://7c84cb527400623792da7da1e4c13697f8929c6237257219e6ff3e81c2cfa782 ... Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-782hd (ro) ... Volumes: default-token-782hd: Type: Secret (a volume populated by a Secret) SecretName: default-token-782hd Optional: false ... [root@node11 ~]# kubectl exec nginx-deployment-5896fbb489-4966s ls /var/run/secrets/kubernetes.io/serviceaccount ca.crt namespace token 容器中的应用就可以通过直接加载这些授权文件，访问操作Kubernetes API了。通过Kubernetes官方的Client包(k8s.io/client-go)，可以自动加载这个目录的文件，不需要配置和编码。 这种把Kubernetes客户端以容器的方式运行在集群里，然后通过default Service Account自动授权的方式，成为\"InClusterConfig\"，是推荐的Kubernetes API编程的授权方式。 kubernetes.io/service-account-token 创建sa时会自动创建默认的secret，或者可以手动创建； kubernetes.io/dockercfg 存放私有Dokcer Registry的认证信息，当Kubernetes在创建Pod并且需要从私有Docker Registry pull镜像时，需要使用认证信息，就会用到kubernetes.io/dockercfg类型的Secret 创建 kubectl create secret docker-registry regsecret \\ --docker-server=harbor.frognew.com \\ --docker-username=the-user \\ --docker-password=the-password \\ --docker-email=the-email \\ --namespace=the-namspace 使用 apiVersion: v1 kind: Pod metadata: name: private-reg spec: containers: - name: private-reg-container image: imagePullSecrets: - name: regsecret 参考 https://kubernetes.io/docs/concepts/configuration/secret/ https://kubernetes.io/cn/docs/concepts/configuration/secret/ https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/ powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/service-account.html":{"url":"kubernetes/service-account.html","title":"Service Account","keywords":"","body":"Service Account ServiceAccount 为 Pod 中的进程提供身份信息，用于pod 访问api server。 自动挂载到容器的 /var/run/secrets/kubernetes.io/serviceaccount 目录中。 在认证时，ServiceAccount 的用户名格式为 system:serviceaccount:(NAMESPACE):(SERVICEACCOUNT)，并从属于两个 group：system:serviceaccounts 和 system:serviceaccounts:(NAMESPACE) 开启sa apiserver 启动参数 --admission-control 加上 ServiceAccount 。 /hyperkube apiserver --advertise-address=192.168.137.101 --etcd-servers=https://192.168.137.101:2379,https://192.168.137.102:2379,https://192.168.137.103:2379 --etcd-quorum-read=true --etcd-cafile=/etc/ssl/etcd/ssl/ca.pem --etcd-certfile=/etc/ssl/etcd/ssl/node-node1.pem --etcd-keyfile=/etc/ssl/etcd/ssl/node-node1-key.pem --insecure-bind-address=127.0.0.1 --apiserver-count=2 --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota --service-cluster-ip-range=10.233.0.0/18 --service-node-port-range=30000-32767 --client-ca-file=/etc/kubernetes/ssl/ca.pem --basic-auth-file=/etc/kubernetes/users/known_users.csv --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem --token-auth-file=/etc/kubernetes/tokens/known_tokens.csv --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem --secure-port=6443 --insecure-port=8080 --storage-backend=etcd3 --v=2 --allow-privileged=true --anonymous-auth=False 查看sa 系统默认给每个 namespace 下面都会创建一个默认的default的sa。 每个sa拥有一个加密的 token，在 secrets 里。 [root@node1 ~]# kubectl get sa --all-namespaces NAMESPACE NAME SECRETS AGE default default 1 8d kube-public default 1 8d kube-system coredns 1 23h kube-system default 1 8d [root@node1 ~]# kubectl get sa default -n kube-system -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-06-06T15:11:35Z name: default namespace: kube-system resourceVersion: \"277\" selfLink: /api/v1/namespaces/kube-system/serviceaccounts/default uid: e77b00db-699b-11e8-aac2-080027d04b6c secrets: - name: default-token-17rhr secret 的类型为 kubernetes.io/service-account-token [root@node1 ~]# kubectl get secret default-token-17rhr -n kube-system -o yaml apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM5ekNDQWQrZ0F3SUJBZ0lKQVArdldaWFZMQ0JxTUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUIydDFZbVV0WTJFd0hoY05NVGd3TmpBMk1UUTBOVFEwV2hjTk5EVXhNREl5TVRRME5UUTBXakFTTVJBdwpEZ1lEVlFRRERBZHJkV0psTFdOaE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCnNFMjBnbkJlQjNuVTF1WTJmR1FKUk96UVlpbXJvN0hFbUlYQVFVaVFiR2VacE5HMWc1MHYyU0RYdlFzS0dMUXIKakJRNVVMd2U2SHhXbEo3NGszQVVpMURLcVBTcEorc3R3TlpIY2xGRWtDbjA3alNPTGxrZjZLUlVkcTlNMHZFTgpreEprRHYrVkxON2dnNzhISVZRMGxkUkh5TWxVcEpTbG9kbkQ5YzVuSG9QWTFKeHo4OE42QWhuTng2RnlnRVVaCkN6QzZXZ202MktPckE1YmNRdGw2eURQbnRTYVV6S3Z6VE94S0tLYXp5cm52cWlDVmdjQzFRRFFKUFRCZFdnNUYKREV0Ukp6RlFtcDJHL3NEcENLOVRqVGZWc0h0R3NyVitUY1VGWWVWM2Eyam9FNC9vSEJINVdoSjRCcC95YUxtNwpuSlpJVmpPNEVmUEtjekhxZmZ6QWt3SURBUUFCbzFBd1RqQWRCZ05WSFE0RUZnUVVHVDNackF2WEFRcStZL2h4CnZTcitGQ3lpMXNNd0h3WURWUjBqQkJnd0ZvQVVHVDNackF2WEFRcStZL2h4dlNyK0ZDeWkxc013REFZRFZSMFQKQkFVd0F3RUIvekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBcG9PbExXL0dnZEtIRUsxSkFyQmtOclRjMlNFLwp2NjFZNWdSZndQdkFMbEwwOHQ1d0hlUjRXMC9BQm9PQUdjL0t5M21OMzFWRmF1cTVZVW8rVFZRdnNsTk5ZZG9ICnUzbVg3a1BlYjIxZkh5WHhubU9tLzdkdTVDcFk5OHBLdnVNaWI5U3Via05nSWpCOVhZN3ZQZFVNQ2M5dnZVaUcKUmVYeGZiczl4UE9FVEJjYTl5a0t3VlVndWtnSTQ3MVViSlRBYzdpbkJjeWJTbWlsT0ppODRrRGdhTzJpc0MvSQp3QVpUVWZiQlZJd0s3MkFmTlhYcWszR2ovMWwzRVRTeXd6VGczc3J2emlYS0NUamFZRDZicWRKZ1gwMkdOeWZFCnprZHVWZEQwKzVtaDhCNEhnVENuemRrQUVQTFJZWVpuVUFMd09VTFRBbExmaHJrRDNoTXJlcUN2T0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== namespace: a3ViZS1zeXN0ZW0= token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUpyZFdKbExYTjVjM1JsYlNJc0ltdDFZbVZ5Ym1WMFpYTXVhVzh2YzJWeWRtbGpaV0ZqWTI5MWJuUXZjMlZqY21WMExtNWhiV1VpT2lKa1pXWmhkV3gwTFhSdmEyVnVMVEUzY21oeUlpd2lhM1ZpWlhKdVpYUmxjeTVwYnk5elpYSjJhV05sWVdOamIzVnVkQzl6WlhKMmFXTmxMV0ZqWTI5MWJuUXVibUZ0WlNJNkltUmxabUYxYkhRaUxDSnJkV0psY201bGRHVnpMbWx2TDNObGNuWnBZMlZoWTJOdmRXNTBMM05sY25acFkyVXRZV05qYjNWdWRDNTFhV1FpT2lKbE56ZGlNREJrWWkwMk9UbGlMVEV4WlRndFlXRmpNaTB3T0RBd01qZGtNRFJpTm1NaUxDSnpkV0lpT2lKemVYTjBaVzA2YzJWeWRtbGpaV0ZqWTI5MWJuUTZhM1ZpWlMxemVYTjBaVzA2WkdWbVlYVnNkQ0o5Lm1MeGFXN0lBWTgyYjNndUFOUXktbzNDcDBBOHUtZHA3dm5UUXFSejV3T0FiWHhzbUtmeVREV2M2RHdZektsVFduX0oybk9wdG82SFh4RjhKSE00UDJXa0lLZThYN2RUcExjVjRIV2JkdllXRUZOckNwNTV6VEkxbkxzVlJkOHFOSmZnRlNhVHk4QXgycXlJalEyOC1zTUR6WUJWTHA5eERwNkxaVkpoV1VvWXg5Rm5WQ3VVUVNBMkNFUXJzZ1BpWkJMc1N2cWZuMl84R0EtZHVDR2E2bHkwZ19sb3hLLTlCQVVEanl3ZFRNTUtpdFg3dktiSE5fYmVUTFdCZDYtaWh4RGw5QmtpSTJmcng5cEUtYmVkSGszUEM2ZDAzRWh2U1pPaFcxbVo2RmI0LTZpVWVRdVJYZEY3NmhZWWxqVDZTNGh6Q1QtQ2U3cUpEQ1dEbFJER0lvZw== kind: Secret metadata: annotations: kubernetes.io/service-account.name: default kubernetes.io/service-account.uid: e77b00db-699b-11e8-aac2-080027d04b6c creationTimestamp: 2018-06-06T15:11:35Z name: default-token-17rhr namespace: kube-system resourceVersion: \"275\" selfLink: /api/v1/namespaces/kube-system/secrets/default-token-17rhr uid: e780df51-699b-11e8-aac2-080027d04b6c type: kubernetes.io/service-account-token 当用户在该 namespace 下创建 pod 时，会默认使用这个 sa ，k8s会默认把 sa 挂载到容器内。 [root@node1 ~]# kubectl get pods busybox1 -n kube-system -o yaml apiVersion: v1 kind: Pod metadata: spec: containers: serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 volumes: - name: default-token-17rhr secret: defaultMode: 420 secretName: default-token-17rhr ... volumes: - name: default-token-17rhr secret: defaultMode: 420 secretName: default-token-17rhr pod的容器里面可以查询到token和crt / # ls -l /var/run/secrets/kubernetes.io/serviceaccount/ total 0 lrwxrwxrwx 1 root root 13 Jun 15 08:22 ca.crt -> ..data/ca.crt lrwxrwxrwx 1 root root 16 Jun 15 08:22 namespace -> ..data/namespace lrwxrwxrwx 1 root root 12 Jun 15 08:22 token -> ..data/token 创建sa 创建一个sa，会自动创建一个secret token，被sa引用。 [root@node1 ~]# cat sa-test.yaml apiVersion: v1 kind: ServiceAccount metadata: name: sa-test [root@node1 ~]# kubectl create -f sa-test.yaml serviceaccount \"sa-test\" created [root@node1 ~]# kubectl get sa/sa-test -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: 2018-06-15T09:29:48Z name: sa-test namespace: default resourceVersion: \"345447\" selfLink: /api/v1/namespaces/default/serviceaccounts/sa-test uid: a5b2b982-707e-11e8-ac37-0800277b75c4 secrets: - name: sa-test-token-xqb7q [root@node1 ~]# kubectl get secrets NAME TYPE DATA AGE default-token-xz2pl kubernetes.io/service-account-token 3 8d sa-test-token-xqb7q kubernetes.io/service-account-token 3 43s [root@node1 ~]# kubectl get secrets/sa-test-token-xqb7q -o yaml apiVersion: v1 data: ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM5ekNDQWQrZ0F3SUJBZ0lKQVArdldaWFZMQ0JxTUEwR0NTcUdTSWIzRFFFQkN3VUFNQkl4RURBT0JnTlYKQkFNTUIydDFZbVV0WTJFd0hoY05NVGd3TmpBMk1UUTBOVFEwV2hjTk5EVXhNREl5TVRRME5UUTBXakFTTVJBdwpEZ1lEVlFRRERBZHJkV0psTFdOaE1JSUJJakFOQmdrcWhraUc5dzBCQVFFRkFBT0NBUThBTUlJQkNnS0NBUUVBCnNFMjBnbkJlQjNuVTF1WTJmR1FKUk96UVlpbXJvN0hFbUlYQVFVaVFiR2VacE5HMWc1MHYyU0RYdlFzS0dMUXIKakJRNVVMd2U2SHhXbEo3NGszQVVpMURLcVBTcEorc3R3TlpIY2xGRWtDbjA3alNPTGxrZjZLUlVkcTlNMHZFTgpreEprRHYrVkxON2dnNzhISVZRMGxkUkh5TWxVcEpTbG9kbkQ5YzVuSG9QWTFKeHo4OE42QWhuTng2RnlnRVVaCkN6QzZXZ202MktPckE1YmNRdGw2eURQbnRTYVV6S3Z6VE94S0tLYXp5cm52cWlDVmdjQzFRRFFKUFRCZFdnNUYKREV0Ukp6RlFtcDJHL3NEcENLOVRqVGZWc0h0R3NyVitUY1VGWWVWM2Eyam9FNC9vSEJINVdoSjRCcC95YUxtNwpuSlpJVmpPNEVmUEtjekhxZmZ6QWt3SURBUUFCbzFBd1RqQWRCZ05WSFE0RUZnUVVHVDNackF2WEFRcStZL2h4CnZTcitGQ3lpMXNNd0h3WURWUjBqQkJnd0ZvQVVHVDNackF2WEFRcStZL2h4dlNyK0ZDeWkxc013REFZRFZSMFQKQkFVd0F3RUIvekFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBcG9PbExXL0dnZEtIRUsxSkFyQmtOclRjMlNFLwp2NjFZNWdSZndQdkFMbEwwOHQ1d0hlUjRXMC9BQm9PQUdjL0t5M21OMzFWRmF1cTVZVW8rVFZRdnNsTk5ZZG9ICnUzbVg3a1BlYjIxZkh5WHhubU9tLzdkdTVDcFk5OHBLdnVNaWI5U3Via05nSWpCOVhZN3ZQZFVNQ2M5dnZVaUcKUmVYeGZiczl4UE9FVEJjYTl5a0t3VlVndWtnSTQ3MVViSlRBYzdpbkJjeWJTbWlsT0ppODRrRGdhTzJpc0MvSQp3QVpUVWZiQlZJd0s3MkFmTlhYcWszR2ovMWwzRVRTeXd6VGczc3J2emlYS0NUamFZRDZicWRKZ1gwMkdOeWZFCnprZHVWZEQwKzVtaDhCNEhnVENuemRrQUVQTFJZWVpuVUFMd09VTFRBbExmaHJrRDNoTXJlcUN2T0E9PQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg== namespace: ZGVmYXVsdA== token: ZXlKaGJHY2lPaUpTVXpJMU5pSXNJblI1Y0NJNklrcFhWQ0o5LmV5SnBjM01pT2lKcmRXSmxjbTVsZEdWekwzTmxjblpwWTJWaFkyTnZkVzUwSWl3aWEzVmlaWEp1WlhSbGN5NXBieTl6WlhKMmFXTmxZV05qYjNWdWRDOXVZVzFsYzNCaFkyVWlPaUprWldaaGRXeDBJaXdpYTNWaVpYSnVaWFJsY3k1cGJ5OXpaWEoyYVdObFlXTmpiM1Z1ZEM5elpXTnlaWFF1Ym1GdFpTSTZJbk5oTFhSbGMzUXRkRzlyWlc0dGVIRmlOM0VpTENKcmRXSmxjbTVsZEdWekxtbHZMM05sY25acFkyVmhZMk52ZFc1MEwzTmxjblpwWTJVdFlXTmpiM1Z1ZEM1dVlXMWxJam9pYzJFdGRHVnpkQ0lzSW10MVltVnlibVYwWlhNdWFXOHZjMlZ5ZG1salpXRmpZMjkxYm5RdmMyVnlkbWxqWlMxaFkyTnZkVzUwTG5WcFpDSTZJbUUxWWpKaU9UZ3lMVGN3TjJVdE1URmxPQzFoWXpNM0xUQTRNREF5TnpkaU56VmpOQ0lzSW5OMVlpSTZJbk41YzNSbGJUcHpaWEoyYVdObFlXTmpiM1Z1ZERwa1pXWmhkV3gwT25OaExYUmxjM1FpZlEucWVmZVZXT2xnTThKYUZ6eG5IaDJQMUMzSXZ0SU9fSGlmRUVBay1vVVZ0cEcxMFJkQWVfemZndmpyWmVCOTVaTlFlREZrZDd2dVpkbXNDY05mMk1mZzhOV0xsZHo3ZDJwUG5PQ0NaUk9xMklxY1R3dzFwc1RFNnhIdWE5RDlEUE9fTzd6Vkx5Y29qV0xDbUN1Zk9qbHpoSGlvR2M3YzhSZE5rRE9zMXhuSEFPY05DcTFkRTFHLTdhVlFCbmxNdG9pTWt3VWJ5SFR3a3dBakpxNHkwbVNXTlg5NllsSWxCVGE2dExJUkNMSTVDRjE4TUYybUV4LURqUGIybkdxMDQ3dHV0aDdqbk9nWW1yaTZWZ3kybWE2Y2tMZWRvOXNzLXJtQm9QbUt1NmJhcjZiSXR0ZkptR2dvUHAtQjQtNVB2N0RqY19FbHdCeHNrQ25GdHVON2oxX0pR kind: Secret metadata: annotations: kubernetes.io/service-account.name: sa-test kubernetes.io/service-account.uid: a5b2b982-707e-11e8-ac37-0800277b75c4 creationTimestamp: 2018-06-15T09:29:48Z name: sa-test-token-xqb7q namespace: default resourceVersion: \"345446\" selfLink: /api/v1/namespaces/default/secrets/sa-test-token-xqb7q uid: a5ec7e4a-707e-11e8-9d66-080027d04b6c type: kubernetes.io/service-account-token 创建sa 的默认 secret token [root@node1 ~]# cat sa-test-secret.yaml apiVersion: v1 kind: Secret metadata: name: sa-test-secret annotations: kubernetes.io/service-account.name: sa-test type: kubernetes.io/service-account-token pod 和 service account 中可以设置 automountServiceAccountToken来取消自动挂载API凭证,如果都设置了，pod设置的优先级更高。apiVersion: v1 kind: ServiceAccount metadata: name: build-robot automountServiceAccountToken: false ... apiVersion: v1 kind: Pod metadata: name: my-pod spec: serviceAccountName: build-robot automountServiceAccountToken: false ... 参考 https://jimmysong.io/kubernetes-handbook/concepts/serviceaccount.html https://blog.csdn.net/u010278923/article/details/72857928 https://www.jianshu.com/p/415c5fc6ddcf https://kubernetes.io/docs/reference/access-authn-authz/authorization/#a-quick-note-on-service-accounts https://github.com/kubernetes/kubernetes/blob/release-1.0/examples/cassandra/README.md powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/configmap.html":{"url":"kubernetes/configmap.html","title":"ConfigMap","keywords":"","body":"ConfigMap ConfigMaps不是属性配置文件的替代品。ConfigMaps只是作为多个properties文件的引用。你可以把它理解为Linux系统中的/etc目录，专门用来存储配置文件的目录。 创建 目录创建 [root@node1 test]# ls config/ game.properties ui.properties [root@node1 test]# cat config/game.properties enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 [root@node1 test]# cat config/ui.properties color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice [root@node1 test]# kubectl create configmap game-config --from-file=config/ configmap \"game-config\" created [root@node1 test]# kubectl get configmap NAME DATA AGE game-config 2 4m [root@node1 test]# kubectl describe configmap game-config Name: game-config Namespace: default Labels: Annotations: Data ==== game.properties: ---- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: ---- color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice Events: [root@node1 test]# kubectl get configmap game-config -o yaml apiVersion: v1 data: game.properties: |- enemies=aliens lives=3 enemies.cheat=true enemies.cheat.level=noGoodRotten secret.code.passphrase=UUDDLRLRBABAS secret.code.allowed=true secret.code.lives=30 ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNice kind: ConfigMap metadata: creationTimestamp: 2018-06-25T11:32:50Z name: game-config namespace: default resourceVersion: \"396096\" selfLink: /api/v1/namespaces/default/configmaps/game-config uid: 7dd2d77c-786b-11e8-a0b4-0800277b75c4 文件创建 [root@node1 test]# kubectl create configmap game-config-2 --from-file=config/game.properties configmap \"game-config-2\" created 环境变量文件创建 [root@node1 test]# cat config/game-env-file.properties enemies=aliens lives=3 allowed=\"true\" # This comment and the empty line above it are ignored [root@node1 test]# kubectl create configmap game-config-3 --from-env-file=config/game-env-file.properties configmap \"game-config-3\" created [root@node1 test]# kubectl get configmap game-config-3 -o yaml apiVersion: v1 data: allowed: '\"true\"' enemies: aliens lives: \"3\" kind: ConfigMap metadata: creationTimestamp: 2018-06-25T12:05:12Z name: game-config-3 namespace: default resourceVersion: \"398547\" selfLink: /api/v1/namespaces/default/configmaps/game-config-3 uid: 039aff2b-7870-11e8-a0b4-0800277b75c4 [root@node1 test]# key=value创建 [root@node1 test]# kubectl create configmap game-config-4 --from-literal=special.how=very --from-literal=special.type=charm configmap \"game-config-4\" created [root@node1 test]# kubectl get configmap game-config-4 -o yaml apiVersion: v1 data: special.how: very special.type: charm kind: ConfigMap metadata: creationTimestamp: 2018-06-25T12:28:31Z name: game-config-4 namespace: default resourceVersion: \"400302\" selfLink: /api/v1/namespaces/default/configmaps/game-config-4 uid: 45a5d8fb-7873-11e8-a0b4-0800277b75c4 [root@node1 test]# 使用 环境变量方式 apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \"/bin/sh\", \"-c\", \"echo $(SPECIAL_LEVEL_KEY) $(SPECIAL_TYPE_KEY)\" ] env: - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_LEVEL - name: SPECIAL_TYPE_KEY valueFrom: configMapKeyRef: name: special-config key: SPECIAL_TYPE restartPolicy: Never volume方式 [root@node1 test]# cat busybox-config.yaml apiVersion: v1 kind: Pod metadata: name: busybox spec: containers: - name: busybox image: busybox:latest command: - sleep - \"3600\" volumeMounts: - name: config-volume mountPath: /etc/config volumes: - name: config-volume configMap: name: game-config-4 [root@node1 test]# vi busybox-config.yaml [root@node1 test]# kubectl create -f busybox-config.yaml pod \"busybox\" created [root@node1 test]# kubectl exec -it busybox ls /etc/config special.how special.type 热更新 使用该 ConfigMap 挂载的 Env 不会同步更新。需要通过滚动更新 pod 的方式来强制重新挂载 ConfigMap。 使用该 ConfigMap 挂载的 Volume 中的数据需要一段时间（实测大概10秒）才能同步更新。 参考 https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/ https://jimmysong.io/kubernetes-handbook/concepts/configmap-hot-update.html powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/kubernetes-RBAC.html":{"url":"kubernetes/kubernetes-RBAC.html","title":"RBAC","keywords":"","body":"Overview Kubernetes集群的访问权限控制由kube-apiserver负责，由Authentication，Authorization，Admission Control三步骤组成： Reference: https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/ 认证 Authentication 认证，身份验证，这个环节面对的输入的整个http request。 有CA证书认证，token，basic auth等多种认证方式。认证信息里面会包括用户身份信息，如user，group等，在后面授权环节使用。 kubeadm部署的集群，默认配置了CA和service account两种认证方式。 CA 认证 token 认证 密码 认证 授权 Authorization Kubernetes中所有的API对象都保存在ETCD中，所有对API对象的操作（增删改查）都需要通过kube-apiserver实现。需要APIServer做授权工作。完成授权工作的机制，就是RBAC。 kubeadm部署的集群，默认配置的认证方式node,RBAC RBAC Reference: https://kubernetes.io/docs/reference/access-authn-authz/rbac/ RBAC: Role-Based Access Control Role: 角色，一组规则，定义了一组对API对象的操作权限。有两种：Role和ClusterRole。 Subject: 被作用者，可以是“人”，“机器”，也可以是Kubernetes定义的User。 RoleBinding: role和subject的绑定关系。也有两种：RoleBinding和ClusterRoleBinding。 Role Role对象只能用于授权对某单一namespace中的资源的访问权限，比如Pod。 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: namespace: mynamespace name: example-role rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] ClusterRole作用于None-namespaced对象，比如Node，定义里面少了namespace的定义。 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrole rules: - apiGroups: [\"\"] resources: [\"pods\"] verbs: [\"get\", \"watch\", \"list\"] Subject Reference: https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/ User 这是一个授权系统里面的逻辑概念，Kubernetes里面没有一个叫User的API对象，在部署和使用的流程中也不需要User，也没创建过User。 User需要通过外部认证服务来提供，比如Keystone。或者，也可以直接给APIServer指定一个用户名，密码文件，授权系统可以从文件里找到对应的User。 大部分情况下使用系统内置的User就足够了。 User是给人用的，命名需全局唯一，None-namespaced。 ServiceAccount 需要绑定到指定的namespace，是给Pod里面的进程使用的。并关联一套凭证，存在Secret里面，这些凭证会同时被挂载到Pod里。 有ServiceAccount的API对象，可以手动创建，也可以api-server自动创建。 默认ServiceAccount是default，对应default的secret。有访问APIServer的绝大多数的权限。 $ kubectl get sa default -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: \"2018-12-07T07:02:13Z\" name: default namespace: default resourceVersion: \"332\" selfLink: /api/v1/namespaces/default/serviceaccounts/default uid: 060d1f4e-f9ee-11e8-924c-080027c2b927 secrets: - name: default-token-782hd RoleBinding RoleBinding给指定的Subject赋予Role的权限，都是在Namespace范围内有效。 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-rolebinding namespace: mynamespace subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: example-role apiGroup: rbac.authorization.k8s.io ClusterRoleBinding给指定Subject赋予ClusterRole的权限，没有Namespace的限制，作用于所有Namespace。 kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: example-clusterrolebinding subjects: - kind: User name: example-user apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: example-clusterrole apiGroup: rbac.authorization.k8s.io ServiceAccount是有namespace的限制的. apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: etcd-operator roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: etcd-operator subjects: - kind: ServiceAccount name: default namespace: etcd Admission Control powered by GitbookUpdated: 2019-01-21 17:23:25 "},"kubernetes/service.html":{"url":"kubernetes/service.html","title":"Service","keywords":"","body":"Service powered by GitbookUpdated: 2019-01-19 17:04:34 "},"kubernetes/pv-pvc.html":{"url":"kubernetes/pv-pvc.html","title":"PV/PVC","keywords":"","body":"PV PVC powered by GitbookUpdated: 2019-01-19 17:04:34 "},"operator/declarative-api.html":{"url":"operator/declarative-api.html","title":"Declarative API","keywords":"","body":"命令式（Imperative ）配置文件操作 vs. Declarative API（声明式API） https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#understanding-kubernetes-objects $ kubectl create -f nginx.yaml $ kubectl replace -f nginx.yaml 通过先create再replace操作，叫做命令式配置文件操作。告诉机器先干什么，再干什么，机器按照一条一条命令去干。 $ kubectl apply -f nginx.yaml 通过apply进行创建和滚动更新，叫做声明式API操作。告诉机器去干什么，不告诉具体指令，机器自动识别完成任务。 replace的执行过程，是使用新的YAML文件中的API对象，替换原有的API对象。 kube-apiserver在响应命令式请求时，一次只能处理一个写请求，否则就会产生冲突的可能，一条一条按顺序执行响应。 apply，是执行了一个对原有API对象的PATCH操作。类似还要set image和edit。 kube-apiserver在响应声明式请求时，一次能处理多个写操作，并且具备Merge的能力。 A declarative API allows you to declare or specify the desired state of your resource and tries to match the actual state to this desired state. Here, the controller interprets the structured data as a record of the user’s desired state, and continually takes action to achieve and maintain this state. 声明式API 原理 https://kubernetes.io/docs/concepts/overview/kubernetes-api/ https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/ https://github.com/resouer/k8s-controller-custom-resource https://github.com/trstringer/k8s-controller-custom-resource https://medium.com/@trstringer/create-kubernetes-controllers-for-core-and-custom-resources-62fc35ad64a3 Kubernetes的所有API对象，如下面树状图： 一个API对象在Etcd里面完整的资源路径由：Group（API组）、verison（API版本）和Resource（API资源类型）三个部门组成。 举例：CronJob 就是API的Resource，batch是它的Group，v2alpha1就是它的Version。 apiVersion: batch/v2alpha1 kind: CronJob ... Kubernetes对Resource，Group，Version解析过程： 先匹配API对象的Group。 再匹配到API对象的Version。 最后匹配API对象的Resource。 Kubernetes的Core API 对象比如Pod，Node等在Core Group下，其他对象在Named Group下： Core Group：REST path is /api/v1 and use apiVersion:v1 Named Group: REST path is /apis/$GROUP_NAME/$VERSION and use apiVersion: $GROUP_NAME/$VERSION 创建过程： 发起创建CronJob的POST请求后，YAML信息被提交给APIServer，APIServer第一个功能就是过滤这个请求，完成一些前置性工作，授权，超时处理，审计等。 请求进入MUX和Routes流程。完成URL和Handler绑定。 APIServer根据定义，创建一个CronJob对象。在这个过程中，APIServer会进行一个Convert工作，把YAML文件转换成Super Version的对象，这个对象是该API资源类型所有版本的字段全集，用户提交的不同版本的YAML文件，都可以用这个对象来处理。 APIServer先后进行Admission和Validation操作。Validation负责验证这个对象各个字段是否合法，验证通过的API对象，保存在API的一个叫Registry的数据结构里。只要一个API对象在Registry里面能查到，就是一个有效的API对象。 APIServer将API对象转换成用户最初提交的版本，进行序列化操作，调用ETCD的API保存。 Declarative API是Kubernetes项目的重中之重，是Google Borg设计思想的集中体现，也是Kubernetes项目中唯一一个被Google和RedHat公司双重控制，其他势力无法从参与的组件。 由于需要监控性能，API完备性，版本化，向后兼容等多种工程化指标，所使用APIServer项目中大量使用了Go语言的代码生成功能，比如自动化Convert，DeepCopy等与API资源相关的操作。 Custom Resource https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/ https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/ https://kubernetes.io/docs/concepts/overview/working-with-objects/kubernetes-objects/#understanding-kubernetes-objects Custom Controller Custom resource simply let you store and retrieve structured data. It is combined with a controller that they become a true declarative API. Custom controller is a controller that users can deploy and update on a running cluster,independently of the cluster's own lifecycle. Custom controller can work any kind of resource, but they are especially effective when combined with custom resources. The Operator pattern is one example of such combination. Adding custom resources CRD(Custom Resource Definition) is simple and can be created without and programming. API Aggregation requires programming,but allows more over API behaviors like how data is stored and convertion between API versions. CustomResourceDefinition CRD API resource allows you to define custom resources. CRD并不是万能的，有很多场景不适用，还有性能瓶颈。比如：不支持protobuf，当API Object 数量 > 1K 或者单个对象 > 1KB，或者高频请求时，CRD的响应都会又问题，所以CRD不能也不应该当作数据库使用。 像Kubernete，或者Etcd本身，最佳使用场景就是作为配置管理的依赖。如果业务需求不能用CRD进行建模的时候，比如需要等待API最终返回，或者需要坚持API的返回值，也是不能用CRD的。同时，当你需要完成的APIServer而不是之关系API对象的时候，需要使用API Aggregator。 Demo - Add new resource using CRD Demo 文件：https://github.com/resouer/k8s-controller-custom-resource 为Kubernetes添加一个叫Network的API资源类型。 作用是用户一旦创建一个Network对象，Kubernetes就应该使用这个对象定义的网络参数，调用真实的网络插件，为用户创建一个真正的网络，用户创建Pod就可以使用这个网络了。 Network对象的YAML文件： apiVersion: samplecrd.k8s.io/v1 kind: Network metadata: name: example-network spec: cidr: \"192.168.0.0/16\" gateway: \"192.168.0.1\" 上面这个文件就是一个CR（Custom Resource），为了能让Kubernetes认识这个CR，需要让Kubernetes明白这个CR的定义是什么，就是CRD（Custom Resource Definition） CRD的YAML： apiVersion: apiextensions.k8s.io/v1beta1 kind: CustomResourceDefinition metadata: # : name: networks.samplecrd.k8s.io spec: # to use for REST API:/apis// group: samplecrd.k8s.io version: v1 names: # CamelCased singular. CR manifests use . kind: Network # to use for the URL:/apis/// plural: networks # Namespaced or Cluster scope: Namespaced scope是Namespaced，定义这个Network是一个属于Namespace的对象，类似Pod。 创建CRD和CR yzw@yzw-vm:~$ kubectl apply -f network.yaml customresourcedefinition.apiextensions.k8s.io/networks.samplecrd.k8s.io created yzw@yzw-vm:~$ kubectl apply -f example-network.yaml network.samplecrd.k8s.io/example-network created yzw@yzw-vm:~$ kubectl get network NAME AGE example-network 13s yzw@yzw-vm:~$ kubectl describe network example-network Name: example-network Namespace: default Labels: Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"samplecrd.k8s.io/v1\",\"kind\":\"Network\",\"metadata\":{\"annotations\":{},\"name\":\"example-network\",\"namespace\":\"default\"},\"spec\":{... API Version: samplecrd.k8s.io/v1 Kind: Network Metadata: Creation Timestamp: 2019-01-04T09:01:10Z Generation: 1 Resource Version: 768534 Self Link: /apis/samplecrd.k8s.io/v1/namespaces/default/networks/example-network UID: 47d514b4-0fff-11e9-9e53-080027c2b927 Spec: Cidr: 192.168.0.0/16 Gateway: 192.168.0.1 Events: Controller Pattern Controller都放在 pkg/controller 目录下，都遵循Kubernetes项目中的一个通用编排模式：控制循环（control loop） for { 实际状态 := 获取集群中对象 X 的实际状态（Actual State） 期望状态 := 获取集群中对象 X 的期望状态（Desired State） if 实际状态 == 期望状态{ 什么都不做 } else { 执行编排动作，将实际状态调整为期望状态 } } 实际状态一般来自于Kubernetes集群本身。期望状态一般来自于用户提交的YAML文件。 Deployment 控制器的实现步骤： Deployment 控制器从Etcd里面获取所有携带“app:nginx”标签的Pod，统计他们的数量，这是实际状态； Deployment 对象的Replicas字段是期望状态； Deployment 控制器讲两个状态进行对比，根据对比结果，确定删除还是创建Pod。 这种对比操作叫调谐（Reconcile），这哥调谐的过程，称作调谐循环（Reconcile Loop）或者同步循环（Sync Loop）。 上半部分是控制器定义，包括期望状态； 下半部分是被控制对象的模板叫PodTemplete组成。 Custom Controller https://github.com/kubernetes/sample-controllerhttps://github.com/resouer/k8s-controller-custom-resource https://blog.openshift.com/kubernetes-deep-dive-code-generation-customresources/ 创建一个GO项目： $ tree ~/go/src/github.com/resouer/k8s-controller-custom-resource . ├── controller.go ├── crd │ └── network.yaml ├── example │ └── example-network.yaml ├── main.go └── pkg └── apis └── samplecrd ├── register.go └── v1 ├── doc.go ├── register.go └── types.go pkg/apis/samplecrd/下创建register.go 用来放全局变量 package samplecrd const ( GroupName = \"samplecrd.k8s.io\" Version = \"v1\" ) pkg/apis/samplecrd/v1 下创建doc.go (Golang的文档源文件) // +k8s:deepcopy-gen=package // +groupName=samplecrd.k8s.io package v1 +[=value] 格式的注释，是K8s进行代码生成要用到的Annotation风格的注释。 +k8s:deepcopy-gen=package 为整个v1包里面的所有类型定义自动生成DeepCopy方法。 +groupName=samplecrd.k8s.io 定义这个包对应的API组的名字。 这些注释又叫 Global Tags 。 pkg/apis/samplecrd/v1 下创建types.go,定义CDR类型有哪些字段，spec字段的内容。 package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" ) // +genclient // +genclient:noStatus // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // Network describes a Network resource type Network struct { // TypeMeta is the metadata for the resource, like kind and apiversion metav1.TypeMeta `json:\",inline\"` // ObjectMeta contains the metadata for the particular object, including // things like... // - name // - namespace // - self link // - labels // - ... etc ... metav1.ObjectMeta `json:\"metadata,omitempty\"` // Spec is the custom resource spec Spec NetworkSpec `json:\"spec\"` } // NetworkSpec is the spec for a Network resource type NetworkSpec struct { // Cidr and Gateway are example custom spec fields // // this is where you would put your custom resource data Cidr string `json:\"cidr\"` Gateway string `json:\"gateway\"` } // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // NetworkList is a list of Network resources type NetworkList struct { metav1.TypeMeta `json:\",inline\"` metav1.ListMeta `json:\"metadata\"` Items []Network `json:\"items\"` } TypeMeta（API元数据）和ObjectMeta（对象元数据），标准K8s对象都有的字段； NetworkSpec 自定义类型的字段； NetworkList 类型用来描述一组Network对象，因为Kubernetes获取所有对象的List()方法，返回值是List类型，而不是对象的类型的数组。 +genclient 为下面API资源类型生成对应的Client代码。 +genclient:noStatus 这个API资源类型没有Status字段，否正生成的Client就会自动带上UpdateStatus字段。 +genclient 只需要写在Network类型，这是主类型；不用写在NetworkList上，这是返回值类型。 不用加+k8s:deepcopy-gen=,Global Tags里面已经定义了。 +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object 生成DeepCopy的时候，实现K8s提供的runtime.Object接口。否则会有编译错误，固定操作。 pkg/apis/samplecrd/v1 下创建register.go文件 package v1 import ( metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\" \"k8s.io/apimachinery/pkg/runtime\" \"k8s.io/apimachinery/pkg/runtime/schema\" \"github.com/resouer/k8s-controller-custom-resource/pkg/apis/samplecrd\" ) // GroupVersion is the identifier for the API which includes // the name of the group and the version of the API var SchemeGroupVersion = schema.GroupVersion{ Group: samplecrd.GroupName, Version: samplecrd.Version, } // create a SchemeBuilder which uses functions to add types to // the scheme var ( SchemeBuilder = runtime.NewSchemeBuilder(addKnownTypes) AddToScheme = SchemeBuilder.AddToScheme ) // Resource takes an unqualified resource and returns a Group qualified GroupResource func Resource(resource string) schema.GroupResource { return SchemeGroupVersion.WithResource(resource).GroupResource() } // Kind takes an unqualified kind and returns back a Group qualified GroupKind func Kind(kind string) schema.GroupKind { return SchemeGroupVersion.WithKind(kind).GroupKind() } // addKnownTypes adds our types to the API scheme by registering // Network and NetworkList func addKnownTypes(scheme *runtime.Scheme) error { scheme.AddKnownTypes( SchemeGroupVersion, &Network{}, &NetworkList{}, ) // register the type in the scheme metav1.AddToGroupVersion(scheme, SchemeGroupVersion) return nil } 通过addKnownTypes让生成的client指定Network和NetworkList类型的定义。 这个文件属于模板写法。 通过k8s.io/code-generator工具自动生成代码： # 代码生成的工作目录，也就是我们的项目路径 $ ROOT_PACKAGE=\"github.com/resouer/k8s-controller-custom-resource\" # API Group $ CUSTOM_RESOURCE_NAME=\"samplecrd\" # API Version $ CUSTOM_RESOURCE_VERSION=\"v1\" # 安装 k8s.io/code-generator $ go get -u k8s.io/code-generator/... $ cd $GOPATH/src/k8s.io/code-generator # 执行代码自动生成，其中 pkg/client 是生成目标目录，pkg/apis 是类型定义目录 $ ./generate-groups.sh all \"$ROOT_PACKAGE/pkg/client\" \"$ROOT_PACKAGE/pkg/apis\" \"$CUSTOM_RESOURCE_NAME:$CUSTOM_RESOURCE_VERSION\" enerating deepcopy funcs Generating clientset for samplecrd:v1 at github.com/resouer/k8s-controller-custom-resource/pkg/client/clientset Generating listers for samplecrd:v1 at github.com/resouer/k8s-controller-custom-resource/pkg/client/listers Generating informers for samplecrd:v1 at github.com/resouer/k8s-controller-custom-resource/pkg/client/informers ./generate-groups.sh参数使用的是跟import一样的相对路径。 生成完成后，项目目录如下： $ tree . ├── controller.go ├── crd │ └── network.yaml ├── example │ └── example-network.yaml ├── main.go └── pkg ├── apis │ └── samplecrd │ ├── constants.go │ └── v1 │ ├── doc.go │ ├── register.go │ ├── types.go │ └── zz_generated.deepcopy.go └── client ├── clientset ├── informers └── listers zz_generated.deepcopy.go 自动生成的DeepCopy代码文件。 生成client目录，K8s为Network类型生成的客户端库，后面写自定义控制器的时候会用到。 main.go 里面完成初始化和启动一共自定义controller。 package main var ( masterURL string kubeconfig string ) func main() { flag.Parse() // set up signals so we handle the first shutdown signal gracefully stopCh := signals.SetupSignalHandler() cfg, err := clientcmd.BuildConfigFromFlags(masterURL, kubeconfig) if err != nil { glog.Fatalf(\"Error building kubeconfig: %s\", err.Error()) } kubeClient, err := kubernetes.NewForConfig(cfg) if err != nil { glog.Fatalf(\"Error building kubernetes clientset: %s\", err.Error()) } networkClient, err := clientset.NewForConfig(cfg) if err != nil { glog.Fatalf(\"Error building example clientset: %s\", err.Error()) } networkInformerFactory := informers.NewSharedInformerFactory(networkClient, time.Second*30) controller := NewController(kubeClient, networkClient, networkInformerFactory.Samplecrd().V1().Networks()) go networkInformerFactory.Start(stopCh) if err = controller.Run(2, stopCh); err != nil { glog.Fatalf(\"Error running controller: %s\", err.Error()) } } func init() { flag.StringVar(&kubeconfig, \"kubeconfig\", \"\", \"Path to a kubeconfig. Only required if out-of-cluster.\") flag.StringVar(&masterURL, \"master\", \"\", \"The address of the Kubernetes API server. Overrides any value in kubeconfig. Only required if out-of-cluster.\") } Informer和API对象一一对应，我们自定义Network对象的Informer就是Network Informer。 networkInformerFactory通过networkClient跟APIServer建立连接，负责维护这个连接的是Relflector包，通过ListAndWatch方法获取和监听Network对象实例化的变化。 ListAndWatch一旦监控盗APIServer端有信的Network实例被创建，删除或者更新，Reflector就会收到“事件通知”。该事件及它对应的API对象 叫增量（Delta），放进FIFO中。 Informer不断从FIFO中Pop增量，判断事件类型，创建或更新本地对象缓存（Store）。 如果事件是Added，Informer会通过Indexer把这个对象实例保存到本地缓存，并创建索引；如果是Deleted，则会从本地缓存删掉。 Informer根据事件类型，触发事先注册好的ResourceEventHandler（AddFunc，DeleteFunc，UpdateFunc）。 main 里面创建2个client和Informer，完成初始化控制器。 Informer除了监听盗事件会同步本地缓存外，还会经过resyncPeriod指定事件，维护缓存，使用最近一次LIST结果强制更新一次。在K8s中，这个缓存强制更新的操作叫resync。 总之：Informer有两个职责：1.维护本地缓存（store）；2.触发Handler。Informer就是一个带本地缓存和索引机制的，可以注册EventHandler的client。； func NewController( kubeclientset kubernetes.Interface, networkclientset clientset.Interface, networkInformer informers.NetworkInformer) *Controller { ... controller := &Controller{ kubeclientset: kubeclientset, networkclientset: networkclientset, networksLister: networkInformer.Lister(), networksSynced: networkInformer.Informer().HasSynced, workqueue: workqueue.NewNamedRateLimitingQueue(..., \"Networks\"), ... } networkInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: controller.enqueueNetwork, UpdateFunc: func(old, new interface{}) { oldNetwork := old.(*samplecrdv1.Network) newNetwork := new.(*samplecrdv1.Network) if oldNetwork.ResourceVersion == newNetwork.ResourceVersion { return } controller.enqueueNetwork(new) }, DeleteFunc: controller.enqueueNetworkForDelete, return controller } 创建了一个work queue，同步Informer和控制循环之间的数据。 为networkInformer注册3个Handler。 实际入队的不是API对象本身，是Key，/。 控制循环不停的从queue里面拿Key，开始执行真正的控制逻辑。 func (c *Controller) Run(threadiness int, stopCh main 中调用controller.Run()启动控制循环。 等待Informer完成一次本地缓存数据同步后，启动多个无限循环的任务。 func (c *Controller) runWorker() { for c.processNextWorkItem() { } } func (c *Controller) processNextWorkItem() bool { obj, shutdown := c.workqueue.Get() ... err := func(obj interface{}) error { ... if err := c.syncHandler(key); err != nil { return fmt.Errorf(\"error syncing '%s': %s\", key, err.Error()) } c.workqueue.Forget(obj) ... return nil }(obj) ... return true } func (c *Controller) syncHandler(key string) error { namespace, name, err := cache.SplitMetaNamespaceKey(key) ... network, err := c.networksLister.Networks(namespace).Get(name) if err != nil { if errors.IsNotFound(err) { glog.Warningf(\"Network does not exist in local cache: %s/%s, will delete it from Neutron ...\", namespace, name) glog.Warningf(\"Network: %s/%s does not exist in local cache, will delete it from Neutron ...\", namespace, name) // FIX ME: call Neutron API to delete this network by name. // // neutron.Delete(namespace, name) return nil } ... return err } glog.Infof(\"[Neutron] Try to process network: %#v ...\", network) // FIX ME: Do diff(). // // actualNetwork, exists := neutron.Get(namespace, name) // // if !exists { // neutron.Create(namespace, name) // } else if !reflect.DeepEqual(actualNetwork, network) { // neutron.Update(namespace, name) // } return nil } 从workqueue.Get一个Key； 如果返回IsNotFound，则标识这个Key是前面通过删除事件加入到队列的，虽然队列有这个Key，但是实例已经被删了。 控制器拿到的这个Key对应的Network对象，是APIServer里保存的“期望状态” “实际状态”得通过访问集群获取。 通过对比期望状态和实际状态的差异，完成一次协调的过程。 # Clone repo $ go build -o samplecrd-controller . $ ./samplecrd-controller -kubeconfig=$HOME/.kube/config -alsologtostderr=true I0915 12:50:29.051349 27159 controller.go:84] Setting up event handlers I0915 12:50:29.051615 27159 controller.go:113] Starting Network control loop I0915 12:50:29.051630 27159 controller.go:116] Waiting for informer caches to sync E0915 12:50:29.066745 27159 reflector.go:134] github.com/resouer/k8s-controller-custom-resource/pkg/client/informers/externalversions/factory.go:117: Failed to list *v1.Network: the server could not find the requested resource (get networks.samplecrd.k8s.io) ... 编译工程，执行控制器，创建,更新，删除Network,可以看到打印信息。 整个流程不仅可以用在自定义API资源，而且可以用在K8s原生的默认API对象上完成复杂的编排功能，比如可以定义一个Deployment的Informer等 Demo - Istio Istio项目时一个基于Kubernetes项目的微服务治理框架： Istio最根本的组件，时运行在每个应用Pod里面的Envoy容器。 Envoy项目是一个高性能C++网络代理，这个代理服务以sidecar容 器的方式，运行在每个被治理的应用Pod中，共享Network Namespace，Envoy容器通过Pod的iptables，把整个Pod的进出流量接管下来。 Istio的控制层（Control Plane）里的Pilot组件，通过调用Envoy容器的API，对Envoy代理进行配置，实现微服务治理。 Dynamic Admission Control / Initializer 当一个Pod或者任何一个API对象被提交给APIServer后，总由一些“初始化”性质的工作需要在它们被Kubernetes项目正式处理之前进行，比如自动加Lables。 “初始化”操作的实现，借助一个叫Admission的功能，就是Kubernetes项目里一组被成为Admission Controller的代码，可以选择性被编译进APIServer中，在API对象创建之初后被立刻调用。 Kubernetes提供了一种不用重新编译APIServer的“热插拔”式的Admisson机制，叫Dynamic Admission Control，又叫Initializer。 举例： demo 地址：https://github.com/resouer/kubernetes-initializer-tutorial 创建一个Pod，只有一个用户容器： apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'] Istio项目要做的，就是在这个Pod YAML被提交给Kubernetes之后，在它对应的API对象里面自动加上Envoy容器的配置： apiVersion: v1 kind: Pod metadata: name: myapp-pod labels: app: myapp spec: containers: - name: myapp-container image: busybox command: ['sh', '-c', 'echo Hello Kubernetes! && sleep 3600'] - name: envoy image: lyft/envoy:845747b88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] ... 被Istio处理后，这个Pod里面，多了一个叫Envoy的容器。 Istio要做的就是编写一个用来为Pod“自动注入”Envoy 容器的Initializer。 先Istio将这个Envoy容器本身的定义，以ConfigMap的方式保存在Kubernetes中, data部分式一个Pod对象的一部分定义： apiVersion: v1 kind: ConfigMap metadata: name: envoy-initializer data: config: | containers: - name: envoy image: lyft/envoy:845747db88f102c0fd262ab234308e9e22f693a1 command: [\"/usr/local/bin/envoy\"] args: - \"--concurrency 4\" - \"--config-path /etc/envoy/envoy.json\" - \"--mode serve\" ports: - containerPort: 80 protocol: TCP resources: limits: cpu: \"1000m\" memory: \"512Mi\" requests: cpu: \"100m\" memory: \"64Mi\" volumeMounts: - name: envoy-conf mountPath: /etc/envoy volumes: - name: envoy-conf configMap: name: envoy Initializer要干的工作就是把Envoy相关字段，自动添加到用户提交的Pod的API对象里。 用户提交的Pod里面本来就有Containers，volume字段，Kubernetes在处理这些的更新请求时，就必须使用类似git merge这样的操作，将两部分内容合并。 Initializer更新用户Pod对象的时候，必须使用PATCH API来完成。这种PATCH API是声明式API最主要的能力。 接下来，Istio将一个编写好的Initializer，作为一个Pod部署在Kubernetes中： apiVersion: v1 kind: Pod metadata: labels: app: envoy-initializer name: envoy-initializer spec: containers: - name: envoy-initializer image: envoy-initializer:0.0.1 imagePullPolicy: Always Kubernetes的Controller的就是一个死循环，不断获取实际状态，与期望状态作对比。 envoy-initializer使用的镜像是一个事先编译好的Custom Controller，主要功能是：不断获取到“实际状态”，就是用户新创建的Pod。它的“期望状态”，则是：这个Pod里被添加Envoy容器的定义。 for { // 获取新创建的 Pod pod := client.GetLatestPod() // Diff 一下，检查是否已经初始化过 if !isInitialized(pod) { // 没有？那就来初始化一下 doSomething(pod) } } func doSomething(pod) { cm := client.Get(ConfigMap, \"envoy-initializer\") newPod := Pod{} newPod.Spec.Containers = cm.Containers newPod.Spec.Volumes = cm.Volumes // 生成 patch 数据 patchBytes := strategicpatch.CreateTwoWayMergePatch(pod, newPod) // 发起 PATCH 请求，修改这个 pod 对象 client.Patch(pod.Name, patchBytes) } 判断如果Pod里面没有添加过Envoy容器，则进行Initialize操作，修改Pod的API（doSomething）。 Initializer： 先从APIServer里拿到ConfigMap； 把ConfigMap里面的containers和volumes字段，直接添加进一个空Pod对象里面； 调用Kubernetes API库提供的合并两个Pod对象的方法CreateTwoWayMergePatch生成woWayMergePatch； 使用这个patch数据，调用Client，发起一个PATCH请求。 这样就完成了对一个用户提交的Pod，自动加上Envoy容器相关字段。 Kubernetes还允许你通过配置，来指定要对什么样的资源进行Initialize操作： apiVersion: admissionregistration.k8s.io/v1alpha1 kind: InitializerConfiguration metadata: name: envoy-config initializers: // 这个名字必须至少包括两个 \".\" - name: envoy.initializer.kubernetes.io rules: - apiGroups: - \"\" // 前面说过， \"\" 就是 core API Group 的意思 apiVersions: - v1 resources: - pods 这个配置对所有Pod进行Initialize操作，指定负责操作的Initializer叫envoy-initializer。 一旦这个InitializerConfiguration被创建，Kubernetes就会把这个Initializer的名字，加到新Pod的Metadata上： apiVersion: v1 kind: Pod metadata: initializers: pending: - name: envoy.initializer.kubernetes.io name: myapp-pod labels: app: myapp ... 每一个新创的Pod，都会自动携带metadata.initializers.pending的Metadata信息。 这个Metadata，就是Initializer的控制器判断这个Pod有没有执行过所负责的初始化的操作的重要一句，就是isInitialized方法的含义。 我们还可以在具体Pod的Annotation里添加一个字段，声明要使用这个Initializer： apiVersion: v1 kind: Pod metadata annotations: \"initializer.kubernetes.io/envoy\": \"true\" ... 就会使用到我们之前定义的envoy-initializer了。 Istio项目的核心，就是由无数个运行Pod中的Envoy容器组成的服务代理网络，这正是Service Mesh的含有。 Kubernetes 声明式API具有对API对象进行在线的更新的能力： 声明式，指的是我职须提交一个定义好的API对象来声明，我所期望的状态是什么样子。 声明式API运行有多个API写端，以PATCH的方式对API对象进行修改，无需关心本原始YAML文件的内容。 有了上面两个能力，Kubernetes项目才可以基于对API对象的增删改查，在完全无需外界干预的情况下，实现对实际状态和期望状态的调谐（Reconcile）过程。 无论对sidecar容器的巧妙设计，还是对Initializer的合理利用，Istio项目的设计和实现，都依托于Kubernetes的声明式API和它所提供的各种编排能力 Note kubectl apply 是通过mvcc实现并发写的。 https://www.envoyproxy.io/docs/envoy/latest/intro/comparison Initializer和Preset都能注入Pod配置，Preset是Initializer的子集，比较适合发布流程离处理比较简单的情况，Initializer是需要写代码的。 Envoy相对Nginx，HAProxy的优势在于编程友好的API，方便容器化，配置方便。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"operator/operator.html":{"url":"operator/operator.html","title":"Operator","keywords":"","body":"Why Operator Overview https://coreos.com/blog/introducing-operators.html https://coreos.com/operators/ Kubernetes 管理“有状态应用” 比较复杂，Operator提供了一种相对更加灵活和编程友好的管理“有状态应用”的解决方案。 目前大部分的分布式应用比如etcd，Redis，Kafka，Spark，TensorFlow等都有官方维护着的Operator，Operator成了开发和部署分布式应用的一项事实标准,成为容器化时代应用开发和发布的一个全新途径。 可以在Operator官方库里，找到一直维护着的一个知名分布式项目的Operator总汇：https://github.com/operator-framework/awesome-operators 虽然Operator是把一个复杂集群应用，抽象成一个具有一定“自治能力”的整体，但是这个自治能力本身不足以解决问题的时候，还需要其他手段来弥补。比如Etcd集群的备份和恢复能力，Etcd Operator就无法解决，通过Etcd Backup/Restore Operator来解决。 Background Kubernetes 项目最具有价值的理念，就是围绕Etcd构建出的一套“面向终态”编排体系，即 “声明式API” 。 “声明式API” 核心原理，就是当用户提交一个API对象描述后，。Kubernetes通过启动“Controller Pattern”的无限循环，通过调谐来保证整个集群里各项资源的状态和你的API对象描述的需求相一致。 Operator也是相同的思想，用户提交的API对象不再是一个单位应用描述，而是一个分布式应用的集群的描述，Operator这个controller会保证集群的状态和用户提交的描述的一致。 History https://mp.weixin.qq.com/s/_S13tCdkf4y3mxXjEWUWiA 《亲历者说：Kubernetes API 与 Operator，不为人知的开发者战争》 这篇文章是Operator的发起者CoreOS的邓洪超，和阿里的张磊写的，讲述了Operator的起源和背后的故事。 TPR(Third Party Resource) --> UAS(User Aggregated APIServer) --> APIServer Aggregator --> CDR(Custom Resource Definition) Operator vs. StatefulSet StatefulSet 是Kubernetes专门来管理有状态应用的，其核心原理就是对分布式应用的两种状态进行保持： 拓扑状态，节点之间的启动顺序； 存储状态，每个节点依赖的持久化数据。 StatefulSet 通过为节点分配有序的DNS名字来保持拓扑状态（为Pod编号），通过远程持久化数据卷方案（Pod和PV绑定）来保持存储状态，比较适用于应用本身节点管理能力不完善的项目，比如 MySQL。像Etcd这种自管理的分布式应用，有自己的数据备份和恢复方法，使用StatefulSet就很别扭，有各种限制。 完全可用在Operator里面创建和控制StatefulSet而不是Pod，Prometheus就是这么干的。 Operator vs. Helm 相比于 Helm、Docker Compose 等描述应用静态关系的编排工具，Operator 定义的乃是应用运行起来后整个集群的动态逻辑。 Helm只是部署工具，集群逻辑还是需要K8s自己来维护。 通过用Helm来部署Operator，以及新对象。 Etcd Operator https://github.com/coreos/etcd-operator Deploy an etcd cluster with 3 nodes # clone etcd-operator repo $ git clone https://github.com/coreos/etcd-operator # create clusterrole and clusterrolebinding $ example/rbac/create_role.sh # deploy etcd operator $ kubectl create -f example/deployment.yaml # deploy etcd cluster $ kubectl apply -f example/example-etcd-cluster.yaml Create ClusterRole and ClusterRoleBinding 对Pod,Service,PVC,Deployment等API有所有权限； 对CRD对象有所有权限； 对属于etcd.database.coreos.com 这个API Group的CR对象etcdbackups，etcdclusters，etcdrestores 有所有权限。 $ example/rbac/create_role.sh Creating role with ROLE_NAME=etcd-operator, NAMESPACE=default clusterrole.rbac.authorization.k8s.io/etcd-operator created Creating role binding with ROLE_NAME=etcd-operator, ROLE_BINDING_NAME=etcd-operator, NAMESPACE=default clusterrolebinding.rbac.authorization.k8s.io/etcd-operator created $ kubectl describe clusterrolebindings.rbac.authorization.k8s.io etcd-operator Name: etcd-operator Labels: Annotations: Role: Kind: ClusterRole Name: etcd-operator Subjects: Kind Name Namespace ---- ---- --------- ServiceAccount default default $ kubectl describe clusterrole etcd-operator Name: etcd-operator Labels: Annotations: PolicyRule: Resources Non-Resource URLs Resource Names Verbs --------- ----------------- -------------- ----- endpoints [] [] [*] events [] [] [*] persistentvolumeclaims [] [] [*] pods [] [] [*] services [] [] [*] customresourcedefinitions.apiextensions.k8s.io [] [] [*] deployments.apps [] [] [*] etcdbackups.etcd.database.coreos.com [] [] [*] etcdclusters.etcd.database.coreos.com [] [] [*] etcdrestores.etcd.database.coreos.com [] [] [*] secrets [] [] [get] Deploy Etcd Operator Etcd Operator 本身就是一个Deployment： apiVersion: extensions/v1beta1 kind: Deployment metadata: name: etcd-operator spec: replicas: 1 template: metadata: labels: name: etcd-operator spec: containers: - name: etcd-operator image: quay.io/coreos/etcd-operator:v0.9.3 command: - etcd-operator # Uncomment to act for resources in all namespaces. More information in doc/user/clusterwide.md #- -cluster-wide env: - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name 直接创建就可以： $ kubectl apply -f example/deployment.yaml deployment.extensions/etcd-operator created Etcd Operator的Pod进入Running状态，CRD就会被自动创建出来： Name：etcdclusters.etcd.database.coreos.com API Group：etcd.database.coreos.com API Kind: EtcdCluster $ kubectl get crd NAME CREATED AT etcdclusters.etcd.database.coreos.com 2019-01-16T01:40:36Z $ kubectl describe crd etcdclusters.etcd.database.coreos.com Name: etcdclusters.etcd.database.coreos.com Namespace: Labels: Annotations: API Version: apiextensions.k8s.io/v1beta1 Kind: CustomResourceDefinition Metadata: Creation Timestamp: 2019-01-16T01:40:36Z Generation: 1 Resource Version: 7260 Self Link: /apis/apiextensions.k8s.io/v1beta1/customresourcedefinitions/etcdclusters.etcd.database.coreos.com UID: b8d35097-192f-11e9-9c3a-080027c2b927 Spec: Conversion: Strategy: None Group: etcd.database.coreos.com Names: Kind: EtcdCluster List Kind: EtcdClusterList Plural: etcdclusters Short Names: etcd ...... Etcd Operator本身就是个自定义资源类型对应的自定义控制器。 Deploy Etcd Cluster 部署Etcd集群,如下就是部署一个EtcdCluster的CR，Etcd节点数为3，yaml如下： apiVersion: \"etcd.database.coreos.com/v1beta2\" kind: \"EtcdCluster\" metadata: name: \"example-etcd-cluster\" spec: size: 3 version: \"3.2.13\" 部署etcd集群，通过修改yaml就可以scale，upgrade集群，Operator保证集群可以failover： $ kubectl apply -f example/example-etcd-cluster.yaml etcdcluster.etcd.database.coreos.com/example-etcd-cluster created $ kubectl get pods NAME READY STATUS RESTARTS AGE example-etcd-cluster-lcxlcf6d8x 1/1 Running 0 55s example-etcd-cluster-mg8x9h4xfg 1/1 Running 0 111s example-etcd-cluster-z8ckrc7cvd 1/1 Running 0 95s Access Etcd Cluster Etcd Operator创建集群后，会自动创建一个client service，通过这个Cluster IP和Port就可以访问Etcd集群： $ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE example-etcd-cluster ClusterIP None 2379/TCP,2380/TCP 18m example-etcd-cluster-client ClusterIP 10.106.102.1 2379/TCP 18m 在Pod里面就可以访问Etcd集群： $ kubectl run --rm -i --tty fun --image quay.io/coreos/etcd:v3.2.13 --restart=Never -- /bin/sh / # ETCDCTL_API=3 etcdctl --endpoints http://example-etcd-cluster-client:2379 member list 74b19bcfe1048628, started, example-etcd-cluster-z8ckrc7cvd, http://example-etcd-cluster-z8ckrc7cvd.example-etcd-cluster.default.svc:2380, http://example-etcd-cluster-z8ckrc7cvd.example-etcd-cluster.default.svc:2379 82f4a1214aae8562, started, example-etcd-cluster-mg8x9h4xfg, http://example-etcd-cluster-mg8x9h4xfg.example-etcd-cluster.default.svc:2380, http://example-etcd-cluster-mg8x9h4xfg.example-etcd-cluster.default.svc:2379 a3ca3926e767f7e0, started, example-etcd-cluster-lcxlcf6d8x, http://example-etcd-cluster-lcxlcf6d8x.example-etcd-cluster.default.svc:2380, http://example-etcd-cluster-lcxlcf6d8x.example-etcd-cluster.default.svc:2379 / # ETCDCTL_API=3 etcdctl --endpoints http://example-etcd-cluster-client:2379 put /test/ok 22 OK / # ETCDCTL_API=3 etcdctl --endpoints http://example-etcd-cluster-client:2379 get /test/ok /test/ok 22 Other 还可以通过Helm部署Operator。 https://github.com/helm/charts/tree/master/stable/etcd-operator How Etcd Operator Works Operator 就是利用Kubernetes的CRD，来描述我们要部署的“有状态应用”，然后在自定义控制器里，通过自定义API对象的变化，完成具体的部署和运维工作。 编写一个Etcd Operator 跟编写一个自定义控制器没啥区别。 Etcd Operator部署Etcd集群，采用的是静态集群Static的方式，没有采用动态集群的方式。 静态集群部署前需要规划好集群拓扑结构，和各个节点的固定IP。 $ etcd --name infra0 --initial-advertise-peer-urls http://10.0.1.10:2380 \\ --listen-peer-urls http://10.0.1.10:2380 \\ ... --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\ --initial-cluster-state new $ etcd --name infra1 --initial-advertise-peer-urls http://10.0.1.11:2380 \\ --listen-peer-urls http://10.0.1.11:2380 \\ ... --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\ --initial-cluster-state new $ etcd --name infra2 --initial-advertise-peer-urls http://10.0.1.12:2380 \\ --listen-peer-urls http://10.0.1.12:2380 \\ ... --initial-cluster-token etcd-cluster-1 \\ --initial-cluster infra0=http://10.0.1.10:2380,infra1=http://10.0.1.11:2380,infra2=http://10.0.1.12:2380 \\ --initial-cluster-state new 没有使用WorkQueue。 // TODO: use workqueue to avoid blocking ... 对每一个EtcdCluster对象，都启动了一个控制循环，“并发”的响应这些对象的变化。 Bootstrap 创建一个“种子节点”。种子节点和普通节点区别是-initial-cluster-state这个参数的值，new代表种子节点，existing为普通节点，Etcd Operator需要把普通节点加入到集群。 Add/RemoveOneMember 给集群创建或者移除Etcd节点，直到集群的节点数等于size。 当Pod还没创建出来，IP也没被分配，Cluster对象会事先创建一个于该EtcdCluster同名的Headless Service。这样Etcd Operator在接下来的所有创建Pod步骤里，就都可以使用Pod的DNS记录（...svc.cluster.local）来替代它的IP地址了。 addOneMember： 生成一个新节点的Pod的名字，比如：example-etcd-cluster-xxx； 调用Etcd Client，etcdctl member add example-etcd-cluster-xxx命名。 使用这个Pod 名字，和已经存在的所有节点列表，组成inital-cluster字段，使用这个字段生成Pod里面的Etcd容器的启动命令：/usr/local/bin/etcd --data-dir=/var/etcd/data --name=example-etcd-cluster-v6v6s6stxd --initial-advertise-peer-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380 --listen-peer-urls=http://0.0.0.0:2380 --listen-client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2379 --initial-cluster=example-etcd-cluster-mbzlg6sd56=http://example-etcd-cluster-mbzlg6sd56.example-etcd-cluster.default.svc:2380,example-etcd-cluster-v6v6s6stxd=http://example-etcd-cluster-v6v6s6stxd.example-etcd-cluster.default.svc:2380 --initial-cluster-state=existing removeOneMember 类似； addOneMember/removeOneMember使用的是Pod的名字，而不是DNS，这些操作会更新Etcd内部维护的拓扑信息，不许集群外部 通过固定编号来固定这个拓扑关系（StatefullSet需要通过编号来固定）。 Etcd基于Raft协议的Key-value存储，只要半数以下节点失效，当前集群依然正常可用，Etcd Operator通过控制循环创建新Pod加入集群，这个集群会一直可用，所以不需要对每个节点的data持久化。 如果半数以上节点失效，集群就丧失了写入能力，进入不可用状态，新创的Pod也无法自动恢复起来了，为了解决这个问题，又有一个Etcd Backup/Restore Operator负责完成对集群进行备份和恢复工作，这两个Operator也在etcd-operator项目中。 Operator SDK https://github.com/operator-framework https://github.com/operator-framework/awesome-operators https://banzaicloud.com/blog/operator-sdk/ https://jimmysong.io/kubernetes-handbook/develop/operator-sdk.html CoreOS 为了方便用户开发Operator，推出了Operator SDK 即operator-framework，简化了开发步骤。 powered by GitbookUpdated: 2019-01-21 17:23:25 "},"app/etcd.html":{"url":"app/etcd.html","title":"Etcd","keywords":"","body":"ETCD kubernetes的所有云数据都保存在/registry目录下，下一层是API对象类型（复数），再下一层是namespace，最后一层是对象名字。 calico网络存储是以v2的API来存储的，kubernetes是v3 API。 环境变量 ETCDCTL_API 默认是2 v2 查询命令 [root@node1 test]# export ETCDCTL_API=2 [root@node1 test]# etcdctl --version etcdctl version: 3.2.4 API version: 2 [root@node1 test]# etcdctl ls /calico [root@node1 test]# etcdctl get /calico/bgp/v1/host/node1/ip_addr_v4 192.168.137.101 v3 查询命令 --prefix 查询所有子目录 -w 输出格式 -m [root@node1 test]# export ETCDCTL_API=3 [root@node1 test]# [root@node1 test]# etcdctl --cacert=/etc/ssl/etcd/ssl/ca.pem --cert=/etc/ssl/etcd/ssl/member-node1.pem --key=/etc/ssl/etcd/ssl/member-node1-key.pem get /registry/configmaps/default/game-config -w=json|python -m json.tool { \"count\": 1, \"header\": { \"cluster_id\": 8635001981321661628, \"member_id\": 12035000773768423000, \"raft_term\": 1682, \"revision\": 416787 }, \"kvs\": [ { \"create_revision\": 396096, \"key\": \"L3JlZ2lzdHJ5L2NvbmZpZ21hcHMvZGVmYXVsdC9nYW1lLWNvbmZpZw==\", \"mod_revision\": 396096, \"value\": \"azhzAAoPCgJ2MRIJQ29uZmlnTWFwEu8CClMKC2dhbWUtY29uZmlnEgAaB2RlZmF1bHQiACokN2RkMmQ3N2MtNzg2Yi0xMWU4LWEwYjQtMDgwMDI3N2I3NWM0MgA4AEILCOKlw9kFEKypk3x6ABKxAQoPZ2FtZS5wcm9wZXJ0aWVzEp0BZW5lbWllcz1hbGllbnMKbGl2ZXM9MwplbmVtaWVzLmNoZWF0PXRydWUKZW5lbWllcy5jaGVhdC5sZXZlbD1ub0dvb2RSb3R0ZW4Kc2VjcmV0LmNvZGUucGFzc3BocmFzZT1VVURETFJMUkJBQkFTCnNlY3JldC5jb2RlLmFsbG93ZWQ9dHJ1ZQpzZWNyZXQuY29kZS5saXZlcz0zMBJkCg11aS5wcm9wZXJ0aWVzElNjb2xvci5nb29kPXB1cnBsZQpjb2xvci5iYWQ9eWVsbG93CmFsbG93LnRleHRtb2RlPXRydWUKaG93Lm5pY2UudG8ubG9vaz1mYWlybHlOaWNlChoAIgA=\", \"version\": 1 } ] } 输出key base64编码了，需要解码 [root@node1 test]# echo L3JlZ2lzdHJ5L2NvbmZpZ21hcHMvZGVmYXVsdC9nYW1lLWNvbmZpZw==|base64 -d /registry/configmaps/default/game-config [root@node1 test]# 遇到的问题 etcd容器内无法查询 /etc # etcdctl --ca-file=$ETCD_TRUSTED_CA_FILE --cert-file=$ETCD_CERT_FILE --key-file=ETCD_KEY_FILE cluster-health Error: open ETCD_KEY_FILE: no such file or directory /etc # etcdctl --ca-file=$ETCD_TRUSTED_CA_FILE --cert-file=$ETCD_CERT_FILE --key-file=$ETCD_KEY_FILE cluster-health cluster may be unhealthy: failed to list members Error: client: etcd cluster is unavailable or misconfigured; error #0: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\" ; error #1: dial tcp 127.0.0.1:4001: getsockopt: connection refused error #0: malformed HTTP response \"\\x15\\x03\\x01\\x00\\x02\\x02\" error #1: dial tcp 127.0.0.1:4001: getsockopt: connection refused 解决办法 /etc # export ETCDCTL_ENDPOINT=https://127.0.0.1:2379 /etc # etcdctl --ca-file=$ETCD_TRUSTED_CA_FILE --cert-file=$ETCD_CERT_FILE --key-file=$ETCD_KEY_FILE cluster-health member 82cf071e8608bc84 is healthy: got healthy result from https://192.168.137.102:2379 member a704e9708804a658 is healthy: got healthy result from https://192.168.137.101:2379 member e30c96cca4e3ace1 is healthy: got healthy result from https://192.168.137.103:2379 cluster is healthy 参考文档 https://jimmysong.io/kubernetes-handbook/guide/using-etcdctl-to-access-kubernetes-data.html https://jimmysong.io/kubernetes-handbook/concepts/etcd.html https://zhangkesheng.github.io/2018/01/25/kubernetes-ha/ powered by GitbookUpdated: 2019-01-21 17:23:25 "},"app/calico.html":{"url":"app/calico.html","title":"Calico","keywords":"","body":"calico install https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/ calicoctrl install installing calicoctl as a binary, a container or a K8S pod. https://docs.projectcalico.org/v3.3/usage/calicoctl/install # curl -O -L https://github.com/projectcalico/calicoctl/releases/download/v3.3.2/calicoctl # chmod +x calicoctl configue https://docs.projectcalico.org/v3.3/usage/calicoctl/configure/ configuration file /etc/calico/calicoctl.cfg --config xxx with ETCD datastore apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: etcdEndpoints: https://etcd1:2379,https://etcd2:2379,https://etcd3:2379 etcdKeyFile: /etc/calico/key.pem etcdCertFile: /etc/calico/cert.pem etcdCACertFile: /etc/calico/ca.pem with the Kubernetes API datastore apiVersion: projectcalico.org/v3 kind: CalicoAPIConfig metadata: spec: datastoreType: \"kubernetes\" kubeconfig: \"~/.kube/config\" environment variables with ETCD datastore # export ETCD_ENDPOINTS=http://localhost:6666 or # ETCD_ENDPOINTS=http://localhost:6666 ./calicoctl node status with the Kubernetes API datastore # export CALICO_DATASTORE_TYPE=kubernetes # export CALICO_KUBECONFIG=~/.kube/config or # DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes commands https://docs.projectcalico.org/v3.3/reference/calicoctl/commands/ # ./calicoctl node status # ./calicoctl get node # ./calicoctl get ipPool -o wide powered by GitbookUpdated: 2019-01-21 17:23:25 "},"app/helm.html":{"url":"app/helm.html","title":"Helm","keywords":"","body":"Helm git：https://github.com/kubernetes/helm doc：https://docs.helm.sh/ ref：https://jimmysong.io/kubernetes-handbook/practice/helm.html 安装 客户端 helm [root@kubespray-node-1 ~]# curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 6740 100 6740 0 0 2147 0 0:00:03 0:00:03 --:--:-- 2148 [root@kubespray-node-1 ~]# [root@kubespray-node-1 ~]# chmod 700 get_helm.sh [root@kubespray-node-1 ~]# ./get_helm.sh Helm v2.9.1 is already latest helm installed into /usr/local/bin/helm Run 'helm init' to configure helm. [root@kubespray-node-1 ~]# helm version Client: &version.Version{SemVer:\"v2.9.1\", GitCommit:\"20adb27c7c5868466912eebdf6664e7390ebe710\", GitTreeState:\"clean\"} 服务端 tiller helm init 下载的地址是：gcr.io/kubernetes-helm/tiller [root@kubespray-node-1 ~]# helm init --skip-refresh -i yinzw/tiller:v2.9.1 Creating /root/.helm/repository/repositories.yaml Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com Adding local repo with URL: http://127.0.0.1:8879/charts $HELM_HOME has been configured at /root/.helm. Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster. Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy. For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation Happy Helming! 使用 命令自动补全： source 命令： helm create xxx helm install ./ helm list helm delete helm search 使用： [root@kubespray-node-1 helm]# git clone https://github.com/kubernetes/charts.git Cloning into 'charts'... remote: Counting objects: 34086, done. remote: Total 34086 (delta 0), reused 0 (delta 0), pack-reused 34086 Receiving objects: 100% (34086/34086), 8.66 MiB | 136.00 KiB/s, done. Resolving deltas: 100% (23722/23722), done. [root@kubespray-node-1 helm]# cd charts/ [root@kubespray-node-1 charts]# ls code-of-conduct.md CONTRIBUTING.md incubator LICENSE OWNERS PROCESSES.md README.md REVIEW_GUIDELINES.md stable test [root@kubespray-node-1 helm]# cd stable/mysql/ [root@kubespray-node-1 mysql]# ls Chart.yaml README.md templates values.yaml [root@kubespray-node-1 mysql]# helm package . Successfully packaged chart and saved it to: /root/helm/charts/stable/mysql/mysql-0.8.2.tgz [root@kubespray-node-1 mysql]# pwd /root/helm/charts/stable/mysql [root@kubespray-node-1 mysql]# ls Chart.yaml mysql-0.8.2.tgz README.md templates values.yaml [root@kubespray-node-1 mysql]# helm search mysql WARNING: Repo \"stable\" is corrupt or missing. Try 'helm repo update'.NAME CHART VERSION APP VERSION DESCRIPTION local/mysql 0.8.2 5.7.14 Fast, reliable, scalable, and easy to use open-... [root@kubespray-node-1 mysql]# helm repo list NAME URL stable https://kubernetes-charts.storage.googleapis.com local http://127.0.0.1:8879/charts ## install [root@kubespray-node-1 mysql]# helm install ./mysql-0.8.2.tgz [root@kubespray-node-1 mysql]# helm install ./ ## 检查配置和模板是否有效 [root@kubespray-node-1 mysql]# helm install --dry-run --debug . [debug] Created tunnel using local port: '36168' [debug] SERVER: \"127.0.0.1:36168\" [debug] Original chart version: \"\" [debug] CHART PATH: /root/helm/charts/stable/mysql NAME: impressive-snail REVISION: 1 RELEASED: Mon Jul 9 18:06:12 2018 [root@kubespray-node-1 mysql]# helm delete --purge ingress-kube-system [root@kubespray-node-1 mysql]# helm delete --purge ingress-openstack [root@kubespray-node-1 mysql]# helm delete --purge ingress-ceph powered by GitbookUpdated: 2019-01-21 17:23:25 "}}